{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I realize that job titles tend to be arbitrary - but I\u2019ve recently gotten told by multiple people that my title is incorrect and compensation is too low based off my job responsibilities.\n\nMy official title is operations analyst and my total comp is around 70k - I currently have 3 years of experience working as an analyst. And my company is a mid sized saas tech firm based on the west coast. \n\nResponsibilities- I  spend 50% of my time in snowflake analyzing data and creating various scripts/schemas using dbt - and 30% of the time using etl/elt tools to load data from various sources into our warehouse.. and the remaining 20% of the time I\u2019m creating dashboards with our bi tool. The business would love the bi portion of my work to be higher - but I really try keep my work to 60hrs a week - and the moving/cleaning of data takes up the majority of time.\n\nI\u2019ve been told my title should be anything ranging from data analyst, analytics engineer, data engineer and even product analyst.. so I\u2019m just curious what everyone on the de subreddit thoughts are regarding my title and compensation? And if I\u2019m not near or at the official data engineer title yet - what steps should I take to move myself in that direction?", "author_fullname": "t2_d11os2fi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I a Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhhqbf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1667146684.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667145351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I realize that job titles tend to be arbitrary - but I\u2019ve recently gotten told by multiple people that my title is incorrect and compensation is too low based off my job responsibilities.&lt;/p&gt;\n\n&lt;p&gt;My official title is operations analyst and my total comp is around 70k - I currently have 3 years of experience working as an analyst. And my company is a mid sized saas tech firm based on the west coast. &lt;/p&gt;\n\n&lt;p&gt;Responsibilities- I  spend 50% of my time in snowflake analyzing data and creating various scripts/schemas using dbt - and 30% of the time using etl/elt tools to load data from various sources into our warehouse.. and the remaining 20% of the time I\u2019m creating dashboards with our bi tool. The business would love the bi portion of my work to be higher - but I really try keep my work to 60hrs a week - and the moving/cleaning of data takes up the majority of time.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been told my title should be anything ranging from data analyst, analytics engineer, data engineer and even product analyst.. so I\u2019m just curious what everyone on the de subreddit thoughts are regarding my title and compensation? And if I\u2019m not near or at the official data engineer title yet - what steps should I take to move myself in that direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yhhqbf", "is_robot_indexable": true, "report_reasons": null, "author": "General-Geologist-53", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhhqbf/am_i_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhhqbf/am_i_a_data_engineer/", "subreddit_subscribers": 78372, "created_utc": 1667145351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI already know Python and SQL and I\u2019m wondering what would be a good skill to learn next for becoming more experienced with data engineering.\n\nThank you in advance!", "author_fullname": "t2_4841f127", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What skill would you learn after Python and SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhhl2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667144994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I already know Python and SQL and I\u2019m wondering what would be a good skill to learn next for becoming more experienced with data engineering.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yhhl2y", "is_robot_indexable": true, "report_reasons": null, "author": "The-Fourth-Hokage", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhhl2y/what_skill_would_you_learn_after_python_and_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhhl2y/what_skill_would_you_learn_after_python_and_sql/", "subreddit_subscribers": 78372, "created_utc": 1667144994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi im working on using CDC (Debezium) to emit events from service databases (PG, Mongo)\n\nthe corresponding kafka topics have 30d retention and I intend on having s3 sink connector for long term storage (open to other ideas here too, I noticed theres a [hudi connector](https://github.com/apache/hudi/tree/master/hudi-kafka-connect) also)\n\nfrom the long term storage in I have a few requirements:\n\n1. need to look for different states that a customer underwent (eg: CDC history for a given ID)\n2. need to be able to anonymise data/delete the personal info for a customer if they offboard (eg: email)\n3. need to aggregate latest state of the system across multiple datastores for business intelligence/analytics (essentially query the \"live\" system but aggregate across DBs)\n4. try to manage cost in the long run since there will be multiple batch jobs consuming this data\n\nI have only been able to think of a solution for (1) and (3) above.\n\n&amp;#x200B;\n\nMy understanding of the differences between a data lake and warehouse:\n\n1. data lake: raw data for everything sent (cdc data in s3)\n2. data warehouse: refined data that may undergo ETL and is usable for analytics queries\n\n&amp;#x200B;\n\nWhen I started research, i was left scratching my head when it comes to some specifics:\n\n1. storage options:\n   1. since im producing avro format in kafka, should I just archive in my data lake using avro too?\n   2. alternative was to convert avro to parquet before writing to s3 data lake but I dont see much benefit \n2. query options:\n   1. I thought using athena might be enough to query s3 data lake but I will incur cost per query which may add up\n   2. I also saw a solution using Hudi, spark, Hive which also achieve similar outcome as athena. But why so much complexity is what I dont understand. I still only think use case (1) and (3) are achieved so is athena the better option? Is this more cost efficient?\n\n&amp;#x200B;\n\nim a data n00b and am eager to learn, so please feel free to correct anything I may have gotten wrong above and kindly help point me in the right direction so i can make a decision based on understanding trade offs involved", "author_fullname": "t2_6guf9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data n00b looking for guidance on how to setup data lake/warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yh0r4a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1667092038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi im working on using CDC (Debezium) to emit events from service databases (PG, Mongo)&lt;/p&gt;\n\n&lt;p&gt;the corresponding kafka topics have 30d retention and I intend on having s3 sink connector for long term storage (open to other ideas here too, I noticed theres a &lt;a href=\"https://github.com/apache/hudi/tree/master/hudi-kafka-connect\"&gt;hudi connector&lt;/a&gt; also)&lt;/p&gt;\n\n&lt;p&gt;from the long term storage in I have a few requirements:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;need to look for different states that a customer underwent (eg: CDC history for a given ID)&lt;/li&gt;\n&lt;li&gt;need to be able to anonymise data/delete the personal info for a customer if they offboard (eg: email)&lt;/li&gt;\n&lt;li&gt;need to aggregate latest state of the system across multiple datastores for business intelligence/analytics (essentially query the &amp;quot;live&amp;quot; system but aggregate across DBs)&lt;/li&gt;\n&lt;li&gt;try to manage cost in the long run since there will be multiple batch jobs consuming this data&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have only been able to think of a solution for (1) and (3) above.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My understanding of the differences between a data lake and warehouse:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;data lake: raw data for everything sent (cdc data in s3)&lt;/li&gt;\n&lt;li&gt;data warehouse: refined data that may undergo ETL and is usable for analytics queries&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;When I started research, i was left scratching my head when it comes to some specifics:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;storage options:\n\n&lt;ol&gt;\n&lt;li&gt;since im producing avro format in kafka, should I just archive in my data lake using avro too?&lt;/li&gt;\n&lt;li&gt;alternative was to convert avro to parquet before writing to s3 data lake but I dont see much benefit &lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;query options:\n\n&lt;ol&gt;\n&lt;li&gt;I thought using athena might be enough to query s3 data lake but I will incur cost per query which may add up&lt;/li&gt;\n&lt;li&gt;I also saw a solution using Hudi, spark, Hive which also achieve similar outcome as athena. But why so much complexity is what I dont understand. I still only think use case (1) and (3) are achieved so is athena the better option? Is this more cost efficient?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;im a data n00b and am eager to learn, so please feel free to correct anything I may have gotten wrong above and kindly help point me in the right direction so i can make a decision based on understanding trade offs involved&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yjeNeh16yHBzz47ot6QLydI6qwqXAWYisdl793R842g.jpg?auto=webp&amp;s=b228343537ac133f1955efebefc69e4033b7505a", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/yjeNeh16yHBzz47ot6QLydI6qwqXAWYisdl793R842g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c28adc1c07a909f078b7b90fc62efe7dd014462", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/yjeNeh16yHBzz47ot6QLydI6qwqXAWYisdl793R842g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bef4b028693359fab85bd9a0fc54e22c6c01bd6e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/yjeNeh16yHBzz47ot6QLydI6qwqXAWYisdl793R842g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0e762616f724f5bfd519ba5b6b053048f7f9442", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/yjeNeh16yHBzz47ot6QLydI6qwqXAWYisdl793R842g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8a987d939753743dc5c35d76faffb69af62cabdc", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/yjeNeh16yHBzz47ot6QLydI6qwqXAWYisdl793R842g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=611066b6ebfbdca334eabd8a2a3eee8a308ecb99", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/yjeNeh16yHBzz47ot6QLydI6qwqXAWYisdl793R842g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d770bc6751b24cb2a35c7b72784f11ee75691019", "width": 1080, "height": 540}], "variants": {}, "id": "Q7-pHHrS_unoxSRtBaQoBlsP_IkzQDLbD9wkmDjIuIo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yh0r4a", "is_robot_indexable": true, "report_reasons": null, "author": "cyberjar09", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yh0r4a/data_n00b_looking_for_guidance_on_how_to_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yh0r4a/data_n00b_looking_for_guidance_on_how_to_setup/", "subreddit_subscribers": 78372, "created_utc": 1667092038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if anyone has experience here.\n\nWe\u2019re looking at making data available to secondary transactional systems, but it occurred to me that the analytics tools could access this way too, allowing us have an abstraction layer and make it easier to switch warehouse platforms down the track.\n\nBut it\u2019s not common - or at least I\u2019ve never seen it.\n\nIt didn\u2019t used to be common in web development either, but it\u2019s now pervasive. Is this a matter of Data lagging behind, or are there reasons not to go this route?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DWH access via API?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_ygxar9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667083546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if anyone has experience here.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re looking at making data available to secondary transactional systems, but it occurred to me that the analytics tools could access this way too, allowing us have an abstraction layer and make it easier to switch warehouse platforms down the track.&lt;/p&gt;\n\n&lt;p&gt;But it\u2019s not common - or at least I\u2019ve never seen it.&lt;/p&gt;\n\n&lt;p&gt;It didn\u2019t used to be common in web development either, but it\u2019s now pervasive. Is this a matter of Data lagging behind, or are there reasons not to go this route?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "ygxar9", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/ygxar9/dwh_access_via_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/ygxar9/dwh_access_via_api/", "subreddit_subscribers": 78372, "created_utc": 1667083546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, \n\nso I am currently studying for the AWS associate certificate and I am taking the exam relatively soon. in my current employer, we use AWS (although they don't have much idea about Data engineering the cloud of choice is AWS :D ). my question is, there are so many tools that are used on the market, from open source tools like Apache Hive, presto and Pig and Oozie to the cloud-based service equivalents like Athena and Glue. \n\n&amp;#x200B;\n\nI can't really imagine my learning path for the next couple of years clearly, so should I go deeper into AWS and become a specialist or should I start learning other clouds like Azure, which has a huge adoption growth where I work right now (in Germany)?\n\n&amp;#x200B;\n\nthat's my question, how do I know the in-demand tools in the market right now to learn them?\n\n&amp;#x200B;\n\nmy goal is to increase my TC over the coming time while learning interesting data engineering stuff.  \n\np.s I have around 5+ years of experience but around 2+ years in the cloud.", "author_fullname": "t2_k1ljg9t7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Develop Skills horizontally or vertically in data engineering ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yh7zf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667115344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, &lt;/p&gt;\n\n&lt;p&gt;so I am currently studying for the AWS associate certificate and I am taking the exam relatively soon. in my current employer, we use AWS (although they don&amp;#39;t have much idea about Data engineering the cloud of choice is AWS :D ). my question is, there are so many tools that are used on the market, from open source tools like Apache Hive, presto and Pig and Oozie to the cloud-based service equivalents like Athena and Glue. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t really imagine my learning path for the next couple of years clearly, so should I go deeper into AWS and become a specialist or should I start learning other clouds like Azure, which has a huge adoption growth where I work right now (in Germany)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;that&amp;#39;s my question, how do I know the in-demand tools in the market right now to learn them?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;my goal is to increase my TC over the coming time while learning interesting data engineering stuff.  &lt;/p&gt;\n\n&lt;p&gt;p.s I have around 5+ years of experience but around 2+ years in the cloud.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "yh7zf6", "is_robot_indexable": true, "report_reasons": null, "author": "TheDataJanit0r", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yh7zf6/develop_skills_horizontally_or_vertically_in_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yh7zf6/develop_skills_horizontally_or_vertically_in_data/", "subreddit_subscribers": 78372, "created_utc": 1667115344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, as the title suggests, I'm looking for decent resources on testing data processes/pipelines.\n\nI'm looking to start simple, but really feel like it's something missing from my skill set. I understand it at a high level from an engineering perspective, but not had any real experience applying it to my day job (DE).\n\nRecommendations of books, videos and stand-out articles, all really appreciated. Thanks, folks!", "author_fullname": "t2_5dzsk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DAE know any good resources on testing data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhcv4p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667132183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, as the title suggests, I&amp;#39;m looking for decent resources on testing data processes/pipelines.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to start simple, but really feel like it&amp;#39;s something missing from my skill set. I understand it at a high level from an engineering perspective, but not had any real experience applying it to my day job (DE).&lt;/p&gt;\n\n&lt;p&gt;Recommendations of books, videos and stand-out articles, all really appreciated. Thanks, folks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yhcv4p", "is_robot_indexable": true, "report_reasons": null, "author": "Ooberdan", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhcv4p/dae_know_any_good_resources_on_testing_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhcv4p/dae_know_any_good_resources_on_testing_data/", "subreddit_subscribers": 78372, "created_utc": 1667132183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm wondering if anyone has any experience running a data pipeline in front of Splunk.\n\nThe main motivator is our cyber security team wants to trial different SIEM solutions like crowdstrike logscale and MS sentinel. I'm very comfortable with splunk and can reroute, filter and transform data on there, but it is not really easy or flexible. \n\nIn splunk circles a product called cribl is a popular choice as the data pipeline because of ease of integration and gui etc.\n\nI would prefer an open source solution, but paid is fine too. \n\nAm I on the right track looking into these products:\n- Apache flink \n- Apache pulsar \n- kafka\n- Apache spark\n\nAny commercial solutions to check out?", "author_fullname": "t2_ds7ey", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline in front Splunk", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yh63ps", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667109827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m wondering if anyone has any experience running a data pipeline in front of Splunk.&lt;/p&gt;\n\n&lt;p&gt;The main motivator is our cyber security team wants to trial different SIEM solutions like crowdstrike logscale and MS sentinel. I&amp;#39;m very comfortable with splunk and can reroute, filter and transform data on there, but it is not really easy or flexible. &lt;/p&gt;\n\n&lt;p&gt;In splunk circles a product called cribl is a popular choice as the data pipeline because of ease of integration and gui etc.&lt;/p&gt;\n\n&lt;p&gt;I would prefer an open source solution, but paid is fine too. &lt;/p&gt;\n\n&lt;p&gt;Am I on the right track looking into these products:\n- Apache flink \n- Apache pulsar \n- kafka\n- Apache spark&lt;/p&gt;\n\n&lt;p&gt;Any commercial solutions to check out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yh63ps", "is_robot_indexable": true, "report_reasons": null, "author": "jihape", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yh63ps/data_pipeline_in_front_splunk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yh63ps/data_pipeline_in_front_splunk/", "subreddit_subscribers": 78372, "created_utc": 1667109827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m running airflow on composer.\nI have a Dag that:\n\n1-fetches large json from an api\n2-writes the json to a file in GCS\n3-reads the file and do transformations \n4-writes the result to a new file on GCS\n\nMy dag keeps stoping at step 3 with a message of type SIGKILL.\n\nHow can I solve such an issue ?\nWhat are the best practices on passing data between tasks in such a manner ?\n\nThank you", "author_fullname": "t2_98269xyy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow - Passing large data volumes between tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhlxei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667154317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m running airflow on composer.\nI have a Dag that:&lt;/p&gt;\n\n&lt;p&gt;1-fetches large json from an api\n2-writes the json to a file in GCS\n3-reads the file and do transformations \n4-writes the result to a new file on GCS&lt;/p&gt;\n\n&lt;p&gt;My dag keeps stoping at step 3 with a message of type SIGKILL.&lt;/p&gt;\n\n&lt;p&gt;How can I solve such an issue ?\nWhat are the best practices on passing data between tasks in such a manner ?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yhlxei", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Mountain9", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhlxei/airflow_passing_large_data_volumes_between_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhlxei/airflow_passing_large_data_volumes_between_tasks/", "subreddit_subscribers": 78372, "created_utc": 1667154317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context is\u2014 engineering team has Postgres source database in an EC2. Change data capture is not going to happen on it, trust me, it\u2019s not haha. I\u2019ll spare the backstory there, but we are stuck with having to daily full reloads of tables in to snowflake. \n\nWhat tools are good performance-wise (potentially handle a daily reload of a table with 100 million rows in a timely manner) that are also affordable? Potentially self-hosted or self-managed? Also not interested in writing completely custom code like python for it. \n\nAirbyte hosted in k8s? Open to ideas. Thank you! \n\nWe\u2019re using Stitch right now, it works really quite well actually. Just hoping to find something else that we could save a bit of money on not using Stitch.", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performant (but affordable) tools for full reloads from Postgres to snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhi1il", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667146094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context is\u2014 engineering team has Postgres source database in an EC2. Change data capture is not going to happen on it, trust me, it\u2019s not haha. I\u2019ll spare the backstory there, but we are stuck with having to daily full reloads of tables in to snowflake. &lt;/p&gt;\n\n&lt;p&gt;What tools are good performance-wise (potentially handle a daily reload of a table with 100 million rows in a timely manner) that are also affordable? Potentially self-hosted or self-managed? Also not interested in writing completely custom code like python for it. &lt;/p&gt;\n\n&lt;p&gt;Airbyte hosted in k8s? Open to ideas. Thank you! &lt;/p&gt;\n\n&lt;p&gt;We\u2019re using Stitch right now, it works really quite well actually. Just hoping to find something else that we could save a bit of money on not using Stitch.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yhi1il", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhi1il/performant_but_affordable_tools_for_full_reloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhi1il/performant_but_affordable_tools_for_full_reloads/", "subreddit_subscribers": 78372, "created_utc": 1667146094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Dataengineers,  \nI am planning to create a streaming data analytics demo project with apache kafka(opensource) and Jupyter notebok with Pyspark(open source) both running on a linux EC2 machine(free tier), using Redshift(free tier) as data repository.   \n\n\nAm I missing something here. What will be the possible roadblock I may face?", "author_fullname": "t2_3vjykfp5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming analytics pipeline using ec2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yh96c5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667118855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Dataengineers,&lt;br/&gt;\nI am planning to create a streaming data analytics demo project with apache kafka(opensource) and Jupyter notebok with Pyspark(open source) both running on a linux EC2 machine(free tier), using Redshift(free tier) as data repository.   &lt;/p&gt;\n\n&lt;p&gt;Am I missing something here. What will be the possible roadblock I may face?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yh96c5", "is_robot_indexable": true, "report_reasons": null, "author": "akhilseban", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yh96c5/streaming_analytics_pipeline_using_ec2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yh96c5/streaming_analytics_pipeline_using_ec2/", "subreddit_subscribers": 78372, "created_utc": 1667118855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2rtceaie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DynamoDB, Ten Years Later", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_yhidtu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/UtqN-9ETk6lxYLyC_W3oNo1QZSQxE3p5aAgZ4nuel9k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667146897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mydistributed.systems", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.mydistributed.systems/2022/10/dynamodb-ten-years-later.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yU0M4Ai8KWwYnjSOVEFyYnfxChaW_gKOgdulABSbISE.jpg?auto=webp&amp;s=782634f9434a2c719ebc141bde1d4918364b5475", "width": 260, "height": 130}, "resolutions": [{"url": "https://external-preview.redd.it/yU0M4Ai8KWwYnjSOVEFyYnfxChaW_gKOgdulABSbISE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6c43dbc1ff9c8839e21373a8668ce83fffd78e9b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/yU0M4Ai8KWwYnjSOVEFyYnfxChaW_gKOgdulABSbISE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=46e48640bba0f6318a5e5ecd56988d347e013e2e", "width": 216, "height": 108}], "variants": {}, "id": "UzvD1SXjYOoQpbX7xz6iLZain6oMkSiDy8NPr4WBuaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "yhidtu", "is_robot_indexable": true, "report_reasons": null, "author": "roohitavaf", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhidtu/dynamodb_ten_years_later/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.mydistributed.systems/2022/10/dynamodb-ten-years-later.html", "subreddit_subscribers": 78372, "created_utc": 1667146897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nI potentially have some support from my company to pay for a course. I am working in Data Analysis and Project Management with Developer experience. Would you rather recommend DataCamp or DataQuest for Data Engineering? I have few understanding of this field.\n\nAdditionally I will do the DataClub Zoomcamp. But I would like to use the support from my company to choose one of these courses. I like them as I as well can do some Data Analytics courses when signed up for a yearly subscription. I tried out both and besides that DataQuest is more do-it-yourself vs. DataCamp being more handholding I do not see much of a difference. When it comes to the course content of course I dont know what would prepare me better for a Data Engineering role.\n\nCould you support with deciding or even have a third or fourth one to share?\n\nThank you. :)", "author_fullname": "t2_9v9dakww", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataCamp vs DataQuest vs other advices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhgovd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667142771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;I potentially have some support from my company to pay for a course. I am working in Data Analysis and Project Management with Developer experience. Would you rather recommend DataCamp or DataQuest for Data Engineering? I have few understanding of this field.&lt;/p&gt;\n\n&lt;p&gt;Additionally I will do the DataClub Zoomcamp. But I would like to use the support from my company to choose one of these courses. I like them as I as well can do some Data Analytics courses when signed up for a yearly subscription. I tried out both and besides that DataQuest is more do-it-yourself vs. DataCamp being more handholding I do not see much of a difference. When it comes to the course content of course I dont know what would prepare me better for a Data Engineering role.&lt;/p&gt;\n\n&lt;p&gt;Could you support with deciding or even have a third or fourth one to share?&lt;/p&gt;\n\n&lt;p&gt;Thank you. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Aspiring Data Engineer / Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yhgovd", "is_robot_indexable": true, "report_reasons": null, "author": "binchentso", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/yhgovd/datacamp_vs_dataquest_vs_other_advices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhgovd/datacamp_vs_dataquest_vs_other_advices/", "subreddit_subscribers": 78372, "created_utc": 1667142771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got 25 worker nodes (on prem data center) for a new Geospatial Data Conversion and Analysis SaaS App\nI'm struggling to figure out best methodology to handle running jobs\nWe want normally one job on one machine because most of our tools and software is multithreaded and if not it's too much of impact on I/O or other resources \nMany big Geospatial data processing jobs are heavy CPU Dependenant and so most worker nodes are 32-64 threads \n\nWe also have one spark cluster for running pyspark and geotrellis and Geomesa and mrgeo \nI was thinking of using Kestra or Luigi but these are new to us.\nAnyone have experience in this and have some recommendations?\nmaps@techmaven.net\nhttps://portfolio.techmaven.net", "author_fullname": "t2_4fps3x6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "job queue workflows/orchestration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhfupx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667140638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got 25 worker nodes (on prem data center) for a new Geospatial Data Conversion and Analysis SaaS App\nI&amp;#39;m struggling to figure out best methodology to handle running jobs\nWe want normally one job on one machine because most of our tools and software is multithreaded and if not it&amp;#39;s too much of impact on I/O or other resources \nMany big Geospatial data processing jobs are heavy CPU Dependenant and so most worker nodes are 32-64 threads &lt;/p&gt;\n\n&lt;p&gt;We also have one spark cluster for running pyspark and geotrellis and Geomesa and mrgeo \nI was thinking of using Kestra or Luigi but these are new to us.\nAnyone have experience in this and have some recommendations?\n&lt;a href=\"mailto:maps@techmaven.net\"&gt;maps@techmaven.net&lt;/a&gt;\n&lt;a href=\"https://portfolio.techmaven.net\"&gt;https://portfolio.techmaven.net&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yhfupx", "is_robot_indexable": true, "report_reasons": null, "author": "techmavengeospatial", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhfupx/job_queue_workflowsorchestration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhfupx/job_queue_workflowsorchestration/", "subreddit_subscribers": 78372, "created_utc": 1667140638.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I was wondering what's the best practice for handling error notifications in AWS Glue? Should this be done within the script or is a separate event driven process better?", "author_fullname": "t2_2vuapfhl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beat Practice for AWS Glue Error Notification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yh7u8e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667114927.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was wondering what&amp;#39;s the best practice for handling error notifications in AWS Glue? Should this be done within the script or is a separate event driven process better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yh7u8e", "is_robot_indexable": true, "report_reasons": null, "author": "TheShitStorms92", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yh7u8e/beat_practice_for_aws_glue_error_notification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yh7u8e/beat_practice_for_aws_glue_error_notification/", "subreddit_subscribers": 78372, "created_utc": 1667114927.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Kusto Detective Agency's 4th (and so far hardest) case is live today. \n\nReminder, you can play at [detective.kusto.io](https://detective.kusto.io/), this is an official Azure minigame to help users learn about Kusto (ADX), you can win prizes and official Microsoft Credly badges (many to be earned!), and you can get a totally free cluster by signing up with the instructions in the onboarding challenge.\n\nHere's a thread on [r/kustodetectiveagency](https://www.reddit.com/r/kustodetectiveagency/) if you're after a hint, maybe some kind souls can help you out! (Or if you just want to show off your badges)\n\n[https://www.reddit.com/r/kustodetectiveagency/comments/yhk0on/case\\_4\\_thread\\_hints\\_tips\\_theories\\_badges/](https://www.reddit.com/r/kustodetectiveagency/comments/yhk0on/case_4_thread_hints_tips_theories_badges/)", "author_fullname": "t2_3urcm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Kusto/Azure Data Explorer] Kusto Detective Agency - Case 4 is live and now KDA has its own subreddit @ r/kustodetectiveagency - win prizes and Microsoft Credly badges whilst sharpening your big data skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhkv6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667152089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kusto Detective Agency&amp;#39;s 4th (and so far hardest) case is live today. &lt;/p&gt;\n\n&lt;p&gt;Reminder, you can play at &lt;a href=\"https://detective.kusto.io/\"&gt;detective.kusto.io&lt;/a&gt;, this is an official Azure minigame to help users learn about Kusto (ADX), you can win prizes and official Microsoft Credly badges (many to be earned!), and you can get a totally free cluster by signing up with the instructions in the onboarding challenge.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a thread on &lt;a href=\"https://www.reddit.com/r/kustodetectiveagency/\"&gt;r/kustodetectiveagency&lt;/a&gt; if you&amp;#39;re after a hint, maybe some kind souls can help you out! (Or if you just want to show off your badges)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/kustodetectiveagency/comments/yhk0on/case_4_thread_hints_tips_theories_badges/\"&gt;https://www.reddit.com/r/kustodetectiveagency/comments/yhk0on/case_4_thread_hints_tips_theories_badges/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "yhkv6r", "is_robot_indexable": true, "report_reasons": null, "author": "wittykitty", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhkv6r/kustoazure_data_explorer_kusto_detective_agency/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhkv6r/kustoazure_data_explorer_kusto_detective_agency/", "subreddit_subscribers": 78372, "created_utc": 1667152089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to work on AWS Glue and pyspark for my next project. I'm new to both Spark and Glue. Any recommended comprehensive resources for beginners?", "author_fullname": "t2_mm4ey504", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark and AWS Glue resources for beginners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_yhb4l6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1667126106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to work on AWS Glue and pyspark for my next project. I&amp;#39;m new to both Spark and Glue. Any recommended comprehensive resources for beginners?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "yhb4l6", "is_robot_indexable": true, "report_reasons": null, "author": "rohithkumar-31", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/yhb4l6/spark_and_aws_glue_resources_for_beginners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/yhb4l6/spark_and_aws_glue_resources_for_beginners/", "subreddit_subscribers": 78372, "created_utc": 1667126106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1dlki6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A polite request to review my resume and suggest edits. Thank you!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_yh6ofu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.45, "author_flair_background_color": "transparent", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Resume Review", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/etk0iFEisGBSzL-bULvtPih1VerEjEY9AiPwS9VKYYI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1667111604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/81hb6gzz0ww91.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/81hb6gzz0ww91.jpg?auto=webp&amp;s=62c8ea16967053a04938e69ca4b62d252b961c51", "width": 2550, "height": 3300}, "resolutions": [{"url": "https://preview.redd.it/81hb6gzz0ww91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=16ed5d8b0d981eceda5236b2be15d64ca33e332d", "width": 108, "height": 139}, {"url": "https://preview.redd.it/81hb6gzz0ww91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5ff862834aa1277f980d26a14e0ac505473673a", "width": 216, "height": 279}, {"url": "https://preview.redd.it/81hb6gzz0ww91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=112a1b48b433bd551299dab58ca65e6788b751e7", "width": 320, "height": 414}, {"url": "https://preview.redd.it/81hb6gzz0ww91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=14c84ad5fac3aa0ac640910803c81bcf515f8d7f", "width": 640, "height": 828}, {"url": "https://preview.redd.it/81hb6gzz0ww91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bdbe6f17f571d1e99d18f659f0c9a9036300bb4", "width": 960, "height": 1242}, {"url": "https://preview.redd.it/81hb6gzz0ww91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49a9ca8d0c2517f7d4988ebdb85db4b915dab79c", "width": 1080, "height": 1397}], "variants": {}, "id": "hYjaTXNHm7CnINbNfxWoGI5WMAdf9mM2p89m4r3QH0o"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "47fd10c4-3440-11ed-99b0-ce1be0dd6276", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#007373", "id": "yh6ofu", "is_robot_indexable": true, "report_reasons": null, "author": "musicplay313", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/yh6ofu/a_polite_request_to_review_my_resume_and_suggest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/81hb6gzz0ww91.jpg", "subreddit_subscribers": 78372, "created_utc": 1667111604.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}