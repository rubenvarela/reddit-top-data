{"kind": "Listing", "data": {"after": "t3_y61vbe", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "the youtube video for the  How to Win a Data Science Competition Learn from Top Kagglers has been taken down as the youtube channel was also deleted, i assume that one of the reasons is due to them being russian which is why their course was also taken down from coursera. I was at the part wherein they talked about their solutions in past competitions which is the final hour i'd say and then the channel got deleted. So if it is ok and if it is possible please give a link of the video as i'd like to finish it\n\nEDIT: i know that the videos are also in bilibili but they don't have english subtitles like in youtube and i'm having a hard time understanding some of the words they're saying due to their accent", "author_fullname": "t2_7bstgl3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone have a copy of the How to Win a Data Science Competition Learn from Top Kagglers video from youtube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5g0hh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 102, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 102, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665924515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;the youtube video for the  How to Win a Data Science Competition Learn from Top Kagglers has been taken down as the youtube channel was also deleted, i assume that one of the reasons is due to them being russian which is why their course was also taken down from coursera. I was at the part wherein they talked about their solutions in past competitions which is the final hour i&amp;#39;d say and then the channel got deleted. So if it is ok and if it is possible please give a link of the video as i&amp;#39;d like to finish it&lt;/p&gt;\n\n&lt;p&gt;EDIT: i know that the videos are also in bilibili but they don&amp;#39;t have english subtitles like in youtube and i&amp;#39;m having a hard time understanding some of the words they&amp;#39;re saying due to their accent&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5g0hh", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible_Squirrel5", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5g0hh/does_anyone_have_a_copy_of_the_how_to_win_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5g0hh/does_anyone_have_a_copy_of_the_how_to_win_a_data/", "subreddit_subscribers": 813945, "created_utc": 1665924515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "It looks like these jobs are very rare and they require a great amount of experience and do not pay well.\n\nI'm commencing a degree in data science but I'm also of a strong anti-capitalist mindset and could not care less about most of what data science seems to be about when it comes to job opportunities, i.e.  \"maximising profit\", often without any ethical considerations and to the detriment of many people. In fact, know I would be fired quickly in any job of that type.\n\nI wonder if I should reconsider my degree and maybe study something else.", "author_fullname": "t2_54lytnyf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does data science look like in the not-for-profit sector and are there any opportunities really?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_y5w2bp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 108, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 0, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": "", "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 108, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665966743.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665965239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like these jobs are very rare and they require a great amount of experience and do not pay well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m commencing a degree in data science but I&amp;#39;m also of a strong anti-capitalist mindset and could not care less about most of what data science seems to be about when it comes to job opportunities, i.e.  &amp;quot;maximising profit&amp;quot;, often without any ethical considerations and to the detriment of many people. In fact, know I would be fired quickly in any job of that type.&lt;/p&gt;\n\n&lt;p&gt;I wonder if I should reconsider my degree and maybe study something else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 125, "id": "award_5f123e3d-4f48-42f4-9c11-e98b566d5897", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "When you come across a feel-good thing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Wholesome", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=16&amp;height=16&amp;auto=webp&amp;s=92932f465d58e4c16b12b6eac4ca07d27e3d11c0", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=32&amp;height=32&amp;auto=webp&amp;s=d11484a208d68a318bf9d4fcf371171a1cb6a7ef", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=48&amp;height=48&amp;auto=webp&amp;s=febdf28b6f39f7da7eb1365325b85e0bb49a9f63", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=64&amp;height=64&amp;auto=webp&amp;s=b4406a2d88bf86fa3dc8a45aacf7e0c7bdccc4fb", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png?width=128&amp;height=128&amp;auto=webp&amp;s=19555b13e3e196b62eeb9160d1ac1d1b372dcb0b", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5izbv4fn0md41_Wholesome.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "call_to_action": "", "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5w2bp", "is_robot_indexable": true, "report_reasons": null, "author": "al0678", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5w2bp/what_does_data_science_look_like_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5w2bp/what_does_data_science_look_like_in_the/", "subreddit_subscribers": 813945, "created_utc": 1665965239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have recently been offered a high level position at a startup that is very AI focused. They are focused on mainly defense projects (which is great for me as it is my domain). Most of their funding has come from SIBR projects. I am worried as they are not well known and reading over their website is like a buzzword bingo for AI/ML. What are some questions I should be asking before I make a decision? Thanks", "author_fullname": "t2_xihvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some red flags for a start up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5yf52", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665971990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently been offered a high level position at a startup that is very AI focused. They are focused on mainly defense projects (which is great for me as it is my domain). Most of their funding has come from SIBR projects. I am worried as they are not well known and reading over their website is like a buzzword bingo for AI/ML. What are some questions I should be asking before I make a decision? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5yf52", "is_robot_indexable": true, "report_reasons": null, "author": "elways_love_child", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5yf52/what_are_some_red_flags_for_a_start_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5yf52/what_are_some_red_flags_for_a_start_up/", "subreddit_subscribers": 813945, "created_utc": 1665971990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Id assume that if you wanted to predict on certain features, youd train the model using only those - however the instructions are explicit. Is there a standard way of dealing with missing features like this? \n\nEg: features for model trained on x1, x2, x3, x4, x5\nFeatures for data to be predicted on: x1, x2, x3", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have an assignment to train a model with specific features, then use it to predict on a dataset containing a subset of the features, is this normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5lodf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665939210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Id assume that if you wanted to predict on certain features, youd train the model using only those - however the instructions are explicit. Is there a standard way of dealing with missing features like this? &lt;/p&gt;\n\n&lt;p&gt;Eg: features for model trained on x1, x2, x3, x4, x5\nFeatures for data to be predicted on: x1, x2, x3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5lodf", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5lodf/i_have_an_assignment_to_train_a_model_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5lodf/i_have_an_assignment_to_train_a_model_with/", "subreddit_subscribers": 813945, "created_utc": 1665939210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "?", "author_fullname": "t2_4xhvqofl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you have ever experienced imposter syndrome, what's something that happened recently that made you feel like a data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5u010", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665959678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5u010", "is_robot_indexable": true, "report_reasons": null, "author": "gengarvibes", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5u010/if_you_have_ever_experienced_imposter_syndrome/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5u010/if_you_have_ever_experienced_imposter_syndrome/", "subreddit_subscribers": 813945, "created_utc": 1665959678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm missing something\n\nThe graph below has been used to show the increase in # of deaths in 2022.\n\nAccording to the bottom line \"Excess is calculated by subtracting deaths from the baseline (...) which is calculated as pre pandemic average of 2015-2019 adjusted for the linear trend\", so shouldn't be meaning that deaths are decreasing and not increasing?\n\nIf I take the baseline and subtract the number of deahts, and deaths are decreasing the number should increase, doesn't it?\n\nHowever, the points on the graph, are indicated as \"excess mortality: x deaths/100k\"\n\nWhat am I missing?\n\nhttps://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;format=png&amp;auto=webp&amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f", "author_fullname": "t2_tf2wdkgw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am missing something, can you help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mxkd8uwe98u91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46b6741bb7d7ff8de7b33cec3692aaec229e741a"}, {"y": 126, "x": 216, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3506915f02bde4e4754fb55903c0e3b6d9917278"}, {"y": 188, "x": 320, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2c0aada9923cc58b9709b40e72f88dc39f87d98"}, {"y": 376, "x": 640, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=15c970f2dd6c1c46fb9f7b76ce56284fca34d578"}, {"y": 564, "x": 960, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2267478557105d11c8ba79dd9027250313656d38"}, {"y": 634, "x": 1080, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=543f8de4ca23392a81d7fd8897a643575e9b5577"}], "s": {"y": 1378, "x": 2344, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;format=png&amp;auto=webp&amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f"}, "id": "mxkd8uwe98u91"}}, "name": "t3_y5r84x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6LirDcn9aXEImo_ytqo0fMNQkp3iwkdn4Jhn7Mavw0s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665952696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m missing something&lt;/p&gt;\n\n&lt;p&gt;The graph below has been used to show the increase in # of deaths in 2022.&lt;/p&gt;\n\n&lt;p&gt;According to the bottom line &amp;quot;Excess is calculated by subtracting deaths from the baseline (...) which is calculated as pre pandemic average of 2015-2019 adjusted for the linear trend&amp;quot;, so shouldn&amp;#39;t be meaning that deaths are decreasing and not increasing?&lt;/p&gt;\n\n&lt;p&gt;If I take the baseline and subtract the number of deahts, and deaths are decreasing the number should increase, doesn&amp;#39;t it?&lt;/p&gt;\n\n&lt;p&gt;However, the points on the graph, are indicated as &amp;quot;excess mortality: x deaths/100k&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;What am I missing?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f\"&gt;https://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5r84x", "is_robot_indexable": true, "report_reasons": null, "author": "Catmageddon_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5r84x/i_am_missing_something_can_you_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5r84x/i_am_missing_something_can_you_help/", "subreddit_subscribers": 813945, "created_utc": 1665952696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to generate a gibberish language to use for character dialogue in a game, similar to the 'Animalese' of Animal Crossing. \n\n\nAt the moment, I am hashing the English input sentences, truncating the hash to match the input sentence length, and then mapping each hex character of the hash to an arbitrary syllable (which is associated with an audio clip). While this works well enough, I am hoping to add some realism using universal sentence embeddings, so that similar sentence meanings map to similar gibberish forms and it hopefully seems more like a real language. I need the following features:\n\n1. Sentences with similar meanings should have a similar gibberish output. For example, if the sentence 'hi there' maps to 'grebgol', the sentence 'hello!' should be likely to translate to 'grebgol' as well, or a string similar to it. \n\n2. A gibberish sentence should have approximately the same number of syllables as the input English sentence. e.g. \"Look!\" might generate \"Shleg!\" but \"This path is dangerous\" might generate \"Vnepsweg zeb sirugkijun\". \n\n\nSo far, my code is generating a sentence embedding for each input sentence using Tensorflow Universal Sentence Encoder. But I am not sure how to go about generating a gibberish string from the embedding. I suppose I need some kind of dimensionality reduction like PCA analysis to obtain a set of one dimensional vectors from my list of input sentences. Then I can map the contents of each vector arbitrarily to my syllable recordings. But then my gibberish sentences will all be the same length - this is where I'm stuck.", "author_fullname": "t2_154hkh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generating gibberish game dialogue from universal sentence encodings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5yskh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665973037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to generate a gibberish language to use for character dialogue in a game, similar to the &amp;#39;Animalese&amp;#39; of Animal Crossing. &lt;/p&gt;\n\n&lt;p&gt;At the moment, I am hashing the English input sentences, truncating the hash to match the input sentence length, and then mapping each hex character of the hash to an arbitrary syllable (which is associated with an audio clip). While this works well enough, I am hoping to add some realism using universal sentence embeddings, so that similar sentence meanings map to similar gibberish forms and it hopefully seems more like a real language. I need the following features:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Sentences with similar meanings should have a similar gibberish output. For example, if the sentence &amp;#39;hi there&amp;#39; maps to &amp;#39;grebgol&amp;#39;, the sentence &amp;#39;hello!&amp;#39; should be likely to translate to &amp;#39;grebgol&amp;#39; as well, or a string similar to it. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A gibberish sentence should have approximately the same number of syllables as the input English sentence. e.g. &amp;quot;Look!&amp;quot; might generate &amp;quot;Shleg!&amp;quot; but &amp;quot;This path is dangerous&amp;quot; might generate &amp;quot;Vnepsweg zeb sirugkijun&amp;quot;. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So far, my code is generating a sentence embedding for each input sentence using Tensorflow Universal Sentence Encoder. But I am not sure how to go about generating a gibberish string from the embedding. I suppose I need some kind of dimensionality reduction like PCA analysis to obtain a set of one dimensional vectors from my list of input sentences. Then I can map the contents of each vector arbitrarily to my syllable recordings. But then my gibberish sentences will all be the same length - this is where I&amp;#39;m stuck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5yskh", "is_robot_indexable": true, "report_reasons": null, "author": "zl1asdfg", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5yskh/generating_gibberish_game_dialogue_from_universal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5yskh/generating_gibberish_game_dialogue_from_universal/", "subreddit_subscribers": 813945, "created_utc": 1665973037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've found I generally rely on business knowledge/ metrics when solving problems at work. Which is needed, but sometimes I feel too challenged when I see a \"new\" kind of problem. I think I probably felt challenged in that manner when I was early on in my career.\n\nAre there some resources available to practise this skill?\nOne way to do this are those case studies (like the ones typically asked in consulting), but otherwise I'm at a loss as to what ways to improve this. Or maybe strategies to help break down an ambiguous problem, where / how can I practise this?", "author_fullname": "t2_1fvpf5a1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for practising problem solving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5s9ns", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665955260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found I generally rely on business knowledge/ metrics when solving problems at work. Which is needed, but sometimes I feel too challenged when I see a &amp;quot;new&amp;quot; kind of problem. I think I probably felt challenged in that manner when I was early on in my career.&lt;/p&gt;\n\n&lt;p&gt;Are there some resources available to practise this skill?\nOne way to do this are those case studies (like the ones typically asked in consulting), but otherwise I&amp;#39;m at a loss as to what ways to improve this. Or maybe strategies to help break down an ambiguous problem, where / how can I practise this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5s9ns", "is_robot_indexable": true, "report_reasons": null, "author": "WillingAstronomer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5s9ns/resources_for_practising_problem_solving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5s9ns/resources_for_practising_problem_solving/", "subreddit_subscribers": 813945, "created_utc": 1665955260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm wondering if traditional statistics/EDA/visualization really scales to \"big data\" and whether it is really useful to process an entire \"big dataset\" for EDA vs just sub-sampling.\n\nI realize of course that deep learning **DOES** scale to big data, **and I'm in no way doubting that**. I'm just asking what other data science technique **ALSO** scale to big data (e.g. billions of samples)?\n\nP.S. I'm also asking in the context of map-reduce. That design paradigm seems to imply that data-processing on that scale is useful outside of the context of Deep learning.   \nI'm wondering if I could get practical examples where that is obviously true (preferably in EDA/statistics but other examples are welcome as well).", "author_fullname": "t2_qrw52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EDA on \"big data\": is it sufficient to just sub-sample?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5pi8e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665948483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if traditional statistics/EDA/visualization really scales to &amp;quot;big data&amp;quot; and whether it is really useful to process an entire &amp;quot;big dataset&amp;quot; for EDA vs just sub-sampling.&lt;/p&gt;\n\n&lt;p&gt;I realize of course that deep learning &lt;strong&gt;DOES&lt;/strong&gt; scale to big data, &lt;strong&gt;and I&amp;#39;m in no way doubting that&lt;/strong&gt;. I&amp;#39;m just asking what other data science technique &lt;strong&gt;ALSO&lt;/strong&gt; scale to big data (e.g. billions of samples)?&lt;/p&gt;\n\n&lt;p&gt;P.S. I&amp;#39;m also asking in the context of map-reduce. That design paradigm seems to imply that data-processing on that scale is useful outside of the context of Deep learning.&lt;br/&gt;\nI&amp;#39;m wondering if I could get practical examples where that is obviously true (preferably in EDA/statistics but other examples are welcome as well).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5pi8e", "is_robot_indexable": true, "report_reasons": null, "author": "Udon_noodles", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5pi8e/eda_on_big_data_is_it_sufficient_to_just_subsample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5pi8e/eda_on_big_data_is_it_sufficient_to_just_subsample/", "subreddit_subscribers": 813945, "created_utc": 1665948483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have given data for users which is right skewed with a long tail, meaning high gmv is driven by few users. Now I have 2 cohorts of users for whom I want to compare gmv distribution. My first instinct was to go for t-test but it has an assumption of normality.  Though I also found I  my readings that if my sample size is large enough (typically &gt; 100) central limit theorem would kick in and the difference in mean should be normally distributed so I should be able to apply t-test on my raw data. \n\nBut there is no literature on effect size calculation if my data is skewed, I am thinking of Cohen's D and since it also assumes normality, perform log normal transformation on my data and perform t test and Cohen's D on that. \n\nFrom my reading transformed t-test p value is applicable for raw data as well but not sure about Cohen's D. \n\nAny guidance on how this kind of analysis is usually done would be really helpful.", "author_fullname": "t2_6ys5mu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Cohen's D valid for effect size on log transformed data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5na1y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665943153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have given data for users which is right skewed with a long tail, meaning high gmv is driven by few users. Now I have 2 cohorts of users for whom I want to compare gmv distribution. My first instinct was to go for t-test but it has an assumption of normality.  Though I also found I  my readings that if my sample size is large enough (typically &amp;gt; 100) central limit theorem would kick in and the difference in mean should be normally distributed so I should be able to apply t-test on my raw data. &lt;/p&gt;\n\n&lt;p&gt;But there is no literature on effect size calculation if my data is skewed, I am thinking of Cohen&amp;#39;s D and since it also assumes normality, perform log normal transformation on my data and perform t test and Cohen&amp;#39;s D on that. &lt;/p&gt;\n\n&lt;p&gt;From my reading transformed t-test p value is applicable for raw data as well but not sure about Cohen&amp;#39;s D. &lt;/p&gt;\n\n&lt;p&gt;Any guidance on how this kind of analysis is usually done would be really helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5na1y", "is_robot_indexable": true, "report_reasons": null, "author": "user19911506", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5na1y/is_cohens_d_valid_for_effect_size_on_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5na1y/is_cohens_d_valid_for_effect_size_on_log/", "subreddit_subscribers": 813945, "created_utc": 1665943153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone. Working on understanding entropy and toying some R code around it. \n\nOne thing Im struggling to understand; when your using entropy to build classification models, why try and minimize entropy instead of maximizing probability? \n\nThe root of the entropy formula is inverse probability, so why minimize this instead of maximizing probability. Is it a pohtato potahto kind of thing? Does it make a difference?", "author_fullname": "t2_ix20cupc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why minimize entropy vs maximize probability when building classification models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5ham7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665928155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. Working on understanding entropy and toying some R code around it. &lt;/p&gt;\n\n&lt;p&gt;One thing Im struggling to understand; when your using entropy to build classification models, why try and minimize entropy instead of maximizing probability? &lt;/p&gt;\n\n&lt;p&gt;The root of the entropy formula is inverse probability, so why minimize this instead of maximizing probability. Is it a pohtato potahto kind of thing? Does it make a difference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5ham7", "is_robot_indexable": true, "report_reasons": null, "author": "crustyporuc", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5ham7/why_minimize_entropy_vs_maximize_probability_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5ham7/why_minimize_entropy_vs_maximize_probability_when/", "subreddit_subscribers": 813945, "created_utc": 1665928155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In my current role (F500 but not BigN), I am a data scientist but most of my day i feel more like a backend engineer. I've implemented a small and simple model that serves a simple purpose, which took me roughly a month to complete. It took another 3 months to wrap it as a microservice not only for the product, but for other products to use if they would like to. It took a while to run the backend, understand their microservice framework, and this is my first time creating one.\n\nI understand being a good backend engineer can't hurt, and if I wanted to, it provides skills if I ever want to pivot to SWE. But I feel like there's a LOT more to learn within the data science field already but now I have to add SWE to that list as well?  \n\nOn the one hand, I understand that DS isn't just building models, cleaning data. It's using data to help the business, regardless of the tools you use. So if microservices and proper deployment gain visibility and actually 'productionize' your model, then it's just something you do.\n\nOn the other hand, my mentor told me that that's why we have many software engineers whose strength is creating microservices and making models available via API. \n\n&amp;#x200B;\n\nWould love to hear any thoughts, and if this is normal among experienced data scientists, please comment. I'm sure some newer data scientists will have their eyes opened since these skills are definitely NOT in those \"data science top 10 tools\"", "author_fullname": "t2_1o8m3t81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle necessary non-DS tasks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y620tn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665982919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my current role (F500 but not BigN), I am a data scientist but most of my day i feel more like a backend engineer. I&amp;#39;ve implemented a small and simple model that serves a simple purpose, which took me roughly a month to complete. It took another 3 months to wrap it as a microservice not only for the product, but for other products to use if they would like to. It took a while to run the backend, understand their microservice framework, and this is my first time creating one.&lt;/p&gt;\n\n&lt;p&gt;I understand being a good backend engineer can&amp;#39;t hurt, and if I wanted to, it provides skills if I ever want to pivot to SWE. But I feel like there&amp;#39;s a LOT more to learn within the data science field already but now I have to add SWE to that list as well?  &lt;/p&gt;\n\n&lt;p&gt;On the one hand, I understand that DS isn&amp;#39;t just building models, cleaning data. It&amp;#39;s using data to help the business, regardless of the tools you use. So if microservices and proper deployment gain visibility and actually &amp;#39;productionize&amp;#39; your model, then it&amp;#39;s just something you do.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, my mentor told me that that&amp;#39;s why we have many software engineers whose strength is creating microservices and making models available via API. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would love to hear any thoughts, and if this is normal among experienced data scientists, please comment. I&amp;#39;m sure some newer data scientists will have their eyes opened since these skills are definitely NOT in those &amp;quot;data science top 10 tools&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y620tn", "is_robot_indexable": true, "report_reasons": null, "author": "Sprayquaza98", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y620tn/how_to_handle_necessary_nonds_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y620tn/how_to_handle_necessary_nonds_tasks/", "subreddit_subscribers": 813945, "created_utc": 1665982919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weekly Entering &amp; Transitioning - Thread 17 Oct, 2022 - 24 Oct, 2022", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y60vf8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665979270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome to this week&amp;#39;s entering &amp;amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learning resources (e.g. books, tutorials, videos)&lt;/li&gt;\n&lt;li&gt;Traditional education (e.g. schools, degrees, electives)&lt;/li&gt;\n&lt;li&gt;Alternative education (e.g. online courses, bootcamps)&lt;/li&gt;\n&lt;li&gt;Job search questions (e.g. resumes, applying, career prospects)&lt;/li&gt;\n&lt;li&gt;Elementary questions (e.g. where to start, what next)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;While you wait for answers from the community, check out the &lt;a href=\"https://www.reddit.com/r/datascience/wiki/frequently-asked-questions\"&gt;FAQ&lt;/a&gt; and Resources pages on our wiki. You can also search for answers in &lt;a href=\"https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;amp;restrict_sr=1&amp;amp;sort=new\"&gt;past weekly threads&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "new", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y60vf8", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y60vf8/weekly_entering_transitioning_thread_17_oct_2022/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/datascience/comments/y60vf8/weekly_entering_transitioning_thread_17_oct_2022/", "subreddit_subscribers": 813945, "created_utc": 1665979270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Good afternoon, \n\nI am curious what others do for freelance work/projects related to data. I am currently versed in Excel, SQL &amp; Tableau and I am looking to put these skills to work in order to make some extra cash. Any and all ideas are greatly appreciated. \n\nThanks in advance!", "author_fullname": "t2_9kxnux7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Freelance Work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5wabn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665965864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon, &lt;/p&gt;\n\n&lt;p&gt;I am curious what others do for freelance work/projects related to data. I am currently versed in Excel, SQL &amp;amp; Tableau and I am looking to put these skills to work in order to make some extra cash. Any and all ideas are greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5wabn", "is_robot_indexable": true, "report_reasons": null, "author": "FuelYourEpic", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5wabn/data_freelance_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5wabn/data_freelance_work/", "subreddit_subscribers": 813945, "created_utc": 1665965864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been trying to search for notebooks where people have used ML on kaggle datasets from the self-driving industry.", "author_fullname": "t2_13t60b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kaggle example notebooks from the self-driving /automotive industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5t4d9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665957395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying to search for notebooks where people have used ML on kaggle datasets from the self-driving industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5t4d9", "is_robot_indexable": true, "report_reasons": null, "author": "drugsarebadmky", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5t4d9/kaggle_example_notebooks_from_the_selfdriving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5t4d9/kaggle_example_notebooks_from_the_selfdriving/", "subreddit_subscribers": 813945, "created_utc": 1665957395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q]: Can one apply multiple linear regression to the means of simulations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5oykw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_28xoyq0t", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "statistics", "selftext": "My problem is sort of hard to explain so I used code to demonstrate. I am simulating different temporal sampling methods from time series data. From this I am looking to see how the sampled data can be used to estimate the annual total from 30-minute data. See this example using R:\n\nExample data:\n\n`set.seed(123)`\n\n`df = bind_rows(tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"North\",a = 56, b=96, c=0.68), tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"East\",a = 85, b=46, c=0.25), tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"South\",a = 60, b=100, c=0.99)) %&gt;% mutate(val = abs(val))`\n\nI calculate the annual sum:\n\n`df_totals= df %&gt;% group_by(group) %&gt;% summarise(annual_amount = mean(val)*(48*365))`\n\nThe loop looks something like this:\n\n`for(j in unique(df$group)){`  \n`df_sub &lt;- df %&gt;% filter(group == j)`  \n`qe &lt;- NULL`  \n`me &lt;- NULL`  \n`we &lt;- NULL`  \n`print(j)`  \n`for(k in 1:25){`  \n`print(k)`  \n`qtemp=tibble()`  \n`for(i in unique(quarters(df_sub$date))){`  \n`q &lt;- df_sub[sample(which(quarters(df_sub$date) == i),1,replace = T),]`  \n`qtemp &lt;- bind_rows(qtemp,q)`  \n`print(\"Quarter Data Report\")`  \n`}`  \n`qe[k] = mean(qtemp$val)*17520`  \n`mtemp=tibble()`  \n`for(i in unique(format(df_sub$date,\"%m\"))){`  \n`m &lt;- df_sub[sample(which(format(df_sub$date,\"%m\") == i),1,replace = T),]`  \n`mtemp &lt;- bind_rows(mtemp,m)`  \n`print(\"Monthly Data Report\")`  \n`}`  \n`me[k] = mean(mtemp$val)*17520`  \n`wtemp=tibble()`  \n`for(i in unique(format(df_sub$date,\"%U\"))){`  \n`w &lt;- df_sub[sample(which(format(df_sub$date,\"%U\") == i),1,replace = T),]`  \n`wtemp &lt;- bind_rows(wtemp,w)`  \n`print(\"Weekly Data Report\")`  \n`}`  \n`we[k] = mean(wtemp$val)*17520`  \n`}`  \n`qe_all = qe_all %&gt;% bind_rows(tibble(est = qe, sample = \"Quarterly\", group =unique(qtemp$group), a = unique(qtemp$a), b = unique(qtemp$b), c = unique(qtemp$c)))`   \n`me_all = me_all %&gt;% bind_rows(tibble(est = me, sample = \"Monthly\", group = unique(mtemp$group), a = unique(mtemp$a), b = unique(mtemp$b), c = unique(mtemp$c)))`   \n`we_all = we_all %&gt;% bind_rows(tibble(est = we, sample = \"Weekly\", group = unique(wtemp$group), a = unique(wtemp$a), b = unique(wtemp$b), c = unique(wtemp$c))) rm(qe,me,we)`  \n`}`\n\n`ests &lt;- ests %&gt;% bind_rows(qe_all, me_all, we_all) %&gt;% left_join(df_totals, by = \"group\") %&gt;% mutate(error = est/annual_amount)`\n\nIn reality the error is much more variable between group and a, b, c.\n\nI am trying to understand how the independent variables a, b, and c affect the dependent variable error for each digital simulated sampling method (Quarterly, Monthly, Weekly). So, there will be 3 regression models. Where y is error, with predictors a, b, and c. In my real data, I am simulating 1000+ times. It doesn't seem correct to use 1000+ observations of y. But what if I took the mean of y (error) for each group (North, East, and South).\n\nHere are my questions, for which I would be grateful to get feedback:\n\n* Is multiple linear regression the best approach here?\n* Would a machine learning method be better, such as random forest?\n* For each regression model, is it reasonable to take the mean error for each group and use the mean error as the dependent variable?\n* Is it an issue for dependent variable to be a percentage in the regression (it's not necessarily constrained between 0 and 1)?\n\nIf I were to use the means, data would look like this, and three regression models would be fit by sample:\n\n`ests %&gt;% group_by(sample, group) %&gt;% summarise(a = unique(a), b = unique(b), c = unique(c), error = mean(error))`\n\nApologies for a long-winded question!", "author_fullname": "t2_28xoyq0t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q]: Can one apply multiple linear regression to the means of simulations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/statistics", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4apku", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665798178.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665796362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.statistics", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My problem is sort of hard to explain so I used code to demonstrate. I am simulating different temporal sampling methods from time series data. From this I am looking to see how the sampled data can be used to estimate the annual total from 30-minute data. See this example using R:&lt;/p&gt;\n\n&lt;p&gt;Example data:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;set.seed(123)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;df = bind_rows(tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;North&amp;quot;,a = 56, b=96, c=0.68), tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;East&amp;quot;,a = 85, b=46, c=0.25), tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;South&amp;quot;,a = 60, b=100, c=0.99)) %&amp;gt;% mutate(val = abs(val))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I calculate the annual sum:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;df_totals= df %&amp;gt;% group_by(group) %&amp;gt;% summarise(annual_amount = mean(val)*(48*365))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The loop looks something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;for(j in unique(df$group)){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;df_sub &amp;lt;- df %&amp;gt;% filter(group == j)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(j)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(k in 1:25){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(k)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(quarters(df_sub$date))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;q &amp;lt;- df_sub[sample(which(quarters(df_sub$date) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qtemp &amp;lt;- bind_rows(qtemp,q)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Quarter Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe[k] = mean(qtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;mtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(format(df_sub$date,&amp;quot;%m&amp;quot;))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;m &amp;lt;- df_sub[sample(which(format(df_sub$date,&amp;quot;%m&amp;quot;) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;mtemp &amp;lt;- bind_rows(mtemp,m)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Monthly Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me[k] = mean(mtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;wtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(format(df_sub$date,&amp;quot;%U&amp;quot;))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;w &amp;lt;- df_sub[sample(which(format(df_sub$date,&amp;quot;%U&amp;quot;) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;wtemp &amp;lt;- bind_rows(wtemp,w)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Weekly Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we[k] = mean(wtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe_all = qe_all %&amp;gt;% bind_rows(tibble(est = qe, sample = &amp;quot;Quarterly&amp;quot;, group =unique(qtemp$group), a = unique(qtemp$a), b = unique(qtemp$b), c = unique(qtemp$c)))&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me_all = me_all %&amp;gt;% bind_rows(tibble(est = me, sample = &amp;quot;Monthly&amp;quot;, group = unique(mtemp$group), a = unique(mtemp$a), b = unique(mtemp$b), c = unique(mtemp$c)))&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we_all = we_all %&amp;gt;% bind_rows(tibble(est = we, sample = &amp;quot;Weekly&amp;quot;, group = unique(wtemp$group), a = unique(wtemp$a), b = unique(wtemp$b), c = unique(wtemp$c))) rm(qe,me,we)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ests &amp;lt;- ests %&amp;gt;% bind_rows(qe_all, me_all, we_all) %&amp;gt;% left_join(df_totals, by = &amp;quot;group&amp;quot;) %&amp;gt;% mutate(error = est/annual_amount)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;In reality the error is much more variable between group and a, b, c.&lt;/p&gt;\n\n&lt;p&gt;I am trying to understand how the independent variables a, b, and c affect the dependent variable error for each digital simulated sampling method (Quarterly, Monthly, Weekly). So, there will be 3 regression models. Where y is error, with predictors a, b, and c. In my real data, I am simulating 1000+ times. It doesn&amp;#39;t seem correct to use 1000+ observations of y. But what if I took the mean of y (error) for each group (North, East, and South).&lt;/p&gt;\n\n&lt;p&gt;Here are my questions, for which I would be grateful to get feedback:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is multiple linear regression the best approach here?&lt;/li&gt;\n&lt;li&gt;Would a machine learning method be better, such as random forest?&lt;/li&gt;\n&lt;li&gt;For each regression model, is it reasonable to take the mean error for each group and use the mean error as the dependent variable?&lt;/li&gt;\n&lt;li&gt;Is it an issue for dependent variable to be a percentage in the regression (it&amp;#39;s not necessarily constrained between 0 and 1)?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If I were to use the means, data would look like this, and three regression models would be fit by sample:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ests %&amp;gt;% group_by(sample, group) %&amp;gt;% summarise(a = unique(a), b = unique(b), c = unique(c), error = mean(error))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Apologies for a long-winded question!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qhfi", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y4apku", "is_robot_indexable": true, "report_reasons": null, "author": "squishytea", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "subreddit_subscribers": 521889, "created_utc": 1665796362.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1665947169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.statistics", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5oykw", "is_robot_indexable": true, "report_reasons": null, "author": "squishytea", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y4apku", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5oykw/q_can_one_apply_multiple_linear_regression_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "subreddit_subscribers": 813945, "created_utc": 1665947169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am going to start uni next year in March and I'm looking to major in finance however I want a second major in data science or statistics. I was wondering whether this would be a good combination for someone looking to find a career in data driven finance markets. Moreover, I would be on scholarship and I don't want to lose my scholarship with bad grades (International student fees are exp). I have no background in computer science (Businees subject O and A'levels with mathematics) so is it easy for a beginner to pick up on data science concepts without grades falling off too much?", "author_fullname": "t2_43lltel8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data science a good major to pair with finance for my bachelors degree?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y67zx3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666004257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am going to start uni next year in March and I&amp;#39;m looking to major in finance however I want a second major in data science or statistics. I was wondering whether this would be a good combination for someone looking to find a career in data driven finance markets. Moreover, I would be on scholarship and I don&amp;#39;t want to lose my scholarship with bad grades (International student fees are exp). I have no background in computer science (Businees subject O and A&amp;#39;levels with mathematics) so is it easy for a beginner to pick up on data science concepts without grades falling off too much?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y67zx3", "is_robot_indexable": true, "report_reasons": null, "author": "Mareehaaa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y67zx3/is_data_science_a_good_major_to_pair_with_finance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y67zx3/is_data_science_a_good_major_to_pair_with_finance/", "subreddit_subscribers": 813945, "created_utc": 1666004257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi,  \nI have two tables :  \n\n\n**User** : ID, Creation\\_Date, Email   \n**API\\_Calls** : ID, Creation\\_Date, User (foreign key), type  \n\n\nI want to be able to construct a user retention table :  \n**User\\_Retention** : Week\\_0, Week\\_1, Week\\_2, Week\\_3   \nThat gives the info of how many users make API Calls (of type==B) :  \nWeek\\_0 = the first week they register \n\nWeek\\_1 = the second week after they register\n\nWeek\\_2 = the third week after they register  \n\n\nWe can compare API\\_Calls.Creation\\_Date and User.Creation\\_Date to see when the call was made with respect to the user account creation.\n\nI'm on Postgres.\n\nAnyone could hep? Thank you very much !:)", "author_fullname": "t2_s0rec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complicated User Retention Query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y61jfh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665981364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI have two tables :  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;User&lt;/strong&gt; : ID, Creation_Date, Email&lt;br/&gt;\n&lt;strong&gt;API_Calls&lt;/strong&gt; : ID, Creation_Date, User (foreign key), type  &lt;/p&gt;\n\n&lt;p&gt;I want to be able to construct a user retention table :&lt;br/&gt;\n&lt;strong&gt;User_Retention&lt;/strong&gt; : Week_0, Week_1, Week_2, Week_3&lt;br/&gt;\nThat gives the info of how many users make API Calls (of type==B) :&lt;br/&gt;\nWeek_0 = the first week they register &lt;/p&gt;\n\n&lt;p&gt;Week_1 = the second week after they register&lt;/p&gt;\n\n&lt;p&gt;Week_2 = the third week after they register  &lt;/p&gt;\n\n&lt;p&gt;We can compare API_Calls.Creation_Date and User.Creation_Date to see when the call was made with respect to the user account creation.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on Postgres.&lt;/p&gt;\n\n&lt;p&gt;Anyone could hep? Thank you very much !:)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y61jfh", "is_robot_indexable": true, "report_reasons": null, "author": "swentso", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y61jfh/complicated_user_retention_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y61jfh/complicated_user_retention_query/", "subreddit_subscribers": 813945, "created_utc": 1665981364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "same as title", "author_fullname": "t2_bsfosv4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Blogs/websites related to Data Science /ML/DL to read?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y61d4n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665980785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;same as title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y61d4n", "is_robot_indexable": true, "report_reasons": null, "author": "Adventurous-Grab-20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y61d4n/best_blogswebsites_related_to_data_science_mldl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y61d4n/best_blogswebsites_related_to_data_science_mldl/", "subreddit_subscribers": 813945, "created_utc": 1665980785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Created a repo to show off some of the code I wrote while taking a class in \u201cData analytics and statistical learning\u201d:\n\nhttps://github.com/soly33tworks/ME-PHYS_Undergraduate_Courses/tree/main/EEE485-Statistical_Learning_n_Data_Analytics\n\nNote: All code is written from scratch using NumPy", "author_fullname": "t2_bwichpts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Small Machine Learning Repo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y66ghc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1665999014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Created a repo to show off some of the code I wrote while taking a class in \u201cData analytics and statistical learning\u201d:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/soly33tworks/ME-PHYS_Undergraduate_Courses/tree/main/EEE485-Statistical_Learning_n_Data_Analytics\"&gt;https://github.com/soly33tworks/ME-PHYS_Undergraduate_Courses/tree/main/EEE485-Statistical_Learning_n_Data_Analytics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Note: All code is written from scratch using NumPy&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U0-PY1upHefv8XCrWw1tEbDN2eLZMWjN3tT34J3zCc4.jpg?auto=webp&amp;s=20cdaeaf871e563fd619634c8d4d681a8a06dcbc", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/U0-PY1upHefv8XCrWw1tEbDN2eLZMWjN3tT34J3zCc4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=42e5b732f74dcf82baab0de15b35f4c360c80e0f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/U0-PY1upHefv8XCrWw1tEbDN2eLZMWjN3tT34J3zCc4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=515659b2b46c35c2ac9f265e3affda897fb15bfb", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/U0-PY1upHefv8XCrWw1tEbDN2eLZMWjN3tT34J3zCc4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8d61157865368b19972fb528ffe1db9e725d090", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/U0-PY1upHefv8XCrWw1tEbDN2eLZMWjN3tT34J3zCc4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb96e9ffc01b22c1b596f1e607926f1b3e38de8f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/U0-PY1upHefv8XCrWw1tEbDN2eLZMWjN3tT34J3zCc4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=33601336cd055e58dca862e2f10df687f10099c1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/U0-PY1upHefv8XCrWw1tEbDN2eLZMWjN3tT34J3zCc4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02960d768aa86e95528691da46eb1f1994d3feec", "width": 1080, "height": 540}], "variants": {}, "id": "V9vhA5AV5Wca08NhywdvI1TuZbpiyIjWIS3NoE1Uyd0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y66ghc", "is_robot_indexable": true, "report_reasons": null, "author": "Sufficient-Sea-2274", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y66ghc/small_machine_learning_repo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y66ghc/small_machine_learning_repo/", "subreddit_subscribers": 813945, "created_utc": 1665999014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "hello. I am applying to msds programs right now! I am actually thinking of doing a phd in data science and i was wondering if it actually helps me during the admission process if i mention in my SOP that I plan to do the phd program in the same school. Does it help? Or it has no impact whatsoever? Thank you!", "author_fullname": "t2_a57t1w3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MSDS admission", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y64b3j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665990972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello. I am applying to msds programs right now! I am actually thinking of doing a phd in data science and i was wondering if it actually helps me during the admission process if i mention in my SOP that I plan to do the phd program in the same school. Does it help? Or it has no impact whatsoever? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y64b3j", "is_robot_indexable": true, "report_reasons": null, "author": "mashh7", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y64b3j/msds_admission/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y64b3j/msds_admission/", "subreddit_subscribers": 813945, "created_utc": 1665990972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I got an interview be a software engineer II at a non-profit. I'm posting in this subreddit because a) I trust this community more than r/cscareerquestions and b)  the role is about developing pipelines to process and analyze neuroimaging data, so it's related to data science.\n\nHere are some relevant details about me:\n\n* I have one year of experience processing and analyzing neuroimaging data in a lab\n* I taught myself python, R, frequentist and Bayesian statistics, and implement new computer vision research for my job\n* I lead a group of PhD students to implement (and improve on) image segmentation algorithms\n* I'm published\n* I have a graduate certificate in neuroscience\n* I have two bachelor degrees in philosophy and English respectively (although I've completed math up to and including differential equations)\n\nHere are some relevant details about the position given my research:\n\n* Given my years of experience, Base pay for a software engineer II at this non-profit is $109,765\n* Base pay for a software engineer in the area is $121,987\n* Cost of living is $44,557\n* I have no idea if they're offering bonuses, stock-options, or profit sharing\n\nThe recruiter/hiring manager wants me to name a salary. I haven't even started the first round of interviews yet.\n\nI don't care how much I'm payed so long as I get my foot in the door. However, I worry that if I value myself too little that they'll be turned off.\n\nHow much should I suggest? I'm thinking 75k to 100k with the caveat that I'm open to negotiation.\n\nAlso, is it a bad sign that they're asking me to name a salary?", "author_fullname": "t2_2ve4vyvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interviewing to be a software engineer II; they're asking me to list a salary; what salary should I name?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5ydbx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665974001.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665971838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got an interview be a software engineer II at a non-profit. I&amp;#39;m posting in this subreddit because a) I trust this community more than &lt;a href=\"/r/cscareerquestions\"&gt;r/cscareerquestions&lt;/a&gt; and b)  the role is about developing pipelines to process and analyze neuroimaging data, so it&amp;#39;s related to data science.&lt;/p&gt;\n\n&lt;p&gt;Here are some relevant details about me:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I have one year of experience processing and analyzing neuroimaging data in a lab&lt;/li&gt;\n&lt;li&gt;I taught myself python, R, frequentist and Bayesian statistics, and implement new computer vision research for my job&lt;/li&gt;\n&lt;li&gt;I lead a group of PhD students to implement (and improve on) image segmentation algorithms&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m published&lt;/li&gt;\n&lt;li&gt;I have a graduate certificate in neuroscience&lt;/li&gt;\n&lt;li&gt;I have two bachelor degrees in philosophy and English respectively (although I&amp;#39;ve completed math up to and including differential equations)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here are some relevant details about the position given my research:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Given my years of experience, Base pay for a software engineer II at this non-profit is $109,765&lt;/li&gt;\n&lt;li&gt;Base pay for a software engineer in the area is $121,987&lt;/li&gt;\n&lt;li&gt;Cost of living is $44,557&lt;/li&gt;\n&lt;li&gt;I have no idea if they&amp;#39;re offering bonuses, stock-options, or profit sharing&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The recruiter/hiring manager wants me to name a salary. I haven&amp;#39;t even started the first round of interviews yet.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t care how much I&amp;#39;m payed so long as I get my foot in the door. However, I worry that if I value myself too little that they&amp;#39;ll be turned off.&lt;/p&gt;\n\n&lt;p&gt;How much should I suggest? I&amp;#39;m thinking 75k to 100k with the caveat that I&amp;#39;m open to negotiation.&lt;/p&gt;\n\n&lt;p&gt;Also, is it a bad sign that they&amp;#39;re asking me to name a salary?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5ydbx", "is_robot_indexable": true, "report_reasons": null, "author": "statius9", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5ydbx/interviewing_to_be_a_software_engineer_ii_theyre/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5ydbx/interviewing_to_be_a_software_engineer_ii_theyre/", "subreddit_subscribers": 813945, "created_utc": 1665971838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My goal is to learn \\`hypothesis testing one sample one sided\\`. Honestly, I don't get it.\n\nHypothesis testing one sample RIGHT SIDED\n\n\\`\\`\\`\n\nH0: \u03bc &lt;= x\u0304\n\nH1: \u03bc &gt; x\u0304\n\n\\`\\`\\`\n\nFor example, I have a \\`pg\\_df = pd.DataFrame(\\[10,9,9,10,11,9,8\\])\\`\n\nThe \\`pg\\_df.mean()\\` is 9.4\n\nQuestions:\n\n1. Google have been saying \u03bc is mean population. But why do we compare it with imaginary mean population e.g \\`H0: \u03bc &lt;= 15\\` instead of the actual mean sample e.g \\`H0: \u03bc &lt;= 9.4\\`?\n\n\\`\\`\\`\n\nstatistic, pvalue = stats.ttest\\_1samp(a = pg\\_df, popmean=15, alternative=\"greater\")statistic, pvalue\n\n\\`\\`\\`\n\nThe p-value is 0.9, which is larger than the significance level of 0.05. Thus, we fail to reject null hypothesis. Which we can conclude that the mean population is lower than 15.\n\n1. If we fail to reject \\`H0: : \u03bc &lt;= 15\\`, is the conclusion \"the mean population is lower than 15\"?", "author_fullname": "t2_5yye765o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is hypothesis testing one sample?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5xp0d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665970081.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665969829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My goal is to learn `hypothesis testing one sample one sided`. Honestly, I don&amp;#39;t get it.&lt;/p&gt;\n\n&lt;p&gt;Hypothesis testing one sample RIGHT SIDED&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;H0: \u03bc &amp;lt;= x\u0304&lt;/p&gt;\n\n&lt;p&gt;H1: \u03bc &amp;gt; x\u0304&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;For example, I have a `pg_df = pd.DataFrame([10,9,9,10,11,9,8])`&lt;/p&gt;\n\n&lt;p&gt;The `pg_df.mean()` is 9.4&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Google have been saying \u03bc is mean population. But why do we compare it with imaginary mean population e.g `H0: \u03bc &amp;lt;= 15` instead of the actual mean sample e.g `H0: \u03bc &amp;lt;= 9.4`?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;statistic, pvalue = stats.ttest_1samp(a = pg_df, popmean=15, alternative=&amp;quot;greater&amp;quot;)statistic, pvalue&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;The p-value is 0.9, which is larger than the significance level of 0.05. Thus, we fail to reject null hypothesis. Which we can conclude that the mean population is lower than 15.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If we fail to reject `H0: : \u03bc &amp;lt;= 15`, is the conclusion &amp;quot;the mean population is lower than 15&amp;quot;?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5xp0d", "is_robot_indexable": true, "report_reasons": null, "author": "kidfromtheast", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5xp0d/what_is_hypothesis_testing_one_sample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5xp0d/what_is_hypothesis_testing_one_sample/", "subreddit_subscribers": 813945, "created_utc": 1665969829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been in industry for about 2 years as a supply chain grad working in data analytics roles. My current employed will pick up the bill for me to get a masters in DS, and a lot of people on here have recommended GT's course.\n\nI graduated undergrad with a 3.2, little to no coding experience. I've reached out to their admissions people and haven't gotten a reply back. Is there any hope for me to get in the program? Is prior coding experience needed for GT's course/or really any good DS master's course?\n\nWould greatly appreciate some feedback. Thanks.", "author_fullname": "t2_wpkj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone know how difficult it is to get into Georgia Techs online DS masters program?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5tivn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665958435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been in industry for about 2 years as a supply chain grad working in data analytics roles. My current employed will pick up the bill for me to get a masters in DS, and a lot of people on here have recommended GT&amp;#39;s course.&lt;/p&gt;\n\n&lt;p&gt;I graduated undergrad with a 3.2, little to no coding experience. I&amp;#39;ve reached out to their admissions people and haven&amp;#39;t gotten a reply back. Is there any hope for me to get in the program? Is prior coding experience needed for GT&amp;#39;s course/or really any good DS master&amp;#39;s course?&lt;/p&gt;\n\n&lt;p&gt;Would greatly appreciate some feedback. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5tivn", "is_robot_indexable": true, "report_reasons": null, "author": "positive_being", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5tivn/does_anyone_know_how_difficult_it_is_to_get_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5tivn/does_anyone_know_how_difficult_it_is_to_get_into/", "subreddit_subscribers": 813945, "created_utc": 1665958435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Borders Are Open \u2014 Are You Ready?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "name": "t3_y61vbe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_dt6ya2pz", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Mehey5s7r1Gnys-T1DKhdXSOF6ooDOn0voI7VYrIFiw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "u_addlerkuhn", "selftext": "Do you remember the last time you were on vacation before the pandemic? Do you recall how easy it was to travel without any restrictions? For two years, [COVID-19](https://www.linkedin.com/posts/cubeware_covid19impact-dataanalytics-advancedanalytics-activity-6902900882808152064-zW0z?utm_source=linkedin_share&amp;utm_medium=member_desktop_web) and lockdowns tied us to our homes. The thought of leaving our houses, let alone traveling abroad, was out of the question. \n\nhttps://preview.redd.it/j6yafqbnoau91.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=71959a81f8d3a72617e94c95d4cf6dfbac2139ef\n\nAccording to the World Tourism Organization (UNWTO), tourism worldwide had one of its worst years in history in [2020](https://www.unwto.org/news/2020-worst-year-in-tourism-history-with-1-billion-fewer-international-arrivals), with foreign visitors down by 74%, revenue deficits projected at $1.1 trillion, and over 100 million tourism employments in jeopardy.\n\nBut now, after a long two-year wait, we have finally reached a point where borders for most countries have reopened! Given the giant losses faced by the tourism industry, however, there are a lot of changes to be made. In order for the industry to bounce back, it\u2019ll no longer be sufficient to rely on just the reopening of borders, visitor arrivals, and hotel bookings.\n\nIn such a data-rich environment, the industry has started to leverage the various types of [big data](https://www.linkedin.com/pulse/big-data-bigger-insights-cubeware-gmbh/) that flow in, consistently. Employing [data analytics](http://www.cubeware.com/advanced-analytics) and Business Intelligence (BI) tools and softwares, tourism-related companies can collect and analyze vital data \u2014 such as, current and past flight details, hotel stays, restaurant visits, and more \u2014 to understand how they can improve their services to keep tourists happy.\n\nSome of the ways in which data analytics helps boost the tourism industry \u2014 especially now \u2014 include **personalized marketing, sustainable tourism, and smart hotels.**\n\nTo learn more about tourism and data, click here: [Borders Are Open \u2014 Are You Ready?](https://www.linkedin.com/pulse/borders-open-you-ready-cubeware-gmbh/)", "author_fullname": "t2_dt6ya2pz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Borders Are Open \u2014 Are You Ready?", "link_flair_richtext": [], "subreddit_name_prefixed": "u/addlerkuhn", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j6yafqbnoau91": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/j6yafqbnoau91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=22ead08232d9c0f59846c2ac031352854a07e59f"}, {"y": 114, "x": 216, "u": "https://preview.redd.it/j6yafqbnoau91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a24a53e1f1285bc51c33c4a251b2df1ca74854e"}, {"y": 169, "x": 320, "u": "https://preview.redd.it/j6yafqbnoau91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=435291ee65e44998c788a826c71feaf8f5967530"}, {"y": 339, "x": 640, "u": "https://preview.redd.it/j6yafqbnoau91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=755c01094f86c3ce309cec7725bbb7aac72a67f7"}, {"y": 509, "x": 960, "u": "https://preview.redd.it/j6yafqbnoau91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2810be4f5e859a20213676f7182dbb4f83966df5"}, {"y": 572, "x": 1080, "u": "https://preview.redd.it/j6yafqbnoau91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6d27c99375ed3721899ddbc1599811ff0baf0bbe"}], "s": {"y": 679, "x": 1280, "u": "https://preview.redd.it/j6yafqbnoau91.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=71959a81f8d3a72617e94c95d4cf6dfbac2139ef"}, "id": "j6yafqbnoau91"}}, "name": "t3_y61kmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "user", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Mehey5s7r1Gnys-T1DKhdXSOF6ooDOn0voI7VYrIFiw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1665981471.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.addlerkuhn", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you remember the last time you were on vacation before the pandemic? Do you recall how easy it was to travel without any restrictions? For two years, &lt;a href=\"https://www.linkedin.com/posts/cubeware_covid19impact-dataanalytics-advancedanalytics-activity-6902900882808152064-zW0z?utm_source=linkedin_share&amp;amp;utm_medium=member_desktop_web\"&gt;COVID-19&lt;/a&gt; and lockdowns tied us to our homes. The thought of leaving our houses, let alone traveling abroad, was out of the question. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j6yafqbnoau91.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=71959a81f8d3a72617e94c95d4cf6dfbac2139ef\"&gt;https://preview.redd.it/j6yafqbnoau91.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=71959a81f8d3a72617e94c95d4cf6dfbac2139ef&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;According to the World Tourism Organization (UNWTO), tourism worldwide had one of its worst years in history in &lt;a href=\"https://www.unwto.org/news/2020-worst-year-in-tourism-history-with-1-billion-fewer-international-arrivals\"&gt;2020&lt;/a&gt;, with foreign visitors down by 74%, revenue deficits projected at $1.1 trillion, and over 100 million tourism employments in jeopardy.&lt;/p&gt;\n\n&lt;p&gt;But now, after a long two-year wait, we have finally reached a point where borders for most countries have reopened! Given the giant losses faced by the tourism industry, however, there are a lot of changes to be made. In order for the industry to bounce back, it\u2019ll no longer be sufficient to rely on just the reopening of borders, visitor arrivals, and hotel bookings.&lt;/p&gt;\n\n&lt;p&gt;In such a data-rich environment, the industry has started to leverage the various types of &lt;a href=\"https://www.linkedin.com/pulse/big-data-bigger-insights-cubeware-gmbh/\"&gt;big data&lt;/a&gt; that flow in, consistently. Employing &lt;a href=\"http://www.cubeware.com/advanced-analytics\"&gt;data analytics&lt;/a&gt; and Business Intelligence (BI) tools and softwares, tourism-related companies can collect and analyze vital data \u2014 such as, current and past flight details, hotel stays, restaurant visits, and more \u2014 to understand how they can improve their services to keep tourists happy.&lt;/p&gt;\n\n&lt;p&gt;Some of the ways in which data analytics helps boost the tourism industry \u2014 especially now \u2014 include &lt;strong&gt;personalized marketing, sustainable tourism, and smart hotels.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To learn more about tourism and data, click here: &lt;a href=\"https://www.linkedin.com/pulse/borders-open-you-ready-cubeware-gmbh/\"&gt;Borders Are Open \u2014 Are You Ready?&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "qa", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?auto=webp&amp;s=fc2557820275905bd5ff22a22fe5ba108fa0b5c2", "width": 800, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05150616e30e3a5f4cce25112b123af72121483b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5afe888b39e77f593008e8472cb987dac48eb37", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f24e5ce44707042f97c73af1204b81e0d4308b11", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c08fa7b8ff0f4689f60a5adb8fb4407992618669", "width": 640, "height": 640}], "variants": {}, "id": "J6Vpee6ZGvRnZAouETf55QDz_z3_5nUQ_XFb6t7L6cc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_6uqcib", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y61kmp", "is_robot_indexable": true, "report_reasons": null, "author": "addlerkuhn", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/u_addlerkuhn/comments/y61kmp/borders_are_open_are_you_ready/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/u_addlerkuhn/comments/y61kmp/borders_are_open_are_you_ready/", "subreddit_subscribers": 0, "created_utc": 1665981471.0, "num_crossposts": 5, "media": null, "is_video": false}], "created": 1665982447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.addlerkuhn", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/user/addlerkuhn/comments/y61kmp/borders_are_open_are_you_ready/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?auto=webp&amp;s=fc2557820275905bd5ff22a22fe5ba108fa0b5c2", "width": 800, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=05150616e30e3a5f4cce25112b123af72121483b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5afe888b39e77f593008e8472cb987dac48eb37", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f24e5ce44707042f97c73af1204b81e0d4308b11", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/j07K0hjSwsE55JrjRhdNEjxwOJ1OKBSi8JUIS7f3jMc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c08fa7b8ff0f4689f60a5adb8fb4407992618669", "width": 640, "height": 640}], "variants": {}, "id": "J6Vpee6ZGvRnZAouETf55QDz_z3_5nUQ_XFb6t7L6cc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y61vbe", "is_robot_indexable": true, "report_reasons": null, "author": "addlerkuhn", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y61kmp", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y61vbe/borders_are_open_are_you_ready/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/user/addlerkuhn/comments/y61kmp/borders_are_open_are_you_ready/", "subreddit_subscribers": 813945, "created_utc": 1665982447.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}