{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "the youtube video for the  How to Win a Data Science Competition Learn from Top Kagglers has been taken down as the youtube channel was also deleted, i assume that one of the reasons is due to them being russian which is why their course was also taken down from coursera. I was at the part wherein they talked about their solutions in past competitions which is the final hour i'd say and then the channel got deleted. So if it is ok and if it is possible please give a link of the video as i'd like to finish it\n\nEDIT: i know that the videos are also in bilibili but they don't have english subtitles like in youtube and i'm having a hard time understanding some of the words they're saying due to their accent", "author_fullname": "t2_7bstgl3u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does anyone have a copy of the How to Win a Data Science Competition Learn from Top Kagglers video from youtube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5g0hh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 91, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 91, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665924515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;the youtube video for the  How to Win a Data Science Competition Learn from Top Kagglers has been taken down as the youtube channel was also deleted, i assume that one of the reasons is due to them being russian which is why their course was also taken down from coursera. I was at the part wherein they talked about their solutions in past competitions which is the final hour i&amp;#39;d say and then the channel got deleted. So if it is ok and if it is possible please give a link of the video as i&amp;#39;d like to finish it&lt;/p&gt;\n\n&lt;p&gt;EDIT: i know that the videos are also in bilibili but they don&amp;#39;t have english subtitles like in youtube and i&amp;#39;m having a hard time understanding some of the words they&amp;#39;re saying due to their accent&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5g0hh", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible_Squirrel5", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5g0hh/does_anyone_have_a_copy_of_the_how_to_win_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5g0hh/does_anyone_have_a_copy_of_the_how_to_win_a_data/", "subreddit_subscribers": 813898, "created_utc": 1665924515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_b7z7a6t4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualization of a column of a dataframe where the values are the number of cars driving in a certain area at a given time. Any tips on how to proceed in regards to outliers? Should I remove them? Which method should I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_y5czuz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PZmKJh7GKN5Aqr9AWJl56onaSFdXKisOm-KWFmOEb2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665914384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/23df2xnj45u91.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/23df2xnj45u91.png?auto=webp&amp;s=d1d965d8f1c3e24f8638d5ed654bd7a7834f5f5b", "width": 1439, "height": 578}, "resolutions": [{"url": "https://preview.redd.it/23df2xnj45u91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0abd28d1c4e3c1b6b26d57ba754e08a3be797336", "width": 108, "height": 43}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b63ccfa4adce257b81f3b5b21a11b630b0ecda2", "width": 216, "height": 86}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a3fbf41ca2c100dd5fe2c1d6fbff449b87d5d76", "width": 320, "height": 128}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55c522e2c97ab55db2c878228a3b214cd6df9a3f", "width": 640, "height": 257}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c54c0affbd933536c327a31bc0d88d496adc1dd7", "width": 960, "height": 385}, {"url": "https://preview.redd.it/23df2xnj45u91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0450942eb931bd0485f22732a21cef1a93fd860a", "width": 1080, "height": 433}], "variants": {}, "id": "sU5-iEpPBGh0qA_qLTEEhlH8k0ii3ALKLch77ad5CSI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5czuz", "is_robot_indexable": true, "report_reasons": null, "author": "Outside-Werewolf-983", "discussion_type": null, "num_comments": 73, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5czuz/visualization_of_a_column_of_a_dataframe_where/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/23df2xnj45u91.png", "subreddit_subscribers": 813898, "created_utc": 1665914384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "It looks like these jobs are very rare and they require a great amount of experience and do not pay well.\n\nI'm commencing a degree in data science but I'm also of a strong anti-capitalist mindset and could not care less about most of what data science seems to be about when it comes to job opportunities, i.e.  \"maximising profit\", often without any ethical considerations and to the detriment of many people. In fact, know I would be fired quickly in any job of that type.\n\nI wonder if I should reconsider my degree and maybe study something else.", "author_fullname": "t2_54lytnyf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does data science look like in the not-for-profit sector and are there any opportunities really?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_y5w2bp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 0, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": "", "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665966743.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665965239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It looks like these jobs are very rare and they require a great amount of experience and do not pay well.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m commencing a degree in data science but I&amp;#39;m also of a strong anti-capitalist mindset and could not care less about most of what data science seems to be about when it comes to job opportunities, i.e.  &amp;quot;maximising profit&amp;quot;, often without any ethical considerations and to the detriment of many people. In fact, know I would be fired quickly in any job of that type.&lt;/p&gt;\n\n&lt;p&gt;I wonder if I should reconsider my degree and maybe study something else.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "call_to_action": "", "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5w2bp", "is_robot_indexable": true, "report_reasons": null, "author": "al0678", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5w2bp/what_does_data_science_look_like_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5w2bp/what_does_data_science_look_like_in_the/", "subreddit_subscribers": 813898, "created_utc": 1665965239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Id assume that if you wanted to predict on certain features, youd train the model using only those - however the instructions are explicit. Is there a standard way of dealing with missing features like this? \n\nEg: features for model trained on x1, x2, x3, x4, x5\nFeatures for data to be predicted on: x1, x2, x3", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have an assignment to train a model with specific features, then use it to predict on a dataset containing a subset of the features, is this normal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5lodf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665939210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Id assume that if you wanted to predict on certain features, youd train the model using only those - however the instructions are explicit. Is there a standard way of dealing with missing features like this? &lt;/p&gt;\n\n&lt;p&gt;Eg: features for model trained on x1, x2, x3, x4, x5\nFeatures for data to be predicted on: x1, x2, x3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5lodf", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5lodf/i_have_an_assignment_to_train_a_model_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5lodf/i_have_an_assignment_to_train_a_model_with/", "subreddit_subscribers": 813898, "created_utc": 1665939210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have recently been offered a high level position at a startup that is very AI focused. They are focused on mainly defense projects (which is great for me as it is my domain). Most of their funding has come from SIBR projects. I am worried as they are not well known and reading over their website is like a buzzword bingo for AI/ML. What are some questions I should be asking before I make a decision? Thanks", "author_fullname": "t2_xihvg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some red flags for a start up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y5yf52", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665971990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently been offered a high level position at a startup that is very AI focused. They are focused on mainly defense projects (which is great for me as it is my domain). Most of their funding has come from SIBR projects. I am worried as they are not well known and reading over their website is like a buzzword bingo for AI/ML. What are some questions I should be asking before I make a decision? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5yf52", "is_robot_indexable": true, "report_reasons": null, "author": "elways_love_child", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5yf52/what_are_some_red_flags_for_a_start_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5yf52/what_are_some_red_flags_for_a_start_up/", "subreddit_subscribers": 813898, "created_utc": 1665971990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm missing something\n\nThe graph below has been used to show the increase in # of deaths in 2022.\n\nAccording to the bottom line \"Excess is calculated by subtracting deaths from the baseline (...) which is calculated as pre pandemic average of 2015-2019 adjusted for the linear trend\", so shouldn't be meaning that deaths are decreasing and not increasing?\n\nIf I take the baseline and subtract the number of deahts, and deaths are decreasing the number should increase, doesn't it?\n\nHowever, the points on the graph, are indicated as \"excess mortality: x deaths/100k\"\n\nWhat am I missing?\n\nhttps://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;format=png&amp;auto=webp&amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f", "author_fullname": "t2_tf2wdkgw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am missing something, can you help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mxkd8uwe98u91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46b6741bb7d7ff8de7b33cec3692aaec229e741a"}, {"y": 126, "x": 216, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3506915f02bde4e4754fb55903c0e3b6d9917278"}, {"y": 188, "x": 320, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b2c0aada9923cc58b9709b40e72f88dc39f87d98"}, {"y": 376, "x": 640, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=15c970f2dd6c1c46fb9f7b76ce56284fca34d578"}, {"y": 564, "x": 960, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2267478557105d11c8ba79dd9027250313656d38"}, {"y": 634, "x": 1080, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=543f8de4ca23392a81d7fd8897a643575e9b5577"}], "s": {"y": 1378, "x": 2344, "u": "https://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;format=png&amp;auto=webp&amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f"}, "id": "mxkd8uwe98u91"}}, "name": "t3_y5r84x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6LirDcn9aXEImo_ytqo0fMNQkp3iwkdn4Jhn7Mavw0s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665952696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m missing something&lt;/p&gt;\n\n&lt;p&gt;The graph below has been used to show the increase in # of deaths in 2022.&lt;/p&gt;\n\n&lt;p&gt;According to the bottom line &amp;quot;Excess is calculated by subtracting deaths from the baseline (...) which is calculated as pre pandemic average of 2015-2019 adjusted for the linear trend&amp;quot;, so shouldn&amp;#39;t be meaning that deaths are decreasing and not increasing?&lt;/p&gt;\n\n&lt;p&gt;If I take the baseline and subtract the number of deahts, and deaths are decreasing the number should increase, doesn&amp;#39;t it?&lt;/p&gt;\n\n&lt;p&gt;However, the points on the graph, are indicated as &amp;quot;excess mortality: x deaths/100k&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;What am I missing?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f\"&gt;https://preview.redd.it/mxkd8uwe98u91.png?width=2344&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1c9f3df777c82e0cb429164eea867ef4e21e96f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5r84x", "is_robot_indexable": true, "report_reasons": null, "author": "Catmageddon_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5r84x/i_am_missing_something_can_you_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5r84x/i_am_missing_something_can_you_help/", "subreddit_subscribers": 813898, "created_utc": 1665952696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Good afternoon, \n\nI am curious what others do for freelance work/projects related to data. I am currently versed in Excel, SQL &amp; Tableau and I am looking to put these skills to work in order to make some extra cash. Any and all ideas are greatly appreciated. \n\nThanks in advance!", "author_fullname": "t2_9kxnux7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Freelance Work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5wabn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665965864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon, &lt;/p&gt;\n\n&lt;p&gt;I am curious what others do for freelance work/projects related to data. I am currently versed in Excel, SQL &amp;amp; Tableau and I am looking to put these skills to work in order to make some extra cash. Any and all ideas are greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5wabn", "is_robot_indexable": true, "report_reasons": null, "author": "FuelYourEpic", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5wabn/data_freelance_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5wabn/data_freelance_work/", "subreddit_subscribers": 813898, "created_utc": 1665965864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "?", "author_fullname": "t2_4xhvqofl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you have ever experienced imposter syndrome, what's something that happened recently that made you feel like a data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5u010", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665959678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5u010", "is_robot_indexable": true, "report_reasons": null, "author": "gengarvibes", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5u010/if_you_have_ever_experienced_imposter_syndrome/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5u010/if_you_have_ever_experienced_imposter_syndrome/", "subreddit_subscribers": 813898, "created_utc": 1665959678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have given data for users which is right skewed with a long tail, meaning high gmv is driven by few users. Now I have 2 cohorts of users for whom I want to compare gmv distribution. My first instinct was to go for t-test but it has an assumption of normality.  Though I also found I  my readings that if my sample size is large enough (typically &gt; 100) central limit theorem would kick in and the difference in mean should be normally distributed so I should be able to apply t-test on my raw data. \n\nBut there is no literature on effect size calculation if my data is skewed, I am thinking of Cohen's D and since it also assumes normality, perform log normal transformation on my data and perform t test and Cohen's D on that. \n\nFrom my reading transformed t-test p value is applicable for raw data as well but not sure about Cohen's D. \n\nAny guidance on how this kind of analysis is usually done would be really helpful.", "author_fullname": "t2_6ys5mu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Cohen's D valid for effect size on log transformed data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5na1y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665943153.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have given data for users which is right skewed with a long tail, meaning high gmv is driven by few users. Now I have 2 cohorts of users for whom I want to compare gmv distribution. My first instinct was to go for t-test but it has an assumption of normality.  Though I also found I  my readings that if my sample size is large enough (typically &amp;gt; 100) central limit theorem would kick in and the difference in mean should be normally distributed so I should be able to apply t-test on my raw data. &lt;/p&gt;\n\n&lt;p&gt;But there is no literature on effect size calculation if my data is skewed, I am thinking of Cohen&amp;#39;s D and since it also assumes normality, perform log normal transformation on my data and perform t test and Cohen&amp;#39;s D on that. &lt;/p&gt;\n\n&lt;p&gt;From my reading transformed t-test p value is applicable for raw data as well but not sure about Cohen&amp;#39;s D. &lt;/p&gt;\n\n&lt;p&gt;Any guidance on how this kind of analysis is usually done would be really helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5na1y", "is_robot_indexable": true, "report_reasons": null, "author": "user19911506", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5na1y/is_cohens_d_valid_for_effect_size_on_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5na1y/is_cohens_d_valid_for_effect_size_on_log/", "subreddit_subscribers": 813898, "created_utc": 1665943153.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone. Working on understanding entropy and toying some R code around it. \n\nOne thing Im struggling to understand; when your using entropy to build classification models, why try and minimize entropy instead of maximizing probability? \n\nThe root of the entropy formula is inverse probability, so why minimize this instead of maximizing probability. Is it a pohtato potahto kind of thing? Does it make a difference?", "author_fullname": "t2_ix20cupc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why minimize entropy vs maximize probability when building classification models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5ham7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665928155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. Working on understanding entropy and toying some R code around it. &lt;/p&gt;\n\n&lt;p&gt;One thing Im struggling to understand; when your using entropy to build classification models, why try and minimize entropy instead of maximizing probability? &lt;/p&gt;\n\n&lt;p&gt;The root of the entropy formula is inverse probability, so why minimize this instead of maximizing probability. Is it a pohtato potahto kind of thing? Does it make a difference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5ham7", "is_robot_indexable": true, "report_reasons": null, "author": "crustyporuc", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5ham7/why_minimize_entropy_vs_maximize_probability_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5ham7/why_minimize_entropy_vs_maximize_probability_when/", "subreddit_subscribers": 813898, "created_utc": 1665928155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My goal is to learn \\`hypothesis testing one sample one sided\\`. Honestly, I don't get it.\n\nHypothesis testing one sample RIGHT SIDED\n\n\\`\\`\\`\n\nH0: \u03bc &lt;= x\u0304\n\nH1: \u03bc &gt; x\u0304\n\n\\`\\`\\`\n\nFor example, I have a \\`pg\\_df = pd.DataFrame(\\[10,9,9,10,11,9,8\\])\\`\n\nThe \\`pg\\_df.mean()\\` is 9.4\n\nQuestions:\n\n1. Google have been saying \u03bc is mean population. But why do we compare it with imaginary mean population e.g \\`H0: \u03bc &lt;= 15\\` instead of the actual mean sample e.g \\`H0: \u03bc &lt;= 9.4\\`?\n\n\\`\\`\\`\n\nstatistic, pvalue = stats.ttest\\_1samp(a = pg\\_df, popmean=15, alternative=\"greater\")statistic, pvalue\n\n\\`\\`\\`\n\nThe p-value is 0.9, which is larger than the significance level of 0.05. Thus, we fail to reject null hypothesis. Which we can conclude that the mean population is lower than 15.\n\n1. If we fail to reject \\`H0: : \u03bc &lt;= 15\\`, is the conclusion \"the mean population is lower than 15\"?", "author_fullname": "t2_5yye765o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is hypothesis testing one sample?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5xp0d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665970081.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665969829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My goal is to learn `hypothesis testing one sample one sided`. Honestly, I don&amp;#39;t get it.&lt;/p&gt;\n\n&lt;p&gt;Hypothesis testing one sample RIGHT SIDED&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;H0: \u03bc &amp;lt;= x\u0304&lt;/p&gt;\n\n&lt;p&gt;H1: \u03bc &amp;gt; x\u0304&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;For example, I have a `pg_df = pd.DataFrame([10,9,9,10,11,9,8])`&lt;/p&gt;\n\n&lt;p&gt;The `pg_df.mean()` is 9.4&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Google have been saying \u03bc is mean population. But why do we compare it with imaginary mean population e.g `H0: \u03bc &amp;lt;= 15` instead of the actual mean sample e.g `H0: \u03bc &amp;lt;= 9.4`?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;statistic, pvalue = stats.ttest_1samp(a = pg_df, popmean=15, alternative=&amp;quot;greater&amp;quot;)statistic, pvalue&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;The p-value is 0.9, which is larger than the significance level of 0.05. Thus, we fail to reject null hypothesis. Which we can conclude that the mean population is lower than 15.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If we fail to reject `H0: : \u03bc &amp;lt;= 15`, is the conclusion &amp;quot;the mean population is lower than 15&amp;quot;?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5xp0d", "is_robot_indexable": true, "report_reasons": null, "author": "kidfromtheast", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5xp0d/what_is_hypothesis_testing_one_sample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5xp0d/what_is_hypothesis_testing_one_sample/", "subreddit_subscribers": 813898, "created_utc": 1665969829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been trying to search for notebooks where people have used ML on kaggle datasets from the self-driving industry.", "author_fullname": "t2_13t60b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kaggle example notebooks from the self-driving /automotive industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5t4d9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665957395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying to search for notebooks where people have used ML on kaggle datasets from the self-driving industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5t4d9", "is_robot_indexable": true, "report_reasons": null, "author": "drugsarebadmky", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5t4d9/kaggle_example_notebooks_from_the_selfdriving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5t4d9/kaggle_example_notebooks_from_the_selfdriving/", "subreddit_subscribers": 813898, "created_utc": 1665957395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've found I generally rely on business knowledge/ metrics when solving problems at work. Which is needed, but sometimes I feel too challenged when I see a \"new\" kind of problem. I think I probably felt challenged in that manner when I was early on in my career.\n\nAre there some resources available to practise this skill?\nOne way to do this are those case studies (like the ones typically asked in consulting), but otherwise I'm at a loss as to what ways to improve this. Or maybe strategies to help break down an ambiguous problem, where / how can I practise this?", "author_fullname": "t2_1fvpf5a1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for practising problem solving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5s9ns", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665955260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found I generally rely on business knowledge/ metrics when solving problems at work. Which is needed, but sometimes I feel too challenged when I see a &amp;quot;new&amp;quot; kind of problem. I think I probably felt challenged in that manner when I was early on in my career.&lt;/p&gt;\n\n&lt;p&gt;Are there some resources available to practise this skill?\nOne way to do this are those case studies (like the ones typically asked in consulting), but otherwise I&amp;#39;m at a loss as to what ways to improve this. Or maybe strategies to help break down an ambiguous problem, where / how can I practise this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5s9ns", "is_robot_indexable": true, "report_reasons": null, "author": "WillingAstronomer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5s9ns/resources_for_practising_problem_solving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5s9ns/resources_for_practising_problem_solving/", "subreddit_subscribers": 813898, "created_utc": 1665955260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm wondering if traditional statistics/EDA/visualization really scales to \"big data\" and whether it is really useful to process an entire \"big dataset\" for EDA vs just sub-sampling.\n\nI realize of course that deep learning **DOES** scale to big data, **and I'm in no way doubting that**. I'm just asking what other data science technique **ALSO** scale to big data (e.g. billions of samples)?\n\nP.S. I'm also asking in the context of map-reduce. That design paradigm seems to imply that data-processing on that scale is useful outside of the context of Deep learning.   \nI'm wondering if I could get practical examples where that is obviously true (preferably in EDA/statistics but other examples are welcome as well).", "author_fullname": "t2_qrw52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EDA on \"big data\": is it sufficient to just sub-sample?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5pi8e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665948483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering if traditional statistics/EDA/visualization really scales to &amp;quot;big data&amp;quot; and whether it is really useful to process an entire &amp;quot;big dataset&amp;quot; for EDA vs just sub-sampling.&lt;/p&gt;\n\n&lt;p&gt;I realize of course that deep learning &lt;strong&gt;DOES&lt;/strong&gt; scale to big data, &lt;strong&gt;and I&amp;#39;m in no way doubting that&lt;/strong&gt;. I&amp;#39;m just asking what other data science technique &lt;strong&gt;ALSO&lt;/strong&gt; scale to big data (e.g. billions of samples)?&lt;/p&gt;\n\n&lt;p&gt;P.S. I&amp;#39;m also asking in the context of map-reduce. That design paradigm seems to imply that data-processing on that scale is useful outside of the context of Deep learning.&lt;br/&gt;\nI&amp;#39;m wondering if I could get practical examples where that is obviously true (preferably in EDA/statistics but other examples are welcome as well).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5pi8e", "is_robot_indexable": true, "report_reasons": null, "author": "Udon_noodles", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5pi8e/eda_on_big_data_is_it_sufficient_to_just_subsample/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5pi8e/eda_on_big_data_is_it_sufficient_to_just_subsample/", "subreddit_subscribers": 813898, "created_utc": 1665948483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q]: Can one apply multiple linear regression to the means of simulations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5oykw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_28xoyq0t", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "statistics", "selftext": "My problem is sort of hard to explain so I used code to demonstrate. I am simulating different temporal sampling methods from time series data. From this I am looking to see how the sampled data can be used to estimate the annual total from 30-minute data. See this example using R:\n\nExample data:\n\n`set.seed(123)`\n\n`df = bind_rows(tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"North\",a = 56, b=96, c=0.68), tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"East\",a = 85, b=46, c=0.25), tibble(date =seq(as.POSIXct(\"2022-01-01\", tz = \"UTC\"),as.POSIXct(\"2022-12-31\", tz = \"UTC\"),by = \"30 min\"), val=rnorm((17473))%&gt;% cumsum(.), group = \"South\",a = 60, b=100, c=0.99)) %&gt;% mutate(val = abs(val))`\n\nI calculate the annual sum:\n\n`df_totals= df %&gt;% group_by(group) %&gt;% summarise(annual_amount = mean(val)*(48*365))`\n\nThe loop looks something like this:\n\n`for(j in unique(df$group)){`  \n`df_sub &lt;- df %&gt;% filter(group == j)`  \n`qe &lt;- NULL`  \n`me &lt;- NULL`  \n`we &lt;- NULL`  \n`print(j)`  \n`for(k in 1:25){`  \n`print(k)`  \n`qtemp=tibble()`  \n`for(i in unique(quarters(df_sub$date))){`  \n`q &lt;- df_sub[sample(which(quarters(df_sub$date) == i),1,replace = T),]`  \n`qtemp &lt;- bind_rows(qtemp,q)`  \n`print(\"Quarter Data Report\")`  \n`}`  \n`qe[k] = mean(qtemp$val)*17520`  \n`mtemp=tibble()`  \n`for(i in unique(format(df_sub$date,\"%m\"))){`  \n`m &lt;- df_sub[sample(which(format(df_sub$date,\"%m\") == i),1,replace = T),]`  \n`mtemp &lt;- bind_rows(mtemp,m)`  \n`print(\"Monthly Data Report\")`  \n`}`  \n`me[k] = mean(mtemp$val)*17520`  \n`wtemp=tibble()`  \n`for(i in unique(format(df_sub$date,\"%U\"))){`  \n`w &lt;- df_sub[sample(which(format(df_sub$date,\"%U\") == i),1,replace = T),]`  \n`wtemp &lt;- bind_rows(wtemp,w)`  \n`print(\"Weekly Data Report\")`  \n`}`  \n`we[k] = mean(wtemp$val)*17520`  \n`}`  \n`qe_all = qe_all %&gt;% bind_rows(tibble(est = qe, sample = \"Quarterly\", group =unique(qtemp$group), a = unique(qtemp$a), b = unique(qtemp$b), c = unique(qtemp$c)))`   \n`me_all = me_all %&gt;% bind_rows(tibble(est = me, sample = \"Monthly\", group = unique(mtemp$group), a = unique(mtemp$a), b = unique(mtemp$b), c = unique(mtemp$c)))`   \n`we_all = we_all %&gt;% bind_rows(tibble(est = we, sample = \"Weekly\", group = unique(wtemp$group), a = unique(wtemp$a), b = unique(wtemp$b), c = unique(wtemp$c))) rm(qe,me,we)`  \n`}`\n\n`ests &lt;- ests %&gt;% bind_rows(qe_all, me_all, we_all) %&gt;% left_join(df_totals, by = \"group\") %&gt;% mutate(error = est/annual_amount)`\n\nIn reality the error is much more variable between group and a, b, c.\n\nI am trying to understand how the independent variables a, b, and c affect the dependent variable error for each digital simulated sampling method (Quarterly, Monthly, Weekly). So, there will be 3 regression models. Where y is error, with predictors a, b, and c. In my real data, I am simulating 1000+ times. It doesn't seem correct to use 1000+ observations of y. But what if I took the mean of y (error) for each group (North, East, and South).\n\nHere are my questions, for which I would be grateful to get feedback:\n\n* Is multiple linear regression the best approach here?\n* Would a machine learning method be better, such as random forest?\n* For each regression model, is it reasonable to take the mean error for each group and use the mean error as the dependent variable?\n* Is it an issue for dependent variable to be a percentage in the regression (it's not necessarily constrained between 0 and 1)?\n\nIf I were to use the means, data would look like this, and three regression models would be fit by sample:\n\n`ests %&gt;% group_by(sample, group) %&gt;% summarise(a = unique(a), b = unique(b), c = unique(c), error = mean(error))`\n\nApologies for a long-winded question!", "author_fullname": "t2_28xoyq0t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q]: Can one apply multiple linear regression to the means of simulations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/statistics", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4apku", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665798178.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665796362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.statistics", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My problem is sort of hard to explain so I used code to demonstrate. I am simulating different temporal sampling methods from time series data. From this I am looking to see how the sampled data can be used to estimate the annual total from 30-minute data. See this example using R:&lt;/p&gt;\n\n&lt;p&gt;Example data:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;set.seed(123)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;df = bind_rows(tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;North&amp;quot;,a = 56, b=96, c=0.68), tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;East&amp;quot;,a = 85, b=46, c=0.25), tibble(date =seq(as.POSIXct(&amp;quot;2022-01-01&amp;quot;, tz = &amp;quot;UTC&amp;quot;),as.POSIXct(&amp;quot;2022-12-31&amp;quot;, tz = &amp;quot;UTC&amp;quot;),by = &amp;quot;30 min&amp;quot;), val=rnorm((17473))%&amp;gt;% cumsum(.), group = &amp;quot;South&amp;quot;,a = 60, b=100, c=0.99)) %&amp;gt;% mutate(val = abs(val))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I calculate the annual sum:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;df_totals= df %&amp;gt;% group_by(group) %&amp;gt;% summarise(annual_amount = mean(val)*(48*365))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The loop looks something like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;for(j in unique(df$group)){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;df_sub &amp;lt;- df %&amp;gt;% filter(group == j)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we &amp;lt;- NULL&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(j)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(k in 1:25){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(k)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(quarters(df_sub$date))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;q &amp;lt;- df_sub[sample(which(quarters(df_sub$date) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qtemp &amp;lt;- bind_rows(qtemp,q)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Quarter Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe[k] = mean(qtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;mtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(format(df_sub$date,&amp;quot;%m&amp;quot;))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;m &amp;lt;- df_sub[sample(which(format(df_sub$date,&amp;quot;%m&amp;quot;) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;mtemp &amp;lt;- bind_rows(mtemp,m)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Monthly Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me[k] = mean(mtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;wtemp=tibble()&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;for(i in unique(format(df_sub$date,&amp;quot;%U&amp;quot;))){&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;w &amp;lt;- df_sub[sample(which(format(df_sub$date,&amp;quot;%U&amp;quot;) == i),1,replace = T),]&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;wtemp &amp;lt;- bind_rows(wtemp,w)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;print(&amp;quot;Weekly Data Report&amp;quot;)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we[k] = mean(wtemp$val)*17520&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;qe_all = qe_all %&amp;gt;% bind_rows(tibble(est = qe, sample = &amp;quot;Quarterly&amp;quot;, group =unique(qtemp$group), a = unique(qtemp$a), b = unique(qtemp$b), c = unique(qtemp$c)))&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;me_all = me_all %&amp;gt;% bind_rows(tibble(est = me, sample = &amp;quot;Monthly&amp;quot;, group = unique(mtemp$group), a = unique(mtemp$a), b = unique(mtemp$b), c = unique(mtemp$c)))&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;we_all = we_all %&amp;gt;% bind_rows(tibble(est = we, sample = &amp;quot;Weekly&amp;quot;, group = unique(wtemp$group), a = unique(wtemp$a), b = unique(wtemp$b), c = unique(wtemp$c))) rm(qe,me,we)&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ests &amp;lt;- ests %&amp;gt;% bind_rows(qe_all, me_all, we_all) %&amp;gt;% left_join(df_totals, by = &amp;quot;group&amp;quot;) %&amp;gt;% mutate(error = est/annual_amount)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;In reality the error is much more variable between group and a, b, c.&lt;/p&gt;\n\n&lt;p&gt;I am trying to understand how the independent variables a, b, and c affect the dependent variable error for each digital simulated sampling method (Quarterly, Monthly, Weekly). So, there will be 3 regression models. Where y is error, with predictors a, b, and c. In my real data, I am simulating 1000+ times. It doesn&amp;#39;t seem correct to use 1000+ observations of y. But what if I took the mean of y (error) for each group (North, East, and South).&lt;/p&gt;\n\n&lt;p&gt;Here are my questions, for which I would be grateful to get feedback:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is multiple linear regression the best approach here?&lt;/li&gt;\n&lt;li&gt;Would a machine learning method be better, such as random forest?&lt;/li&gt;\n&lt;li&gt;For each regression model, is it reasonable to take the mean error for each group and use the mean error as the dependent variable?&lt;/li&gt;\n&lt;li&gt;Is it an issue for dependent variable to be a percentage in the regression (it&amp;#39;s not necessarily constrained between 0 and 1)?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If I were to use the means, data would look like this, and three regression models would be fit by sample:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ests %&amp;gt;% group_by(sample, group) %&amp;gt;% summarise(a = unique(a), b = unique(b), c = unique(c), error = mean(error))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Apologies for a long-winded question!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qhfi", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y4apku", "is_robot_indexable": true, "report_reasons": null, "author": "squishytea", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "subreddit_subscribers": 521885, "created_utc": 1665796362.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1665947169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.statistics", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5oykw", "is_robot_indexable": true, "report_reasons": null, "author": "squishytea", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y4apku", "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5oykw/q_can_one_apply_multiple_linear_regression_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/statistics/comments/y4apku/q_can_one_apply_multiple_linear_regression_to_the/", "subreddit_subscribers": 813898, "created_utc": 1665947169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm trying to generate a gibberish language to use for character dialogue in a game, similar to the 'Animalese' of Animal Crossing. \n\n\nAt the moment, I am hashing the English input sentences, truncating the hash to match the input sentence length, and then mapping each hex character of the hash to an arbitrary syllable (which is associated with an audio clip). While this works well enough, I am hoping to add some realism using universal sentence embeddings, so that similar sentence meanings map to similar gibberish forms and it hopefully seems more like a real language. I need the following features:\n\n1. Sentences with similar meanings should have a similar gibberish output. For example, if the sentence 'hi there' maps to 'grebgol', the sentence 'hello!' should be likely to translate to 'grebgol' as well, or a string similar to it. \n\n2. A gibberish sentence should have approximately the same number of syllables as the input English sentence. e.g. \"Look!\" might generate \"Shleg!\" but \"This path is dangerous\" might generate \"Vnepsweg zeb sirugkijun\". \n\n\nSo far, my code is generating a sentence embedding for each input sentence using Tensorflow Universal Sentence Encoder. But I am not sure how to go about generating a gibberish string from the embedding. I suppose I need some kind of dimensionality reduction like PCA analysis to obtain a set of one dimensional vectors from my list of input sentences. Then I can map the contents of each vector arbitrarily to my syllable recordings. But then my gibberish sentences will all be the same length - this is where I'm stuck.", "author_fullname": "t2_154hkh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generating gibberish game dialogue from universal sentence encodings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y5yskh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665973037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to generate a gibberish language to use for character dialogue in a game, similar to the &amp;#39;Animalese&amp;#39; of Animal Crossing. &lt;/p&gt;\n\n&lt;p&gt;At the moment, I am hashing the English input sentences, truncating the hash to match the input sentence length, and then mapping each hex character of the hash to an arbitrary syllable (which is associated with an audio clip). While this works well enough, I am hoping to add some realism using universal sentence embeddings, so that similar sentence meanings map to similar gibberish forms and it hopefully seems more like a real language. I need the following features:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Sentences with similar meanings should have a similar gibberish output. For example, if the sentence &amp;#39;hi there&amp;#39; maps to &amp;#39;grebgol&amp;#39;, the sentence &amp;#39;hello!&amp;#39; should be likely to translate to &amp;#39;grebgol&amp;#39; as well, or a string similar to it. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A gibberish sentence should have approximately the same number of syllables as the input English sentence. e.g. &amp;quot;Look!&amp;quot; might generate &amp;quot;Shleg!&amp;quot; but &amp;quot;This path is dangerous&amp;quot; might generate &amp;quot;Vnepsweg zeb sirugkijun&amp;quot;. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So far, my code is generating a sentence embedding for each input sentence using Tensorflow Universal Sentence Encoder. But I am not sure how to go about generating a gibberish string from the embedding. I suppose I need some kind of dimensionality reduction like PCA analysis to obtain a set of one dimensional vectors from my list of input sentences. Then I can map the contents of each vector arbitrarily to my syllable recordings. But then my gibberish sentences will all be the same length - this is where I&amp;#39;m stuck.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5yskh", "is_robot_indexable": true, "report_reasons": null, "author": "zl1asdfg", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5yskh/generating_gibberish_game_dialogue_from_universal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5yskh/generating_gibberish_game_dialogue_from_universal/", "subreddit_subscribers": 813898, "created_utc": 1665973037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I got an interview be a software engineer II at a non-profit. I'm posting in this subreddit because a) I trust this community more than r/cscareerquestions and b)  the role is about developing pipelines to process and analyze neuroimaging data, so it's related to data science.\n\nHere are some relevant details about me:\n\n* I have one year of experience processing and analyzing neuroimaging data in a lab\n* I taught myself python, R, frequentist and Bayesian statistics, and implement new computer vision research for my job\n* I lead a group of PhD students to implement (and improve on) image segmentation algorithms\n* I'm published\n* I have a graduate certificate in neuroscience\n* I have two bachelor degrees in philosophy and English respectively (although I've completed math up to and including differential equations)\n\nHere are some relevant details about the position given my research:\n\n* Given my years of experience, Base pay for a software engineer II at this non-profit is $109,765\n* Base pay for a software engineer in the area is $121,987\n* Cost of living is $44,557\n* I have no idea if they're offering bonuses, stock-options, or profit sharing\n\nThe recruiter/hiring manager wants me to name a salary. I haven't even started the first round of interviews yet.\n\nI don't care how much I'm payed so long as I get my foot in the door. However, I worry that if I value myself too little that they'll be turned off.\n\nHow much should I suggest? I'm thinking 75k to 100k with the caveat that I'm open to negotiation.\n\nAlso, is it a bad sign that they're asking me to name a salary?", "author_fullname": "t2_2ve4vyvh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interviewing to be a software engineer II; they're asking me to list a salary; what salary should I name?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y5ydbx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665974001.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665971838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got an interview be a software engineer II at a non-profit. I&amp;#39;m posting in this subreddit because a) I trust this community more than &lt;a href=\"/r/cscareerquestions\"&gt;r/cscareerquestions&lt;/a&gt; and b)  the role is about developing pipelines to process and analyze neuroimaging data, so it&amp;#39;s related to data science.&lt;/p&gt;\n\n&lt;p&gt;Here are some relevant details about me:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I have one year of experience processing and analyzing neuroimaging data in a lab&lt;/li&gt;\n&lt;li&gt;I taught myself python, R, frequentist and Bayesian statistics, and implement new computer vision research for my job&lt;/li&gt;\n&lt;li&gt;I lead a group of PhD students to implement (and improve on) image segmentation algorithms&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m published&lt;/li&gt;\n&lt;li&gt;I have a graduate certificate in neuroscience&lt;/li&gt;\n&lt;li&gt;I have two bachelor degrees in philosophy and English respectively (although I&amp;#39;ve completed math up to and including differential equations)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here are some relevant details about the position given my research:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Given my years of experience, Base pay for a software engineer II at this non-profit is $109,765&lt;/li&gt;\n&lt;li&gt;Base pay for a software engineer in the area is $121,987&lt;/li&gt;\n&lt;li&gt;Cost of living is $44,557&lt;/li&gt;\n&lt;li&gt;I have no idea if they&amp;#39;re offering bonuses, stock-options, or profit sharing&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The recruiter/hiring manager wants me to name a salary. I haven&amp;#39;t even started the first round of interviews yet.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t care how much I&amp;#39;m payed so long as I get my foot in the door. However, I worry that if I value myself too little that they&amp;#39;ll be turned off.&lt;/p&gt;\n\n&lt;p&gt;How much should I suggest? I&amp;#39;m thinking 75k to 100k with the caveat that I&amp;#39;m open to negotiation.&lt;/p&gt;\n\n&lt;p&gt;Also, is it a bad sign that they&amp;#39;re asking me to name a salary?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5ydbx", "is_robot_indexable": true, "report_reasons": null, "author": "statius9", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5ydbx/interviewing_to_be_a_software_engineer_ii_theyre/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5ydbx/interviewing_to_be_a_software_engineer_ii_theyre/", "subreddit_subscribers": 813898, "created_utc": 1665971838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been in industry for about 2 years as a supply chain grad working in data analytics roles. My current employed will pick up the bill for me to get a masters in DS, and a lot of people on here have recommended GT's course.\n\nI graduated undergrad with a 3.2, little to no coding experience. I've reached out to their admissions people and haven't gotten a reply back. Is there any hope for me to get in the program? Is prior coding experience needed for GT's course/or really any good DS master's course?\n\nWould greatly appreciate some feedback. Thanks.", "author_fullname": "t2_wpkj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone know how difficult it is to get into Georgia Techs online DS masters program?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5tivn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665958435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been in industry for about 2 years as a supply chain grad working in data analytics roles. My current employed will pick up the bill for me to get a masters in DS, and a lot of people on here have recommended GT&amp;#39;s course.&lt;/p&gt;\n\n&lt;p&gt;I graduated undergrad with a 3.2, little to no coding experience. I&amp;#39;ve reached out to their admissions people and haven&amp;#39;t gotten a reply back. Is there any hope for me to get in the program? Is prior coding experience needed for GT&amp;#39;s course/or really any good DS master&amp;#39;s course?&lt;/p&gt;\n\n&lt;p&gt;Would greatly appreciate some feedback. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5tivn", "is_robot_indexable": true, "report_reasons": null, "author": "positive_being", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5tivn/does_anyone_know_how_difficult_it_is_to_get_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5tivn/does_anyone_know_how_difficult_it_is_to_get_into/", "subreddit_subscribers": 813898, "created_utc": 1665958435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Data Enthusiasts,\n\nWhat kind of projects should i be focusing on or doing more of from a entry level job perspective? Also, what do recruiters mostly look for entry level graduates in data science field?\n\nAny projects ideas would be helpful?", "author_fullname": "t2_bsfosv4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Projects relating to DS/ML?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y58scn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665898781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Data Enthusiasts,&lt;/p&gt;\n\n&lt;p&gt;What kind of projects should i be focusing on or doing more of from a entry level job perspective? Also, what do recruiters mostly look for entry level graduates in data science field?&lt;/p&gt;\n\n&lt;p&gt;Any projects ideas would be helpful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y58scn", "is_robot_indexable": true, "report_reasons": null, "author": "Adventurous-Grab-20", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y58scn/projects_relating_to_dsml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y58scn/projects_relating_to_dsml/", "subreddit_subscribers": 813898, "created_utc": 1665898781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a vague vision about DS, I am studying Python, and finished the challenger track from udacity.\n\nI am in my senior year, and I want to learn the necessary tools to be a data analyst, and then I will move to be a data scientist in the future. my purpose is to work asp, without lacking prior knowledge and experience. I still have some time after graduation.\n\nI found some roadmaps, I want your advice is it good? what should I do while studying? should I study algorithms and data structure? will this give me a better chance? and after all, **how did you manage to get through the mentality of the begging?**  \nThe links:\n\n1.  [GitHub - AhmedUZaki/Eng.-Mohamed-Hammad-AI-Recommendations: The repo contains books, tutorials and resources based on the recommendations of Eng Mohammed Hammad.](https://github.com/AhmedUZaki/Eng.-Mohamed-Hammad-AI-Recommendations) \n2.  [GitHub - datasciencemasters/go: The Open Source Data Science Masters](https://github.com/datasciencemasters/go) \n3.  [data-science-roadmap/README.md at master \u00b7 boringPpl/data-science-roadmap \u00b7 GitHub](https://github.com/boringPpl/data-science-roadmap/blob/master/README.md) \n4.  [medium.com](https://medium.com/@davidventuri/not-a-real-degree-data-science-curriculum-2021-19ba9af2c1d4)\n\nfor some reason, I am inclined to the last one.\n\nThank you very much.", "author_fullname": "t2_d7wwgryw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what's next? for a beginner studying ds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5tm7o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1665958679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a vague vision about DS, I am studying Python, and finished the challenger track from udacity.&lt;/p&gt;\n\n&lt;p&gt;I am in my senior year, and I want to learn the necessary tools to be a data analyst, and then I will move to be a data scientist in the future. my purpose is to work asp, without lacking prior knowledge and experience. I still have some time after graduation.&lt;/p&gt;\n\n&lt;p&gt;I found some roadmaps, I want your advice is it good? what should I do while studying? should I study algorithms and data structure? will this give me a better chance? and after all, &lt;strong&gt;how did you manage to get through the mentality of the begging?&lt;/strong&gt;&lt;br/&gt;\nThe links:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; &lt;a href=\"https://github.com/AhmedUZaki/Eng.-Mohamed-Hammad-AI-Recommendations\"&gt;GitHub - AhmedUZaki/Eng.-Mohamed-Hammad-AI-Recommendations: The repo contains books, tutorials and resources based on the recommendations of Eng Mohammed Hammad.&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt; &lt;a href=\"https://github.com/datasciencemasters/go\"&gt;GitHub - datasciencemasters/go: The Open Source Data Science Masters&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt; &lt;a href=\"https://github.com/boringPpl/data-science-roadmap/blob/master/README.md\"&gt;data-science-roadmap/README.md at master \u00b7 boringPpl/data-science-roadmap \u00b7 GitHub&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt; &lt;a href=\"https://medium.com/@davidventuri/not-a-real-degree-data-science-curriculum-2021-19ba9af2c1d4\"&gt;medium.com&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;for some reason, I am inclined to the last one.&lt;/p&gt;\n\n&lt;p&gt;Thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-u71bMaYwvTQDH4bq-Y5WcHIfydyVHpqi15AoKLqO0g.jpg?auto=webp&amp;s=89355a72512a80fa7ac222e34788a639ac9bb806", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/-u71bMaYwvTQDH4bq-Y5WcHIfydyVHpqi15AoKLqO0g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4aea6c3973e5993f6da97669fda96803442a5fe", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/-u71bMaYwvTQDH4bq-Y5WcHIfydyVHpqi15AoKLqO0g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d09fd99cb313166988b450cc42e4cbb5ef3b697", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/-u71bMaYwvTQDH4bq-Y5WcHIfydyVHpqi15AoKLqO0g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8233f55a0f290d9ef94b45d45605d092ee9a6f5f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/-u71bMaYwvTQDH4bq-Y5WcHIfydyVHpqi15AoKLqO0g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a939375accfc121770e86168e2737b11dc81d055", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/-u71bMaYwvTQDH4bq-Y5WcHIfydyVHpqi15AoKLqO0g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55a1b705cfb510f7beef0c8e4aff3f567f7f080b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/-u71bMaYwvTQDH4bq-Y5WcHIfydyVHpqi15AoKLqO0g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8d7978db0565786d60c453b055f291491113b8f4", "width": 1080, "height": 540}], "variants": {}, "id": "qNFotevTdVDQYlv7TDJ-5lUB1HFJbiuldtccV2eJU80"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5tm7o", "is_robot_indexable": true, "report_reasons": null, "author": "Hamed_AlKhateeb", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5tm7o/whats_next_for_a_beginner_studying_ds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5tm7o/whats_next_for_a_beginner_studying_ds/", "subreddit_subscribers": 813898, "created_utc": 1665958679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi I am looking for opportunities in Data Science field. Any opportunity will be highly appreciated. Please reach out to me on samir_harris3@hotmail.com\n\nhttps://www.linkedin.com/in/emmanuel-h-87312918", "author_fullname": "t2_mcbs6ugc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for DataScience Internship", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5jv0s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665934733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am looking for opportunities in Data Science field. Any opportunity will be highly appreciated. Please reach out to me on &lt;a href=\"mailto:samir_harris3@hotmail.com\"&gt;samir_harris3@hotmail.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/in/emmanuel-h-87312918\"&gt;https://www.linkedin.com/in/emmanuel-h-87312918&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5jv0s", "is_robot_indexable": true, "report_reasons": null, "author": "Necessary-Comfort-89", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5jv0s/looking_for_datascience_internship/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5jv0s/looking_for_datascience_internship/", "subreddit_subscribers": 813898, "created_utc": 1665934733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "First time posting and I need advice please.\nApologies for the bad explanation.. I\u2019m not a data scientist or studying it. \n\n/ / /\n\nMy startup is needing a service that can essentially scan (or scrape) all LinkedIn profiles for cold-outreach campaigns.\n\nWe would want AI to search LinkedIn for our target audience based on 3 primary categories.. \n\nSo we feed the algorithm some data of the idea client we want to email\u2026 \n\n***specific occupation , age  and the person plays an instrument as a hobby***\n\nThen the service can look for people on LinkedIn who match the input criteria. Giving us a simple output data list (name, LinkedIn profile, possible email) \n\n / / /\n\nI am 100% new to this. So sorry I\u2019m advance for poorly explaining this. Just need an efficient way to gather possible leads for cold outreach marketing campaigns. Thanks!", "author_fullname": "t2_24u1i0h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Help!] - Easy Data Scraping LinkedIn Profiles for Lead Generation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y5xryx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.13, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665970073.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First time posting and I need advice please.\nApologies for the bad explanation.. I\u2019m not a data scientist or studying it. &lt;/p&gt;\n\n&lt;p&gt;/ / /&lt;/p&gt;\n\n&lt;p&gt;My startup is needing a service that can essentially scan (or scrape) all LinkedIn profiles for cold-outreach campaigns.&lt;/p&gt;\n\n&lt;p&gt;We would want AI to search LinkedIn for our target audience based on 3 primary categories.. &lt;/p&gt;\n\n&lt;p&gt;So we feed the algorithm some data of the idea client we want to email\u2026 &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;specific occupation , age  and the person plays an instrument as a hobby&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Then the service can look for people on LinkedIn who match the input criteria. Giving us a simple output data list (name, LinkedIn profile, possible email) &lt;/p&gt;\n\n&lt;p&gt;/ / /&lt;/p&gt;\n\n&lt;p&gt;I am 100% new to this. So sorry I\u2019m advance for poorly explaining this. Just need an efficient way to gather possible leads for cold outreach marketing campaigns. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y5xryx", "is_robot_indexable": true, "report_reasons": null, "author": "ProdByBeezi", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y5xryx/help_easy_data_scraping_linkedin_profiles_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y5xryx/help_easy_data_scraping_linkedin_profiles_for/", "subreddit_subscribers": 813898, "created_utc": 1665970073.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}