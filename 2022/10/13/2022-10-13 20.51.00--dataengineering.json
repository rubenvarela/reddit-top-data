{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would say I am still in the early years of being a data engineer and there is so much to learn. But lurking in this subreddit has helped me learn new tools, techniques, best practices, and so much more. So thanks to everyone that contributes in this subreddit. To those that give helpful advice to someone that may have asked the same question 100x. This subreddit has been like a free mentoring program at times!", "author_fullname": "t2_awlj3rif", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thanks to everyone here that contributes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2mdxi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 172, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 172, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665626547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would say I am still in the early years of being a data engineer and there is so much to learn. But lurking in this subreddit has helped me learn new tools, techniques, best practices, and so much more. So thanks to everyone that contributes in this subreddit. To those that give helpful advice to someone that may have asked the same question 100x. This subreddit has been like a free mentoring program at times!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_c42dc561-0b41-40b6-a23d-ef7e110e739e", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=16&amp;height=16&amp;auto=webp&amp;s=18383f4034d47b396b314c033c7d5e9be96df785", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=32&amp;height=32&amp;auto=webp&amp;s=3ed8a934f66150002b39c40f1863f101fba5a8bf", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=48&amp;height=48&amp;auto=webp&amp;s=f10a2d3469f32ce0351da34e234a9955f80f7c1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=64&amp;height=64&amp;auto=webp&amp;s=2e13b802594180e56037df3d51520b43a0462a29", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=128&amp;height=128&amp;auto=webp&amp;s=79d3a9614f27e2d922c52137caeb06f2bdb33cc5", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "So buff, wow", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Buff Doge", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=16&amp;height=16&amp;auto=webp&amp;s=18383f4034d47b396b314c033c7d5e9be96df785", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=32&amp;height=32&amp;auto=webp&amp;s=3ed8a934f66150002b39c40f1863f101fba5a8bf", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=48&amp;height=48&amp;auto=webp&amp;s=f10a2d3469f32ce0351da34e234a9955f80f7c1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=64&amp;height=64&amp;auto=webp&amp;s=2e13b802594180e56037df3d51520b43a0462a29", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png?width=128&amp;height=128&amp;auto=webp&amp;s=79d3a9614f27e2d922c52137caeb06f2bdb33cc5", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/zc4a9vk5zmc51_BuffDoge.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y2mdxi", "is_robot_indexable": true, "report_reasons": null, "author": "darthsatoshious", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2mdxi/thanks_to_everyone_here_that_contributes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2mdxi/thanks_to_everyone_here_that_contributes/", "subreddit_subscribers": 76337, "created_utc": 1665626547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello!\n\nI've been trying to learn about data engineering concepts recently through the help of this subreddit and the data engineering Zoom-Camp. I'm really happy to say I finished putting together my first functioning DE project (really my first project ever :) ) and wanted to share to celebrate/ get feedback!\n\n[Fit-pipe DE Project](https://github.com/rickyriled/data_engineering_project_1)\n\nThe goal of this project was to just get the various technologies I was learning about interconnected, and to pull in/transform some simple data that I found interesting with them -- specifically, my fit-bit heart rate data!\n\nIn short, terraform was used to build a data lake in GCS, and then I scheduled regular batch jobs through a prefect DAG to pull in my fitbit data, transform it with PySpark, and then push the updated data to the cloud. From there I just made a really simple visualization to test if things were working on google data studios.\n\nhttps://preview.redd.it/24wnijf2mit91.png?width=1566&amp;format=png&amp;auto=webp&amp;s=b9b48e19ac442b5070c0fe07e1dd4fb17b20f3d1\n\nUltimately there were a few things I left out due to issues with my local environment/ a lack of computing power; e.g. airflow running in docker was too computationally heavy for my MacBook air, so I switched to prefect; and various python dependency issues held me back from connecting to big query and developing a data warehouse to pull from.\n\nIn the future, I wan't to try and more appropriately use PySpark for data transforming, as I ultimately used very little of what the tool has to offer. Additionally, though I didn't use it, the various difficulties I had setting up my environment taught me the value of docker containers.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI wanted to give a shout out to some of the repos that I found help in/ drew inspiration from too:\n\n[MarcosMJD Global Historical Climatology Pipeline](https://github.com/MarcosMJD/ghcn-d)\n\n[ris-tlp adiophile-e2e-pipeline](https://github.com/ris-tlp/audiophile-e2e-pipeline)\n\n[Data Engineering Zoom Camp](https://github.com/DataTalksClub/data-engineering-zoomcamp)\n\n&amp;#x200B;\n\nCheers!", "author_fullname": "t2_80ytrhvd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Celebrating my first Data Engineering Project -- Fitbit data with PySpark, GCP, prefect, and terraform!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"24wnijf2mit91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/24wnijf2mit91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f5712caf69cf55c9d250ce00a4e197b17435043"}, {"y": 74, "x": 216, "u": "https://preview.redd.it/24wnijf2mit91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=29d38fc531ae4bcb09f80242e0a8ac8dd8e7a16c"}, {"y": 110, "x": 320, "u": "https://preview.redd.it/24wnijf2mit91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed9358c28552d48d02e73ba04589f7ef2b96e1f3"}, {"y": 220, "x": 640, "u": "https://preview.redd.it/24wnijf2mit91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fa32f6a2d6bc4253618c69f49e444f5ae0dddb74"}, {"y": 330, "x": 960, "u": "https://preview.redd.it/24wnijf2mit91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd9372d2f809a7365cba9ad0bb0efb1d96e45d39"}, {"y": 371, "x": 1080, "u": "https://preview.redd.it/24wnijf2mit91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=804e85bf3317afa8072e5bb08800c6ab8a7fa4e6"}], "s": {"y": 539, "x": 1566, "u": "https://preview.redd.it/24wnijf2mit91.png?width=1566&amp;format=png&amp;auto=webp&amp;s=b9b48e19ac442b5070c0fe07e1dd4fb17b20f3d1"}, "id": "24wnijf2mit91"}}, "name": "t3_y2r6ml", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jckCG1C4PU8f16IRNFsRSCqF4KQqZ2Nl5Bv4smqe-So.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1665641787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to learn about data engineering concepts recently through the help of this subreddit and the data engineering Zoom-Camp. I&amp;#39;m really happy to say I finished putting together my first functioning DE project (really my first project ever :) ) and wanted to share to celebrate/ get feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/rickyriled/data_engineering_project_1\"&gt;Fit-pipe DE Project&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The goal of this project was to just get the various technologies I was learning about interconnected, and to pull in/transform some simple data that I found interesting with them -- specifically, my fit-bit heart rate data!&lt;/p&gt;\n\n&lt;p&gt;In short, terraform was used to build a data lake in GCS, and then I scheduled regular batch jobs through a prefect DAG to pull in my fitbit data, transform it with PySpark, and then push the updated data to the cloud. From there I just made a really simple visualization to test if things were working on google data studios.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/24wnijf2mit91.png?width=1566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9b48e19ac442b5070c0fe07e1dd4fb17b20f3d1\"&gt;https://preview.redd.it/24wnijf2mit91.png?width=1566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9b48e19ac442b5070c0fe07e1dd4fb17b20f3d1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ultimately there were a few things I left out due to issues with my local environment/ a lack of computing power; e.g. airflow running in docker was too computationally heavy for my MacBook air, so I switched to prefect; and various python dependency issues held me back from connecting to big query and developing a data warehouse to pull from.&lt;/p&gt;\n\n&lt;p&gt;In the future, I wan&amp;#39;t to try and more appropriately use PySpark for data transforming, as I ultimately used very little of what the tool has to offer. Additionally, though I didn&amp;#39;t use it, the various difficulties I had setting up my environment taught me the value of docker containers.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I wanted to give a shout out to some of the repos that I found help in/ drew inspiration from too:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/MarcosMJD/ghcn-d\"&gt;MarcosMJD Global Historical Climatology Pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ris-tlp/audiophile-e2e-pipeline\"&gt;ris-tlp adiophile-e2e-pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/DataTalksClub/data-engineering-zoomcamp\"&gt;Data Engineering Zoom Camp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JDF6yAYdY4PTWbKQSV-h6q1mraVQLk2JzJ364UN3Osc.jpg?auto=webp&amp;s=528a4416b5ed2c1d6173ff5644fbdb37c02851a7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/JDF6yAYdY4PTWbKQSV-h6q1mraVQLk2JzJ364UN3Osc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db19bce53481c83f2bce3cba33fda915e2c95703", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/JDF6yAYdY4PTWbKQSV-h6q1mraVQLk2JzJ364UN3Osc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3d519c1156551e9c504390c44e3211df2f17188", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/JDF6yAYdY4PTWbKQSV-h6q1mraVQLk2JzJ364UN3Osc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a5af335508ae13100eebf2b5f3accf03ec5f82a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/JDF6yAYdY4PTWbKQSV-h6q1mraVQLk2JzJ364UN3Osc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6822f3675a8d93cb733439e683ef4b900f01088e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/JDF6yAYdY4PTWbKQSV-h6q1mraVQLk2JzJ364UN3Osc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8eaa23731c3f81d927e8a458e87e792574ca9a84", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/JDF6yAYdY4PTWbKQSV-h6q1mraVQLk2JzJ364UN3Osc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=defc5b295ac19762e968551b37ab12a70e8cdda7", "width": 1080, "height": 540}], "variants": {}, "id": "GVQ3ewzXsB4P0x4w8oxxnuiipK13XGHiTzNHkkbrHP4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "y2r6ml", "is_robot_indexable": true, "report_reasons": null, "author": "Particular-Bet-1828", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2r6ml/celebrating_my_first_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2r6ml/celebrating_my_first_data_engineering_project/", "subreddit_subscribers": 76337, "created_utc": 1665641787.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nMore of a conceptual question:\n\nLet's say I have a extract and load pipeline that collect data from a API and loads in a raw table in DW like Big Query. \n\nEverytime the pipeline runs it gets the data from the last day and inserts in a table, sometimes the pipeline could run twice in the same day by mistake (Some analyst could run it manually accidentally) and I wold get duplicated rows for the last day.\n\nI'd like to know if it's ok to check if the key already exists before inserting and then delete the old values, OR if it's better to create some sort of datetime column and when it comes to read the table apply a max(datetime) for guarantee I would get the most updated values.\n\nCan someone help me out with this doubt?\n\n&amp;#x200B;\n\nThanks!!", "author_fullname": "t2_3d6ridh2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract and Load Strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y30uqp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665673031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;More of a conceptual question:&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I have a extract and load pipeline that collect data from a API and loads in a raw table in DW like Big Query. &lt;/p&gt;\n\n&lt;p&gt;Everytime the pipeline runs it gets the data from the last day and inserts in a table, sometimes the pipeline could run twice in the same day by mistake (Some analyst could run it manually accidentally) and I wold get duplicated rows for the last day.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know if it&amp;#39;s ok to check if the key already exists before inserting and then delete the old values, OR if it&amp;#39;s better to create some sort of datetime column and when it comes to read the table apply a max(datetime) for guarantee I would get the most updated values.&lt;/p&gt;\n\n&lt;p&gt;Can someone help me out with this doubt?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y30uqp", "is_robot_indexable": true, "report_reasons": null, "author": "PurpleSir6567", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y30uqp/extract_and_load_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y30uqp/extract_and_load_strategy/", "subreddit_subscribers": 76337, "created_utc": 1665673031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm downloading a bunch of Economic Indicators from the Federal Reserve Economic Data site.\n\nExamples might be Monthly CPI number, Monthly Unemployment rate, Monthly Average 30 Year Mortgage rates, Quarterly unemployment claims, etc.\n\nSome are released weekly, some monthly, quarterly, annually just depending on the metric.\n\nI have been thinking of how to model this data. Either having a MonthlyEconomicIndicators and QuarterlyEconomic Indicators and any other levels of granularity that we're grouping.\n\nOr...\n\nHaving individual data points for each Indicator.\n\nGrouping things together makes sense because it's semi-similar data and would probably be easy to work with. The primary key would be whatever date field were using.\n\nHaving individual tables makes some sense because different data points have different amounts of history. Some may only go back to 2010 but others go to 2000. Also I don't want the table to get too wide so it's too hard finding what you're looking for. Primary key would still be the date.\n\nOtherwise the third format that doesn't seem appealing is one with a column that says the indicator and a column that says the value.\n\nThe data will mostly be used for data Scientists to do Modeling and Analysis.\n\nThoughts?", "author_fullname": "t2_94m29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Economic Indicators - One Table or Many?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y32kzh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665677283.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m downloading a bunch of Economic Indicators from the Federal Reserve Economic Data site.&lt;/p&gt;\n\n&lt;p&gt;Examples might be Monthly CPI number, Monthly Unemployment rate, Monthly Average 30 Year Mortgage rates, Quarterly unemployment claims, etc.&lt;/p&gt;\n\n&lt;p&gt;Some are released weekly, some monthly, quarterly, annually just depending on the metric.&lt;/p&gt;\n\n&lt;p&gt;I have been thinking of how to model this data. Either having a MonthlyEconomicIndicators and QuarterlyEconomic Indicators and any other levels of granularity that we&amp;#39;re grouping.&lt;/p&gt;\n\n&lt;p&gt;Or...&lt;/p&gt;\n\n&lt;p&gt;Having individual data points for each Indicator.&lt;/p&gt;\n\n&lt;p&gt;Grouping things together makes sense because it&amp;#39;s semi-similar data and would probably be easy to work with. The primary key would be whatever date field were using.&lt;/p&gt;\n\n&lt;p&gt;Having individual tables makes some sense because different data points have different amounts of history. Some may only go back to 2010 but others go to 2000. Also I don&amp;#39;t want the table to get too wide so it&amp;#39;s too hard finding what you&amp;#39;re looking for. Primary key would still be the date.&lt;/p&gt;\n\n&lt;p&gt;Otherwise the third format that doesn&amp;#39;t seem appealing is one with a column that says the indicator and a column that says the value.&lt;/p&gt;\n\n&lt;p&gt;The data will mostly be used for data Scientists to do Modeling and Analysis.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y32kzh", "is_robot_indexable": true, "report_reasons": null, "author": "bojanderson", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y32kzh/economic_indicators_one_table_or_many/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y32kzh/economic_indicators_one_table_or_many/", "subreddit_subscribers": 76337, "created_utc": 1665677283.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI'm struggling trying to figure out the best (most efficient) way of running a large Python model using a poetry bash script (**poetry run python file/\\_\\_main\\_\\_.py**) on Azure. The model requires an input of files from a child folder, and then uploads data back into a child folder.\n\nI'm using Prefect, and have deployed the *Agent* onto AKS with *Storage* pointing towards blob storage. \n\nI'm envisioning the following:\n\n1. Extract Data from API and Save to Blob Storage &gt;&gt; Data Factory\n2. Run the model (located in Blob Storage?) &gt;&gt; **???? run bash script to invoke it?**\n3. Save the output files &gt;&gt; Done automatically once model is ran\n4. Upload to Snowflake &gt;&gt; Data Factory/CLI w Prefect? \n\nFirst time dealing with such a cumbersome model, and I'm also a relatively new Data Engineer. Not finding much help in the form of other online resources. Any advice is greatly appreciated :)\n\nBonus points if anyone can recommend a solid resource/book about the aforementioned topics!\n\nThanks so much,\n\nAzazazazaz3", "author_fullname": "t2_8y4c8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying a large (&gt;100gb ram usage) Model on Azure (Prefect, AKS, Blob Storage)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y36o3w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665687232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m struggling trying to figure out the best (most efficient) way of running a large Python model using a poetry bash script (&lt;strong&gt;poetry run python file/__main__.py&lt;/strong&gt;) on Azure. The model requires an input of files from a child folder, and then uploads data back into a child folder.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Prefect, and have deployed the &lt;em&gt;Agent&lt;/em&gt; onto AKS with &lt;em&gt;Storage&lt;/em&gt; pointing towards blob storage. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m envisioning the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Extract Data from API and Save to Blob Storage &amp;gt;&amp;gt; Data Factory&lt;/li&gt;\n&lt;li&gt;Run the model (located in Blob Storage?) &amp;gt;&amp;gt; &lt;strong&gt;???? run bash script to invoke it?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Save the output files &amp;gt;&amp;gt; Done automatically once model is ran&lt;/li&gt;\n&lt;li&gt;Upload to Snowflake &amp;gt;&amp;gt; Data Factory/CLI w Prefect? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;First time dealing with such a cumbersome model, and I&amp;#39;m also a relatively new Data Engineer. Not finding much help in the form of other online resources. Any advice is greatly appreciated :)&lt;/p&gt;\n\n&lt;p&gt;Bonus points if anyone can recommend a solid resource/book about the aforementioned topics!&lt;/p&gt;\n\n&lt;p&gt;Thanks so much,&lt;/p&gt;\n\n&lt;p&gt;Azazazazaz3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y36o3w", "is_robot_indexable": true, "report_reasons": null, "author": "azazazazaz3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y36o3w/deploying_a_large_100gb_ram_usage_model_on_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y36o3w/deploying_a_large_100gb_ram_usage_model_on_azure/", "subreddit_subscribers": 76337, "created_utc": 1665687232.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/DE! We're currently working on a transition from SQL Server/In-house tools to a Databricks implementation and I have some concerns/questions about its direction. Hoping I can get someone else's perspective or further materials to read before I go into battle with the consultants.\n\n \n\nHigh level, we take client's data in, we clean it up, and run analysis on the cleaned up data. The cleaning and analysis portions are a mix of standard and custom processes. We're looking to standardize this into a bronze, silver, gold process where bronze is loaded raw data, the cleanup happens in silver, and the gold layer has the final presentation needed for our analysis. Makes sense in theory, but as we know the devil is in the details. Below are my main concerns about the current proposal:\n\n \n\n**The silver cleanup process can pull directly from clean gold data.**\n\nIn my opinion, this makes a dependency nightmare, and also the whole process non-deterministic. We do need to sometimes reference the output of our measurement in the data cleanup process. I would think in situations like this we would store the output of gold, and then have that then be an input to the bronze layer. This way we would have a set of inputs always equal the same output, and not create a state machine. \n\n \n\n**Data gets loaded to silver and then edited in-place**\n\nIn their proposal we aggregate bronze data, load it into silver, and then run processes that manipulate the tables while they're still in silver. In my opinion, this makes silver data time-dependent. I would think these cleanup processes would be a part of the pipeline from bronze to silver. I understand that the data does have to live somewhere so maybe this is a common practice that I am unfamiliar with. The datasets we're working with are generally 20 to 350 million lines and haven't surpassed 5gb but may in the future.\n\n \n\nAny thoughts or further reading materials on the above would be much appreciated. Thank you all in advance!!", "author_fullname": "t2_3rvgz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Architecture Best Practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y36nxq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665687220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/DE\"&gt;r/DE&lt;/a&gt;! We&amp;#39;re currently working on a transition from SQL Server/In-house tools to a Databricks implementation and I have some concerns/questions about its direction. Hoping I can get someone else&amp;#39;s perspective or further materials to read before I go into battle with the consultants.&lt;/p&gt;\n\n&lt;p&gt;High level, we take client&amp;#39;s data in, we clean it up, and run analysis on the cleaned up data. The cleaning and analysis portions are a mix of standard and custom processes. We&amp;#39;re looking to standardize this into a bronze, silver, gold process where bronze is loaded raw data, the cleanup happens in silver, and the gold layer has the final presentation needed for our analysis. Makes sense in theory, but as we know the devil is in the details. Below are my main concerns about the current proposal:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The silver cleanup process can pull directly from clean gold data.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In my opinion, this makes a dependency nightmare, and also the whole process non-deterministic. We do need to sometimes reference the output of our measurement in the data cleanup process. I would think in situations like this we would store the output of gold, and then have that then be an input to the bronze layer. This way we would have a set of inputs always equal the same output, and not create a state machine. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data gets loaded to silver and then edited in-place&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In their proposal we aggregate bronze data, load it into silver, and then run processes that manipulate the tables while they&amp;#39;re still in silver. In my opinion, this makes silver data time-dependent. I would think these cleanup processes would be a part of the pipeline from bronze to silver. I understand that the data does have to live somewhere so maybe this is a common practice that I am unfamiliar with. The datasets we&amp;#39;re working with are generally 20 to 350 million lines and haven&amp;#39;t surpassed 5gb but may in the future.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts or further reading materials on the above would be much appreciated. Thank you all in advance!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y36nxq", "is_robot_indexable": true, "report_reasons": null, "author": "EggLampBasket", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y36nxq/databricks_architecture_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y36nxq/databricks_architecture_best_practices/", "subreddit_subscribers": 76337, "created_utc": 1665687220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a DE on an executive position, with several years of experience. For many years now I've been leading a reasonably large (60 people) team of applied mathematicians, engineers, software developers, actuaries, etc. in a business-side data engineering function within the Risk department in a large bank in a non-US country.\n\nOur current level of service and NPS is suffering a bit and I've concluded the main problem  is that my team is not really a data engineering team built from the ground up, (with ad hoc job descriptions, people actually matching that job description, adequate compensation reflecting this etc.). Hence, I'm working with HR to remediate this, and the first step is to rewrite our job descriptions for DE.\n\nI have already written some draft JDs but I thought it was a good idea to ask for your opinions on the following matters:\n\n&amp;#x200B;\n\n1. What would you say are the top five **soft skills** (i.e. non-technical) a DE must definitely have?How are you actually assessing those skills in interviews? (it's way easier to apply a technical test than to assess a particular soft skill in such a reduced interview period)\n2. Apart from leadership and team management skills, how would you clearly differentiate a **junior** data engineer vs a **senior** data engineer and a **head** of DE (years of experience, variety of technologies, specific academic background, specific soft skills...)? \n3. Over the years I've sometimes found it better to hire business people (i.e. academically and with actual business experience) specializing in DE stuff, than computer scientists with experience trying to learn the business. **What's your take on this** and how would you **reflect it in the job description**?\n4. What would you say are the specific DE skills that really **up the bar in terms of \"desirability\"** (and, hence, pay range)?\n\n&amp;#x200B;\n\nThank you so much in advance for any pointers or tips regarding this.\n\nBest,", "author_fullname": "t2_1ae5qbb2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me figure out a good job description for DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y31ch7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665674241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a DE on an executive position, with several years of experience. For many years now I&amp;#39;ve been leading a reasonably large (60 people) team of applied mathematicians, engineers, software developers, actuaries, etc. in a business-side data engineering function within the Risk department in a large bank in a non-US country.&lt;/p&gt;\n\n&lt;p&gt;Our current level of service and NPS is suffering a bit and I&amp;#39;ve concluded the main problem  is that my team is not really a data engineering team built from the ground up, (with ad hoc job descriptions, people actually matching that job description, adequate compensation reflecting this etc.). Hence, I&amp;#39;m working with HR to remediate this, and the first step is to rewrite our job descriptions for DE.&lt;/p&gt;\n\n&lt;p&gt;I have already written some draft JDs but I thought it was a good idea to ask for your opinions on the following matters:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What would you say are the top five &lt;strong&gt;soft skills&lt;/strong&gt; (i.e. non-technical) a DE must definitely have?How are you actually assessing those skills in interviews? (it&amp;#39;s way easier to apply a technical test than to assess a particular soft skill in such a reduced interview period)&lt;/li&gt;\n&lt;li&gt;Apart from leadership and team management skills, how would you clearly differentiate a &lt;strong&gt;junior&lt;/strong&gt; data engineer vs a &lt;strong&gt;senior&lt;/strong&gt; data engineer and a &lt;strong&gt;head&lt;/strong&gt; of DE (years of experience, variety of technologies, specific academic background, specific soft skills...)? &lt;/li&gt;\n&lt;li&gt;Over the years I&amp;#39;ve sometimes found it better to hire business people (i.e. academically and with actual business experience) specializing in DE stuff, than computer scientists with experience trying to learn the business. &lt;strong&gt;What&amp;#39;s your take on this&lt;/strong&gt; and how would you &lt;strong&gt;reflect it in the job description&lt;/strong&gt;?&lt;/li&gt;\n&lt;li&gt;What would you say are the specific DE skills that really &lt;strong&gt;up the bar in terms of &amp;quot;desirability&amp;quot;&lt;/strong&gt; (and, hence, pay range)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you so much in advance for any pointers or tips regarding this.&lt;/p&gt;\n\n&lt;p&gt;Best,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y31ch7", "is_robot_indexable": true, "report_reasons": null, "author": "biyectivo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y31ch7/help_me_figure_out_a_good_job_description_for_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y31ch7/help_me_figure_out_a_good_job_description_for_de/", "subreddit_subscribers": 76337, "created_utc": 1665674241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I applied to a job, a question was do you have experience with Snowflake and SQL? Where I replies yes\n\nSo if I have an interview would it be easy to quickly do some tutorials and wing the interview or not?", "author_fullname": "t2_pll9p2x2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I already have experience in SQL and data warehousing, how difficult will it be learning snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2zh9h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665669575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied to a job, a question was do you have experience with Snowflake and SQL? Where I replies yes&lt;/p&gt;\n\n&lt;p&gt;So if I have an interview would it be easy to quickly do some tutorials and wing the interview or not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y2zh9h", "is_robot_indexable": true, "report_reasons": null, "author": "Do_I_know_you_1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2zh9h/i_already_have_experience_in_sql_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2zh9h/i_already_have_experience_in_sql_and_data/", "subreddit_subscribers": 76337, "created_utc": 1665669575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say you have a source database like Postgres. Data engineering team pipes that data in to a data warehouse landing location. For purposes of metadata, data catalog, etc who is the owner of those raw tables? Data engineering certainly owns the pipelines, maybe they own the tables, how about the data itself?", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data that is replicated from a source database to landing in data warehouse. Who is the \u201cowner\u201d?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y367r0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665686168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say you have a source database like Postgres. Data engineering team pipes that data in to a data warehouse landing location. For purposes of metadata, data catalog, etc who is the owner of those raw tables? Data engineering certainly owns the pipelines, maybe they own the tables, how about the data itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y367r0", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y367r0/data_that_is_replicated_from_a_source_database_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y367r0/data_that_is_replicated_from_a_source_database_to/", "subreddit_subscribers": 76337, "created_utc": 1665686168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_q9a58hxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DuckDB - Modern Data Stack in a Box with DuckDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_y31oid", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-md-jPF1vRQTud0XCPODyiZjaDMXi7frOPYybhgKFeA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665675090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "duckdb.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GctXJNpN2X3nQLnJV4YKiGNicM-bQELTDEHwQJ3tLlo.jpg?auto=webp&amp;s=df4fb13c0741919fd9f695ba304cb6d3a1fb56ed", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/GctXJNpN2X3nQLnJV4YKiGNicM-bQELTDEHwQJ3tLlo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ac7c3882de773b950cd2e3cd83aba08b84d6fa7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/GctXJNpN2X3nQLnJV4YKiGNicM-bQELTDEHwQJ3tLlo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e011b38bce0303748d54aed1967329a61ba0bf14", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/GctXJNpN2X3nQLnJV4YKiGNicM-bQELTDEHwQJ3tLlo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=28287676b7e382af057ff233ebabb92eb44395da", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/GctXJNpN2X3nQLnJV4YKiGNicM-bQELTDEHwQJ3tLlo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17a7e3ab4756e9f324d7dfbf41f4877a51c3a790", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/GctXJNpN2X3nQLnJV4YKiGNicM-bQELTDEHwQJ3tLlo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b38599a9c28c8b8760b7158ed21e418d1d88ff32", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/GctXJNpN2X3nQLnJV4YKiGNicM-bQELTDEHwQJ3tLlo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4eaa0d481f6cb0b5bde29d1b7160f655cc8a2cc8", "width": 1080, "height": 567}], "variants": {}, "id": "jWyiaF4Jb7ULQyU8SCl75THeEbJM9dbSQ9YXdauXufk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "y31oid", "is_robot_indexable": true, "report_reasons": null, "author": "m___ke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y31oid/duckdb_modern_data_stack_in_a_box_with_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html", "subreddit_subscribers": 76337, "created_utc": 1665675090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it just me or are ETL tools painful for working with APIs? My team uses Pentaho and making simple API calls is fine but anything complex turns into a mess of string manipulation. My boss wants the team to continue using Pentaho but APIs just seem so much easier to work with in python, especially if they have a good sdk.  \n\n\nIs it just me? Is it the tool?", "author_fullname": "t2_mo4lb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Tools for Rest APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y381x6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665690497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it just me or are ETL tools painful for working with APIs? My team uses Pentaho and making simple API calls is fine but anything complex turns into a mess of string manipulation. My boss wants the team to continue using Pentaho but APIs just seem so much easier to work with in python, especially if they have a good sdk.  &lt;/p&gt;\n\n&lt;p&gt;Is it just me? Is it the tool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y381x6", "is_robot_indexable": true, "report_reasons": null, "author": "Phantazein", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y381x6/etl_tools_for_rest_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y381x6/etl_tools_for_rest_apis/", "subreddit_subscribers": 76337, "created_utc": 1665690497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Same as the title. I have heard that many folks store their db creds in excel file or google sheet. What do you guys use ?", "author_fullname": "t2_57e44nxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you store your database creds ? Let assume you have multiple databases (around 20-30) , then how do you manage or store their creds so that they are shared and stored securely", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2y06x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665665649.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Same as the title. I have heard that many folks store their db creds in excel file or google sheet. What do you guys use ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y2y06x", "is_robot_indexable": true, "report_reasons": null, "author": "RstarPhoneix", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2y06x/where_do_you_store_your_database_creds_let_assume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2y06x/where_do_you_store_your_database_creds_let_assume/", "subreddit_subscribers": 76337, "created_utc": 1665665649.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \n\nI was thinking of starting a course from Bertosz Konieczny ([http://www.becomedataengineer.com](http://www.becomedataengineer.com)) and I was wondering if anyone here ever attended? Is it worth it? Do you have any other recommendations? I have a bsc in CS and work as a BI Analyst", "author_fullname": "t2_q92vm6kg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2ux8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1665655920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\n\n&lt;p&gt;I was thinking of starting a course from Bertosz Konieczny (&lt;a href=\"http://www.becomedataengineer.com\"&gt;http://www.becomedataengineer.com&lt;/a&gt;) and I was wondering if anyone here ever attended? Is it worth it? Do you have any other recommendations? I have a bsc in CS and work as a BI Analyst&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yslgPJk-szPt97jkswC3Nx8cvM3a11gPRuYDKNj3yWg.jpg?auto=webp&amp;s=1542e8920c80e86e1f3702fefcd58c69b7829606", "width": 1474, "height": 1382}, "resolutions": [{"url": "https://external-preview.redd.it/yslgPJk-szPt97jkswC3Nx8cvM3a11gPRuYDKNj3yWg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff91bba2b9b8b3a41e8cf5e68a51eed1b893de42", "width": 108, "height": 101}, {"url": "https://external-preview.redd.it/yslgPJk-szPt97jkswC3Nx8cvM3a11gPRuYDKNj3yWg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25b60bf72a20b20cc3ca903cd4a74efe9e4def04", "width": 216, "height": 202}, {"url": "https://external-preview.redd.it/yslgPJk-szPt97jkswC3Nx8cvM3a11gPRuYDKNj3yWg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd0bc0e4ffde9bbdd4f20a8a6e095869196eebaa", "width": 320, "height": 300}, {"url": "https://external-preview.redd.it/yslgPJk-szPt97jkswC3Nx8cvM3a11gPRuYDKNj3yWg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=20a9466b6a7c92cd7785c42e1f9465f7e947baec", "width": 640, "height": 600}, {"url": "https://external-preview.redd.it/yslgPJk-szPt97jkswC3Nx8cvM3a11gPRuYDKNj3yWg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fdb787c49667fd09789ae56baa7e9ee88504b636", "width": 960, "height": 900}, {"url": "https://external-preview.redd.it/yslgPJk-szPt97jkswC3Nx8cvM3a11gPRuYDKNj3yWg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f851b9a33fdf60277729b30c26dbae0f3d8540f7", "width": 1080, "height": 1012}], "variants": {}, "id": "bySzyovL-sih7_Y4SroZ3grkLCHk2pBvj7mf-kkUrh0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y2ux8h", "is_robot_indexable": true, "report_reasons": null, "author": "tyrion25", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2ux8h/de_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2ux8h/de_course/", "subreddit_subscribers": 76337, "created_utc": 1665655920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am currently taking a data mining course which asks to use PySpark to implement our assignments. Is anyone an expert in PySpark and  would be willing to tutor me? I live in LA so getting in-person tutoring would be great, but I am happy to  do over zoom as well. Happy to negotiate pay. Thank you!", "author_fullname": "t2_iljnct2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark Tutor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2pllu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665636355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am currently taking a data mining course which asks to use PySpark to implement our assignments. Is anyone an expert in PySpark and  would be willing to tutor me? I live in LA so getting in-person tutoring would be great, but I am happy to  do over zoom as well. Happy to negotiate pay. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y2pllu", "is_robot_indexable": true, "report_reasons": null, "author": "Relative_Practice_93", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2pllu/pyspark_tutor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2pllu/pyspark_tutor/", "subreddit_subscribers": 76337, "created_utc": 1665636355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given years of experience can you limit your workday to &lt; 3 hours?", "author_fullname": "t2_xgf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "As a Data Engineer, how much of my job can I automate away?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y38e6m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665691245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given years of experience can you limit your workday to &amp;lt; 3 hours?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y38e6m", "is_robot_indexable": true, "report_reasons": null, "author": "Ricearoni33", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y38e6m/as_a_data_engineer_how_much_of_my_job_can_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y38e6m/as_a_data_engineer_how_much_of_my_job_can_i/", "subreddit_subscribers": 76337, "created_utc": 1665691245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have the below error and I believe it is caused by making the schema definition of my data an ArrayType\n\n    22/10/14 02:09:24 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 200)1]\n    java.lang.ClassCastException: class org.apache.spark.unsafe.types.UTF8String cannot be cast to class org.apache.spark.sql.catalyst.util.ArrayData (org.apache.spark.unsafe.types.UTF8String and org.apache.spark.sql.catalyst.util.ArrayData are in unnamed module of loader 'app')\n    \tat org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray(rows.scala:48)\n\nMy schema definition is:\n\n    Schema = ArrayType(StructType([\n        StructField(\"ID\", StringType(), True),        \n        StructField(\"SELF\", BooleanType(), True),\n        StructField(\"MATCH-TYPE\", StringType(), True),\n        StructField(\"ACCT-TYPE\", StringType(), True),\n        StructField('ts', TimestampType(), True) \n    ]))\n\nI then apply my schema via the following:\n\n    df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), Schema).alias('parsed_value'))\n    \n    # then i flatten the structure of my data with explode\n    df = df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))\n\nI then go on to do a group by function to count ID within a 30s window with 10s slide:\n\n    Counts = df \\\n        .groupBy(window(df.ts, \"30 seconds\", \"10 seconds\"))\\\n        .agg(F.count(\"ID\") \\\n        .alias(\"ID_count\")) \\\n        .select(\"window\",\"ID_count\")\n\nWhen I go to writeStream, I get the error at the top\n\n    query = Counts \\\n        .writeStream \\\n        .outputMode(\"complete\") \\\n        .format(\"console\") \\\n        .trigger(processingTime='2 seconds') \\\n        .start()\n\nCould anyone offer any advice?\n\nThank you!", "author_fullname": "t2_8gl6ycus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark Kafka Streaming Issue: writeStream error when using ArrayType", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y31fo7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665674462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have the below error and I believe it is caused by making the schema definition of my data an ArrayType&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;22/10/14 02:09:24 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 200)1]\njava.lang.ClassCastException: class org.apache.spark.unsafe.types.UTF8String cannot be cast to class org.apache.spark.sql.catalyst.util.ArrayData (org.apache.spark.unsafe.types.UTF8String and org.apache.spark.sql.catalyst.util.ArrayData are in unnamed module of loader &amp;#39;app&amp;#39;)\n    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getArray(rows.scala:48)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My schema definition is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Schema = ArrayType(StructType([\n    StructField(&amp;quot;ID&amp;quot;, StringType(), True),        \n    StructField(&amp;quot;SELF&amp;quot;, BooleanType(), True),\n    StructField(&amp;quot;MATCH-TYPE&amp;quot;, StringType(), True),\n    StructField(&amp;quot;ACCT-TYPE&amp;quot;, StringType(), True),\n    StructField(&amp;#39;ts&amp;#39;, TimestampType(), True) \n]))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I then apply my schema via the following:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df = df.select(F.from_json(F.col(&amp;quot;value&amp;quot;).cast(&amp;quot;string&amp;quot;), Schema).alias(&amp;#39;parsed_value&amp;#39;))\n\n# then i flatten the structure of my data with explode\ndf = df.select(F.explode(F.col(&amp;quot;parsed_value&amp;quot;)).alias(&amp;#39;unnested_value&amp;#39;))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I then go on to do a group by function to count ID within a 30s window with 10s slide:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Counts = df \\\n    .groupBy(window(df.ts, &amp;quot;30 seconds&amp;quot;, &amp;quot;10 seconds&amp;quot;))\\\n    .agg(F.count(&amp;quot;ID&amp;quot;) \\\n    .alias(&amp;quot;ID_count&amp;quot;)) \\\n    .select(&amp;quot;window&amp;quot;,&amp;quot;ID_count&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When I go to writeStream, I get the error at the top&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;query = Counts \\\n    .writeStream \\\n    .outputMode(&amp;quot;complete&amp;quot;) \\\n    .format(&amp;quot;console&amp;quot;) \\\n    .trigger(processingTime=&amp;#39;2 seconds&amp;#39;) \\\n    .start()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Could anyone offer any advice?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y31fo7", "is_robot_indexable": true, "report_reasons": null, "author": "ApprehensiveFerret44", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y31fo7/pyspark_kafka_streaming_issue_writestream_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y31fo7/pyspark_kafka_streaming_issue_writestream_error/", "subreddit_subscribers": 76337, "created_utc": 1665674462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Views for metrics management in your data stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_y315de", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mav4GN5tebZFWWPjkldO67WzlB5jfXxPerx8UFjJ6bk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665673757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/introducing-views", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tOZD8FimYNeaLO2b_46NA3hYXR_fe12WfGyOF6qBIyM.jpg?auto=webp&amp;s=720db075b3f3d651c6582cf248941e81e271ed89", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/tOZD8FimYNeaLO2b_46NA3hYXR_fe12WfGyOF6qBIyM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04754e8f26ce6584e9f7320d8ae9296c39d8f00a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/tOZD8FimYNeaLO2b_46NA3hYXR_fe12WfGyOF6qBIyM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=84e24831b702095b74ca46cfc87106f53f562fd7", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/tOZD8FimYNeaLO2b_46NA3hYXR_fe12WfGyOF6qBIyM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=42180b84b31128e1d7d311e1cb71759f17409f68", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/tOZD8FimYNeaLO2b_46NA3hYXR_fe12WfGyOF6qBIyM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d91a2c7ff7148e5c6046c4cc99111be4cc6e5a0e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/tOZD8FimYNeaLO2b_46NA3hYXR_fe12WfGyOF6qBIyM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b38d3aed37b9694d06acb98c3a6c5e7d812c92cb", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/tOZD8FimYNeaLO2b_46NA3hYXR_fe12WfGyOF6qBIyM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b25948a4c01bdd718dbb2e5c2f1b46f94c493136", "width": 1080, "height": 567}], "variants": {}, "id": "k0-CLnsMWk6ABOwLuc_FkI8Cgzjm-4pFTqEYyd2KSRA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "y315de", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y315de/introducing_views_for_metrics_management_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/introducing-views", "subreddit_subscribers": 76337, "created_utc": 1665673757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_aehfkmkb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft Ignite Day 1 | Data Hot News", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_y2xabq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QSzH9aM-3nM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Microsoft Ignite Day 1 | Data Hot News\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Microsoft Ignite Day 1 | Data Hot News", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QSzH9aM-3nM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Microsoft Ignite Day 1 | Data Hot News\"&gt;&lt;/iframe&gt;", "author_name": "Cloud and Data Science", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QSzH9aM-3nM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/c/CloudDataScience"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QSzH9aM-3nM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Microsoft Ignite Day 1 | Data Hot News\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/y2xabq", "height": 200}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/3BIOFQqcxkrc8E8xq-bLCK3gpzdl4nLvZr80SWF4R30.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665663622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtube.com/watch?v=QSzH9aM-3nM&amp;feature=share", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TbyA2PmC61LpUWPkU0wwEM2Q_s3kIJSYoq_4NtXWlpA.jpg?auto=webp&amp;s=00641063ce5c84a889670ed59bb5289ec56442bf", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/TbyA2PmC61LpUWPkU0wwEM2Q_s3kIJSYoq_4NtXWlpA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=97cf6b16ef636f5f0a14aa630c84929ccfb4daad", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/TbyA2PmC61LpUWPkU0wwEM2Q_s3kIJSYoq_4NtXWlpA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=63d2076f42f07fb54032cb159c108204f4a9e713", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/TbyA2PmC61LpUWPkU0wwEM2Q_s3kIJSYoq_4NtXWlpA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=dd165898622623333fcb4e6ccf78cf561347c54a", "width": 320, "height": 240}], "variants": {}, "id": "_spbrECWORzSSvZbYGwpsNGn97ynEuvVfRqECJtHjVo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y2xabq", "is_robot_indexable": true, "report_reasons": null, "author": "Successful-Aide3077", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2xabq/microsoft_ignite_day_1_data_hot_news/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtube.com/watch?v=QSzH9aM-3nM&amp;feature=share", "subreddit_subscribers": 76337, "created_utc": 1665663622.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Microsoft Ignite Day 1 | Data Hot News", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/QSzH9aM-3nM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen title=\"Microsoft Ignite Day 1 | Data Hot News\"&gt;&lt;/iframe&gt;", "author_name": "Cloud and Data Science", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/QSzH9aM-3nM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/c/CloudDataScience"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, as per title I would like to start a streaming project with Kafka. I'm searching for some interesting free data sources but it doesn't seem easy to find one, I'm trying to avoid some inflated ones like Twitter of Facebook API. A project about IoT would be really cool.\n\n I would also like to keep the project as cheap as possible.\n\nThanks!", "author_fullname": "t2_1xbf9q7w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Data Streaming Sources for a Personal Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2t2e4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665648892.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, as per title I would like to start a streaming project with Kafka. I&amp;#39;m searching for some interesting free data sources but it doesn&amp;#39;t seem easy to find one, I&amp;#39;m trying to avoid some inflated ones like Twitter of Facebook API. A project about IoT would be really cool.&lt;/p&gt;\n\n&lt;p&gt;I would also like to keep the project as cheap as possible.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y2t2e4", "is_robot_indexable": true, "report_reasons": null, "author": "aerdna69", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2t2e4/free_data_streaming_sources_for_a_personal_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2t2e4/free_data_streaming_sources_for_a_personal_project/", "subreddit_subscribers": 76337, "created_utc": 1665648892.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone know if Oracle Data Pump Export can be used for an Oracle cloud data migration to AWS RDS?", "author_fullname": "t2_78dddb58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Oracle Data Pump Export", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2oc4r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665632292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know if Oracle Data Pump Export can be used for an Oracle cloud data migration to AWS RDS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y2oc4r", "is_robot_indexable": true, "report_reasons": null, "author": "Both-Trainer-1308", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2oc4r/using_oracle_data_pump_export/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2oc4r/using_oracle_data_pump_export/", "subreddit_subscribers": 76337, "created_utc": 1665632292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi, so I graduated last year with a bs in finance and pretty much worked retail since and never had an internship in finance. I had a job offer as analyst at a bulge bracket bank this year and completely turned it down and realized traditional finance is not what I wanted to do. After researching roles and such I really liked BI &amp; data engineering. Thinking data engineering may be the better path to pursue for job growth.\n\nJust wondering what I would need to do to be successful in a switch. I may plan on going back to school next year to pursue an MSCS or MS in data science. But really focused on being a self learner at the moment and building my own knowledge and experience. \n\nI want to create a data engineering portfolio and work on my own projects by 2023 as well. I know SQL and plan to pick up python starting this week.\n\nAny project tips/portfolio help? Where do you recommend I keep my online portfolio?\n\nBest way to learn python as a newbie?\n\nBooks, YouTube channels, &amp; any other free resources you would like to recommend?\n\nSo far using datacamp\u2019s path to becoming a data engineer \n\nThank you", "author_fullname": "t2_k9d12ch2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "career change help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2f6df", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665611531.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665607982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi, so I graduated last year with a bs in finance and pretty much worked retail since and never had an internship in finance. I had a job offer as analyst at a bulge bracket bank this year and completely turned it down and realized traditional finance is not what I wanted to do. After researching roles and such I really liked BI &amp;amp; data engineering. Thinking data engineering may be the better path to pursue for job growth.&lt;/p&gt;\n\n&lt;p&gt;Just wondering what I would need to do to be successful in a switch. I may plan on going back to school next year to pursue an MSCS or MS in data science. But really focused on being a self learner at the moment and building my own knowledge and experience. &lt;/p&gt;\n\n&lt;p&gt;I want to create a data engineering portfolio and work on my own projects by 2023 as well. I know SQL and plan to pick up python starting this week.&lt;/p&gt;\n\n&lt;p&gt;Any project tips/portfolio help? Where do you recommend I keep my online portfolio?&lt;/p&gt;\n\n&lt;p&gt;Best way to learn python as a newbie?&lt;/p&gt;\n\n&lt;p&gt;Books, YouTube channels, &amp;amp; any other free resources you would like to recommend?&lt;/p&gt;\n\n&lt;p&gt;So far using datacamp\u2019s path to becoming a data engineer &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y2f6df", "is_robot_indexable": true, "report_reasons": null, "author": "uaintgotnoyzy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2f6df/career_change_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2f6df/career_change_help/", "subreddit_subscribers": 76337, "created_utc": 1665607982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any experience with Dremio? Interviewing for a job with a company who uses it, opinions would be helpful.", "author_fullname": "t2_qbiqlkfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Dremio...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y34e1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665681707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any experience with Dremio? Interviewing for a job with a company who uses it, opinions would be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y34e1f", "is_robot_indexable": true, "report_reasons": null, "author": "K_D20", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y34e1f/thoughts_on_dremio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y34e1f/thoughts_on_dremio/", "subreddit_subscribers": 76337, "created_utc": 1665681707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering what the key difference is between the three, I hear a lot from fellow colleagues that its very different but when I do my research on it they seem quite similar in their responsibilities with client side (front end) being design/navigation, writing code and debugging and service side (back end) being more data application development?\n\nWould love some perspective on it and whether they are in fact different ends of the data spectrum!", "author_fullname": "t2_swx1rmld", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question: Difference Between Data Engineers, Developers and UX/UX Designer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y2n2wa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665628542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what the key difference is between the three, I hear a lot from fellow colleagues that its very different but when I do my research on it they seem quite similar in their responsibilities with client side (front end) being design/navigation, writing code and debugging and service side (back end) being more data application development?&lt;/p&gt;\n\n&lt;p&gt;Would love some perspective on it and whether they are in fact different ends of the data spectrum!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y2n2wa", "is_robot_indexable": true, "report_reasons": null, "author": "TheOpportunist01", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y2n2wa/question_difference_between_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y2n2wa/question_difference_between_data_engineers/", "subreddit_subscribers": 76337, "created_utc": 1665628542.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}