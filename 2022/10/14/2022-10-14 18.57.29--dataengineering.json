{"kind": "Listing", "data": {"after": "t3_y39jvt", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's amazing how many organizations workflows still revolve around Excel. I've seen CFOs and COOs folders filled with 20 different versions of the same Excel file.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_y3rxoe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 235, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 235, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/u2tP7lah92HheOLypwAH34gooHBP6HmPPfFKYszocq0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665748604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/a893r4wagrt91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/a893r4wagrt91.jpg?auto=webp&amp;s=ffe62982b850df8b2923dfeb8825a7f40f8bbb10", "width": 800, "height": 806}, "resolutions": [{"url": "https://preview.redd.it/a893r4wagrt91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fb95561d79456ae42e043a8969988ea9630ac91", "width": 108, "height": 108}, {"url": "https://preview.redd.it/a893r4wagrt91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=28454e9644a240260c031f0a3e35c5381a02351b", "width": 216, "height": 217}, {"url": "https://preview.redd.it/a893r4wagrt91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6ee7e941a1847f9f48c7925815c22052bed98368", "width": 320, "height": 322}, {"url": "https://preview.redd.it/a893r4wagrt91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7fb903397688ed3dfacc07d729e271c1310382e0", "width": 640, "height": 644}], "variants": {}, "id": "ytcLwq84KlM4vpE-ARDZJErIH4k2o3Zkr6RXjPyY4Lk"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "y3rxoe", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3rxoe/its_amazing_how_many_organizations_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/a893r4wagrt91.jpg", "subreddit_subscribers": 76426, "created_utc": 1665748604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just a word of caution.  Lots of engineers dis product managers.  I am not a PM, but I can say that talking negatively about your colleagues (in any function) will come back to bite you in the ass.  You know what they say about karma. She is not kind. If you are working with a product manager who has some gaps in her/his knowledge, do the right thing and help to educate and support them.  You are on the same team.", "author_fullname": "t2_pl4q8ng7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stop dissing product managers (career advice)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3ifb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665717173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just a word of caution.  Lots of engineers dis product managers.  I am not a PM, but I can say that talking negatively about your colleagues (in any function) will come back to bite you in the ass.  You know what they say about karma. She is not kind. If you are working with a product manager who has some gaps in her/his knowledge, do the right thing and help to educate and support them.  You are on the same team.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y3ifb7", "is_robot_indexable": true, "report_reasons": null, "author": "droppedorphan", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3ifb7/stop_dissing_product_managers_career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3ifb7/stop_dissing_product_managers_career_advice/", "subreddit_subscribers": 76426, "created_utc": 1665717173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it just me or are ETL tools painful for working with APIs? My team uses Pentaho and making simple API calls is fine but anything complex turns into a mess of string manipulation. My boss wants the team to continue using Pentaho but APIs just seem so much easier to work with in python, especially if they have a good sdk.  \n\n\nIs it just me? Is it the tool?", "author_fullname": "t2_mo4lb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL Tools for Rest APIs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y381x6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665690497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it just me or are ETL tools painful for working with APIs? My team uses Pentaho and making simple API calls is fine but anything complex turns into a mess of string manipulation. My boss wants the team to continue using Pentaho but APIs just seem so much easier to work with in python, especially if they have a good sdk.  &lt;/p&gt;\n\n&lt;p&gt;Is it just me? Is it the tool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y381x6", "is_robot_indexable": true, "report_reasons": null, "author": "Phantazein", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y381x6/etl_tools_for_rest_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y381x6/etl_tools_for_rest_apis/", "subreddit_subscribers": 76426, "created_utc": 1665690497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ever since WFH has become the norm there seems to be a new craze going on called over employment. Essentially people work 2-3 work from home jobs concurrently either with or without the permission of their employers.\n\nThis ranges a whole spectrum of industries and there is a whole sub reddit dedicated to it but I wanted to bring the discussion here because Data Engineering and other data related positions seem like they would be ideal for this type of set up. Specifically because of the automation scripting that are used in so many data related task.\n\nWhat I am seeing is people are able to automate the most basic job functions and only require manual intervention in the event that something breaks or when making upgrades or enhancements. It seems there is quite a bit of controversy around this and if it's ethical and obviously it's frowned upon by many employees.\n\nIf I were to do this I would prefer to do it the legit way as I would like to keep the great relationship I have with my employer. My current data engineering position usually involves at least 2 zoom meetings a day and the times aren't consistent so if I considered taking a second role it would have to be something with no or minimal meetings. Also my primary position I would keep salaried with the benefits and anything else I would only consider doing on a contractual / hourly basis. \n\nDo such jobs exist where basically you are given a project and a time frame to compete it within but besides that you don't need to be available for calls or meetings at any set hours ? That would be the ideal situation for me if it was something I could work on outside the hours of my primary job and on the weekends.\n\nObviously the money is an important reason to do this but my primary motivation is also keeping a diverse skill set sharp. My job right now is almost entirely based in SQL and PowerShell and the only platforms we are using is Azure, on prem SQL Server and the ETL tools are SSIS or Data Factory. I put a lot of effort into learning Python for Data Science and Data Analyst task and I'm kinda bummed I'm not using it. It would also be nice to take on a second gig that uses a different platform like AWS so I can keep myself relevant and up to date with all the main cloud environments.\n\nIs anyone here doing this successfully ?", "author_fullname": "t2_dfcot0zg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone hold down two DE positions at once ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3aayl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665695642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since WFH has become the norm there seems to be a new craze going on called over employment. Essentially people work 2-3 work from home jobs concurrently either with or without the permission of their employers.&lt;/p&gt;\n\n&lt;p&gt;This ranges a whole spectrum of industries and there is a whole sub reddit dedicated to it but I wanted to bring the discussion here because Data Engineering and other data related positions seem like they would be ideal for this type of set up. Specifically because of the automation scripting that are used in so many data related task.&lt;/p&gt;\n\n&lt;p&gt;What I am seeing is people are able to automate the most basic job functions and only require manual intervention in the event that something breaks or when making upgrades or enhancements. It seems there is quite a bit of controversy around this and if it&amp;#39;s ethical and obviously it&amp;#39;s frowned upon by many employees.&lt;/p&gt;\n\n&lt;p&gt;If I were to do this I would prefer to do it the legit way as I would like to keep the great relationship I have with my employer. My current data engineering position usually involves at least 2 zoom meetings a day and the times aren&amp;#39;t consistent so if I considered taking a second role it would have to be something with no or minimal meetings. Also my primary position I would keep salaried with the benefits and anything else I would only consider doing on a contractual / hourly basis. &lt;/p&gt;\n\n&lt;p&gt;Do such jobs exist where basically you are given a project and a time frame to compete it within but besides that you don&amp;#39;t need to be available for calls or meetings at any set hours ? That would be the ideal situation for me if it was something I could work on outside the hours of my primary job and on the weekends.&lt;/p&gt;\n\n&lt;p&gt;Obviously the money is an important reason to do this but my primary motivation is also keeping a diverse skill set sharp. My job right now is almost entirely based in SQL and PowerShell and the only platforms we are using is Azure, on prem SQL Server and the ETL tools are SSIS or Data Factory. I put a lot of effort into learning Python for Data Science and Data Analyst task and I&amp;#39;m kinda bummed I&amp;#39;m not using it. It would also be nice to take on a second gig that uses a different platform like AWS so I can keep myself relevant and up to date with all the main cloud environments.&lt;/p&gt;\n\n&lt;p&gt;Is anyone here doing this successfully ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y3aayl", "is_robot_indexable": true, "report_reasons": null, "author": "DrRedmondNYC", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3aayl/anyone_hold_down_two_de_positions_at_once/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3aayl/anyone_hold_down_two_de_positions_at_once/", "subreddit_subscribers": 76426, "created_utc": 1665695642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was hired as a data analyst and the company was supposed to be outsourcing the ETL. However, the ETL company they went with isn't working out and I'm being tasked with building the pipeline. I do have a basic understanding, some of the technical skills and what I don't know I can quickly learn. The money that was being spent on the ETL company is being reallocated and there's nothing else in the budget for data. \n\n\nCurrently I have Stitch set up to load some of our data sources in BigQuery and I'm creating views  in BigQuery to transform the data. It's not a great solution but I needed something quick. The plan is to do the transformations in dbt. Stitch doesn't have enough of the connectors we need, most notably Amazon Seller Central, so I'm exploring other options. I tried out AirByte but it's too over normalized and I haven't had success getting the BigQuery denormalized destination to work. I've also used Meltano some. \n\n\nWhat are some other good free options? If you were in my position what pipeline would you set up? It's an ecommerce company so Shopify, Amazon Seller Central, Google Analytics, Google Ads, Facebook Ads, Big Query, NetSuite are some of the connectors I'll need. \n\n\nAre there any good learning resources out there for someone like me. I'm taking python courses and doing as much research as possible, reading Fundamentals of Data Engineering and Data Science on the Google Cloud Platform. But I really need to hit the ground running and would do better if I could learn as I go.", "author_fullname": "t2_2q171de9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data analyst tasked with building data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3i3t1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665716240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired as a data analyst and the company was supposed to be outsourcing the ETL. However, the ETL company they went with isn&amp;#39;t working out and I&amp;#39;m being tasked with building the pipeline. I do have a basic understanding, some of the technical skills and what I don&amp;#39;t know I can quickly learn. The money that was being spent on the ETL company is being reallocated and there&amp;#39;s nothing else in the budget for data. &lt;/p&gt;\n\n&lt;p&gt;Currently I have Stitch set up to load some of our data sources in BigQuery and I&amp;#39;m creating views  in BigQuery to transform the data. It&amp;#39;s not a great solution but I needed something quick. The plan is to do the transformations in dbt. Stitch doesn&amp;#39;t have enough of the connectors we need, most notably Amazon Seller Central, so I&amp;#39;m exploring other options. I tried out AirByte but it&amp;#39;s too over normalized and I haven&amp;#39;t had success getting the BigQuery denormalized destination to work. I&amp;#39;ve also used Meltano some. &lt;/p&gt;\n\n&lt;p&gt;What are some other good free options? If you were in my position what pipeline would you set up? It&amp;#39;s an ecommerce company so Shopify, Amazon Seller Central, Google Analytics, Google Ads, Facebook Ads, Big Query, NetSuite are some of the connectors I&amp;#39;ll need. &lt;/p&gt;\n\n&lt;p&gt;Are there any good learning resources out there for someone like me. I&amp;#39;m taking python courses and doing as much research as possible, reading Fundamentals of Data Engineering and Data Science on the Google Cloud Platform. But I really need to hit the ground running and would do better if I could learn as I go.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3i3t1", "is_robot_indexable": true, "report_reasons": null, "author": "lahma_mama", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3i3t1/data_analyst_tasked_with_building_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3i3t1/data_analyst_tasked_with_building_data_pipeline/", "subreddit_subscribers": 76426, "created_utc": 1665716240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, since one of the features of Spark is lazy evaluation, so something is executed only when action is called.  \nHow do i log then?", "author_fullname": "t2_sx1wry60", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how do u log when u use pyspark(Spark).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y37u5x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665689990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, since one of the features of Spark is lazy evaluation, so something is executed only when action is called.&lt;br/&gt;\nHow do i log then?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y37u5x", "is_robot_indexable": true, "report_reasons": null, "author": "AcceptableProcess772", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y37u5x/how_do_u_log_when_u_use_pysparkspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y37u5x/how_do_u_log_when_u_use_pysparkspark/", "subreddit_subscribers": 76426, "created_utc": 1665689990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We test ran a pipeline in Azure after moving from Informatica. Trust me, it was one of the most satisfying thing that I've ever seen. Where Informatica takes ages to aggregate the data, azure did in few minutes. I literally got instant goosebumps imagining the sheer power of Azure n/w.\n\nSo, what is the most amazing thing that you've experienced with technology?", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the most beautiful part of Cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_y3oq3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 0, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": "", "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665738569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665738209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We test ran a pipeline in Azure after moving from Informatica. Trust me, it was one of the most satisfying thing that I&amp;#39;ve ever seen. Where Informatica takes ages to aggregate the data, azure did in few minutes. I literally got instant goosebumps imagining the sheer power of Azure n/w.&lt;/p&gt;\n\n&lt;p&gt;So, what is the most amazing thing that you&amp;#39;ve experienced with technology?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "call_to_action": "", "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y3oq3y", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3oq3y/whats_the_most_beautiful_part_of_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3oq3y/whats_the_most_beautiful_part_of_cloud/", "subreddit_subscribers": 76426, "created_utc": 1665738209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_265t3i5h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "46 Best Resources to learn Big Data (YouTube, Books, Courses, &amp; Tutorials)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_y3n2kj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iVYzb0ZONTnAtIxXFHPb0JsxXMTCzKDky62rfDInoDU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665732151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mltut.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.mltut.com/best-resources-to-learn-big-data/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Zth95306cFBt-OnZ1hAtNCTDJRuihEklulXdwH0vIwI.jpg?auto=webp&amp;s=614384e41816c34adb88a5a6bcc25a4bd09b7e0c", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/Zth95306cFBt-OnZ1hAtNCTDJRuihEklulXdwH0vIwI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc69e201936d4caaf45dd5a2029eb4df50df141", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Zth95306cFBt-OnZ1hAtNCTDJRuihEklulXdwH0vIwI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb620cf085f300cfe313947aa09bb4a9e6ad711a", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Zth95306cFBt-OnZ1hAtNCTDJRuihEklulXdwH0vIwI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ab9c5c73f3d77856d5fddcd988784a6e34b11edb", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Zth95306cFBt-OnZ1hAtNCTDJRuihEklulXdwH0vIwI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=548473bed1ecf68b1fbfdd93b8006a04a2f75bd7", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Zth95306cFBt-OnZ1hAtNCTDJRuihEklulXdwH0vIwI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7d42c8d4abefae19bc7abca0c76dd56f57d11e37", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/Zth95306cFBt-OnZ1hAtNCTDJRuihEklulXdwH0vIwI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31c8ffb1731b385a2807e1d638e9d9f356b4031f", "width": 1080, "height": 607}], "variants": {}, "id": "6H8zHTpMnkqcsEbLwABRmGyofeCjFJsg8wvMe_ttDf4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "y3n2kj", "is_robot_indexable": true, "report_reasons": null, "author": "MlTut", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3n2kj/46_best_resources_to_learn_big_data_youtube_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.mltut.com/best-resources-to-learn-big-data/", "subreddit_subscribers": 76426, "created_utc": 1665732151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How is this done if you have to permanently delete thousands of records from several core tables? What sort of checks do you implement to ensure you are only getting the relevant rows?", "author_fullname": "t2_1inf5n5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pipelines to delete user data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 0, "top_awarded_type": null, "hide_score": false, "name": "t3_y3fxwx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 0, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": "", "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665710097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How is this done if you have to permanently delete thousands of records from several core tables? What sort of checks do you implement to ensure you are only getting the relevant rows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "call_to_action": "", "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y3fxwx", "is_robot_indexable": true, "report_reasons": null, "author": "changiairport", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3fxwx/pipelines_to_delete_user_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3fxwx/pipelines_to_delete_user_data/", "subreddit_subscribers": 76426, "created_utc": 1665710097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say you have a source database like Postgres. Data engineering team pipes that data in to a data warehouse landing location. For purposes of metadata, data catalog, etc who is the owner of those raw tables? Data engineering certainly owns the pipelines, maybe they own the tables, how about the data itself?", "author_fullname": "t2_c6w52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data that is replicated from a source database to landing in data warehouse. Who is the \u201cowner\u201d?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y367r0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665686168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say you have a source database like Postgres. Data engineering team pipes that data in to a data warehouse landing location. For purposes of metadata, data catalog, etc who is the owner of those raw tables? Data engineering certainly owns the pipelines, maybe they own the tables, how about the data itself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y367r0", "is_robot_indexable": true, "report_reasons": null, "author": "EmergenL", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y367r0/data_that_is_replicated_from_a_source_database_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y367r0/data_that_is_replicated_from_a_source_database_to/", "subreddit_subscribers": 76426, "created_utc": 1665686168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was told that you can't run Spark on Windows by a prof (which I don't think is true at all) and that I have to run a Linux through VM to be able to use it which I think is ridiculous and I'm **not** gonna do that. So I have two options here, to run it through WSL (will have to install Java, Scala, etc. as well which I won't really use in WSL) or use PySpark.\n\nI'm wondering if there's a difference between using Spark on a native Linux system, on WSL, and through PySpark? Running it on a WSL is probably the same as using it on a Linux, but is there a significant performance difference? How about PySpark, is it basically just the same thing except in Python?", "author_fullname": "t2_6cpkhkm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is PySpark and Spark the same thing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3nzq2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665735513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was told that you can&amp;#39;t run Spark on Windows by a prof (which I don&amp;#39;t think is true at all) and that I have to run a Linux through VM to be able to use it which I think is ridiculous and I&amp;#39;m &lt;strong&gt;not&lt;/strong&gt; gonna do that. So I have two options here, to run it through WSL (will have to install Java, Scala, etc. as well which I won&amp;#39;t really use in WSL) or use PySpark.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if there&amp;#39;s a difference between using Spark on a native Linux system, on WSL, and through PySpark? Running it on a WSL is probably the same as using it on a Linux, but is there a significant performance difference? How about PySpark, is it basically just the same thing except in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3nzq2", "is_robot_indexable": true, "report_reasons": null, "author": "Strong-File", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3nzq2/is_pyspark_and_spark_the_same_thing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3nzq2/is_pyspark_and_spark_the_same_thing/", "subreddit_subscribers": 76426, "created_utc": 1665735513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nI have the desire to create an analytics side hustle but do not want to host the data service on my machine. I would ideally use a cloud service so if my computer crashed, it would be no big deal.\n\nSome background information:\n\nI would like to create a system to execute API calls in Python and store the data.\n\nWith this data I would like to perform ML and data mining for analysis.\n\nThese Python scripts should run on a schedule.\n\nThis data would then be displayed in Tableau Cloud for customers to have their analytics.\n\nHow should I go about this? I am trying to minimize costs but if the tool is amazing I would be willing to spend the money.\n\nThank you in advance.", "author_fullname": "t2_5fves3ac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Service Infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3djxw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665703609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have the desire to create an analytics side hustle but do not want to host the data service on my machine. I would ideally use a cloud service so if my computer crashed, it would be no big deal.&lt;/p&gt;\n\n&lt;p&gt;Some background information:&lt;/p&gt;\n\n&lt;p&gt;I would like to create a system to execute API calls in Python and store the data.&lt;/p&gt;\n\n&lt;p&gt;With this data I would like to perform ML and data mining for analysis.&lt;/p&gt;\n\n&lt;p&gt;These Python scripts should run on a schedule.&lt;/p&gt;\n\n&lt;p&gt;This data would then be displayed in Tableau Cloud for customers to have their analytics.&lt;/p&gt;\n\n&lt;p&gt;How should I go about this? I am trying to minimize costs but if the tool is amazing I would be willing to spend the money.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3djxw", "is_robot_indexable": true, "report_reasons": null, "author": "TraditionalWorth1436", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3djxw/data_service_infrastructure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3djxw/data_service_infrastructure/", "subreddit_subscribers": 76426, "created_utc": 1665703609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have IoT data in the Postgres database, it is mostly timestamped data with columns as \n\ntimestamp(utc), org\\_id, sensor\\_id, parameter\\_uuid, value\n\nI want to move this data to parquet files and I want to partition by day. So it should be\n\nyear &gt; month &gt; day. How can I achieve this with python?", "author_fullname": "t2_ck6w5h7u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Partitioning parquet data files with a timestamp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3q4f7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665743027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have IoT data in the Postgres database, it is mostly timestamped data with columns as &lt;/p&gt;\n\n&lt;p&gt;timestamp(utc), org_id, sensor_id, parameter_uuid, value&lt;/p&gt;\n\n&lt;p&gt;I want to move this data to parquet files and I want to partition by day. So it should be&lt;/p&gt;\n\n&lt;p&gt;year &amp;gt; month &amp;gt; day. How can I achieve this with python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y3q4f7", "is_robot_indexable": true, "report_reasons": null, "author": "Frosting_Quirky", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3q4f7/partitioning_parquet_data_files_with_a_timestamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3q4f7/partitioning_parquet_data_files_with_a_timestamp/", "subreddit_subscribers": 76426, "created_utc": 1665743027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Moderately old school BI professional here, and I am curious how good at being a dbt developer are you?\n\nI would say that I am intermediate with no real plans to become advanced. Would rather learn more DE than AE. \n\nWhat do you think your level is? Where do you see yourself going?", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How good of a dbt developer are you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y40gc0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665769617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Moderately old school BI professional here, and I am curious how good at being a dbt developer are you?&lt;/p&gt;\n\n&lt;p&gt;I would say that I am intermediate with no real plans to become advanced. Would rather learn more DE than AE. &lt;/p&gt;\n\n&lt;p&gt;What do you think your level is? Where do you see yourself going?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y40gc0", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y40gc0/how_good_of_a_dbt_developer_are_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y40gc0/how_good_of_a_dbt_developer_are_you/", "subreddit_subscribers": 76426, "created_utc": 1665769617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have in one case an on prem MySQL server and in another case an on prem SQL server. I wanted to use CDC on some tables and publish them to a Kafka topic that then get consumed by Snowflake. \n\nThe way I understood to do this previously was to use Debezium and Kafka. I was also warned Debezium is fidgety af and I don't have Java experience so I'm not particularly keen on it.\n\nWhat would you guys recommend for approach for this for someone that feels very comfortable with python and shell stuff. I'd like to use free open source stuff.\n\nThanks for any input in advance.", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debezium+Kafka alternatives for CDC stream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y3zj27", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665767347.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have in one case an on prem MySQL server and in another case an on prem SQL server. I wanted to use CDC on some tables and publish them to a Kafka topic that then get consumed by Snowflake. &lt;/p&gt;\n\n&lt;p&gt;The way I understood to do this previously was to use Debezium and Kafka. I was also warned Debezium is fidgety af and I don&amp;#39;t have Java experience so I&amp;#39;m not particularly keen on it.&lt;/p&gt;\n\n&lt;p&gt;What would you guys recommend for approach for this for someone that feels very comfortable with python and shell stuff. I&amp;#39;d like to use free open source stuff.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any input in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y3zj27", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3zj27/debeziumkafka_alternatives_for_cdc_stream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3zj27/debeziumkafka_alternatives_for_cdc_stream/", "subreddit_subscribers": 76426, "created_utc": 1665767347.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Please bear with my ignorant question here.\n\nQuestion: shall I keep the request response as Json then stage data or transform data into table then stage?\n\nI am thinking to make a data warehouse calling some data to store the data in a staging db, then later on to transform the data and insert to Redshift. However, I am not so sure **if I should store json data in staging db** (Flow one in the chart below) **or it is okay that I convert the json into table then stage the data** (Flow two in the chart below)**?**\n\nIf staging json is the better choice, I am thinking to use dynamoDB by using key value. Does this make senses to you ?\n\nThanks so much in advance.\n\nhttps://preview.redd.it/36xqoywzhst91.png?width=3055&amp;format=png&amp;auto=webp&amp;s=e4a14cce3f01bd19fe7de2b99644946f7cc250e9", "author_fullname": "t2_pd2piq1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practice for staging data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "media_metadata": {"36xqoywzhst91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 46, "x": 108, "u": "https://preview.redd.it/36xqoywzhst91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ee70072a344b86421daf55a3e7958b240ace641"}, {"y": 92, "x": 216, "u": "https://preview.redd.it/36xqoywzhst91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e227e1a362a3e3d6741d155a7b8d8148ca52f9e"}, {"y": 137, "x": 320, "u": "https://preview.redd.it/36xqoywzhst91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=74d6616138dd7f4995b560b4af4072d51fcf3c6d"}, {"y": 274, "x": 640, "u": "https://preview.redd.it/36xqoywzhst91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c48608508272640e4be50ed51583759e0cde6fb9"}, {"y": 411, "x": 960, "u": "https://preview.redd.it/36xqoywzhst91.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6cca47f80c18c005dff0b219d6ac33ffdbaea725"}, {"y": 462, "x": 1080, "u": "https://preview.redd.it/36xqoywzhst91.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bcad7e4c296cd3fbf59fafc19314fd14359a37dc"}], "s": {"y": 1308, "x": 3055, "u": "https://preview.redd.it/36xqoywzhst91.png?width=3055&amp;format=png&amp;auto=webp&amp;s=e4a14cce3f01bd19fe7de2b99644946f7cc250e9"}, "id": "36xqoywzhst91"}}, "name": "t3_y3wy2n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dUkbmHwGL5UgeQsmA8Uz8TdEcJOPaeCb8fwwXtzwBKc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665761207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please bear with my ignorant question here.&lt;/p&gt;\n\n&lt;p&gt;Question: shall I keep the request response as Json then stage data or transform data into table then stage?&lt;/p&gt;\n\n&lt;p&gt;I am thinking to make a data warehouse calling some data to store the data in a staging db, then later on to transform the data and insert to Redshift. However, I am not so sure &lt;strong&gt;if I should store json data in staging db&lt;/strong&gt; (Flow one in the chart below) &lt;strong&gt;or it is okay that I convert the json into table then stage the data&lt;/strong&gt; (Flow two in the chart below)&lt;strong&gt;?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;If staging json is the better choice, I am thinking to use dynamoDB by using key value. Does this make senses to you ?&lt;/p&gt;\n\n&lt;p&gt;Thanks so much in advance.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/36xqoywzhst91.png?width=3055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a14cce3f01bd19fe7de2b99644946f7cc250e9\"&gt;https://preview.redd.it/36xqoywzhst91.png?width=3055&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a14cce3f01bd19fe7de2b99644946f7cc250e9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3wy2n", "is_robot_indexable": true, "report_reasons": null, "author": "uk_dataguy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3wy2n/best_practice_for_staging_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3wy2n/best_practice_for_staging_data/", "subreddit_subscribers": 76426, "created_utc": 1665761207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a data team, we have our own Azure subscription, where we have data lake, a Linux vm, Databricks, Azure SQL DB, etc, all within our control and budgets.\n\nWe also have a Windows Azure VM, managed by IT, that is connected to an SFTP endpoint for a 3rd party client system. I wrote some Python on that VM to grab some files from their endpoint via paramiko and upload them to our Azure storage account. \n\nWhat I want to do is take the Python off of the VM and onto the Linux box because that would bring that job, or at least the code for the job, and the scheduling of that job, off an IT-managed box and put it within our sphere of influence.\n\nUnfortunately, the 3rd party client refuses to open their endpoint up to any new connections, because it was difficult enough for them to figure out their VPN to the Windows box in the first place. \n\nTherefore, I cannot NOT use the IT-managed box to reach the SFTP endpoint, but thought that I might be able to do something like this (I'm sorry it sounds so hacky but I'm not really sure how to verbalise my thoughts):\n\n* Put the script on the Linux box\n* Programmatically remote from the Linux box to the Windows VM\n* Run the Python script on the Linux box and somehow call the SFTP endpoint that's on the Windows box \"through\" (?) Linux and retrieve the file \n* Upload the file to the storage account\n\nI'm not sure if this is doable, maybe it's just \"not how security works\" but thought I'd ask you guys.", "author_fullname": "t2_plnwp2sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Programmatically remote from Linux to Windows and retrieve data from SFTP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3pn2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665741410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a data team, we have our own Azure subscription, where we have data lake, a Linux vm, Databricks, Azure SQL DB, etc, all within our control and budgets.&lt;/p&gt;\n\n&lt;p&gt;We also have a Windows Azure VM, managed by IT, that is connected to an SFTP endpoint for a 3rd party client system. I wrote some Python on that VM to grab some files from their endpoint via paramiko and upload them to our Azure storage account. &lt;/p&gt;\n\n&lt;p&gt;What I want to do is take the Python off of the VM and onto the Linux box because that would bring that job, or at least the code for the job, and the scheduling of that job, off an IT-managed box and put it within our sphere of influence.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, the 3rd party client refuses to open their endpoint up to any new connections, because it was difficult enough for them to figure out their VPN to the Windows box in the first place. &lt;/p&gt;\n\n&lt;p&gt;Therefore, I cannot NOT use the IT-managed box to reach the SFTP endpoint, but thought that I might be able to do something like this (I&amp;#39;m sorry it sounds so hacky but I&amp;#39;m not really sure how to verbalise my thoughts):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Put the script on the Linux box&lt;/li&gt;\n&lt;li&gt;Programmatically remote from the Linux box to the Windows VM&lt;/li&gt;\n&lt;li&gt;Run the Python script on the Linux box and somehow call the SFTP endpoint that&amp;#39;s on the Windows box &amp;quot;through&amp;quot; (?) Linux and retrieve the file &lt;/li&gt;\n&lt;li&gt;Upload the file to the storage account&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m not sure if this is doable, maybe it&amp;#39;s just &amp;quot;not how security works&amp;quot; but thought I&amp;#39;d ask you guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3pn2w", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering-Worker459", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3pn2w/programmatically_remote_from_linux_to_windows_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3pn2w/programmatically_remote_from_linux_to_windows_and/", "subreddit_subscribers": 76426, "created_utc": 1665741410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Alright - Data scientist here so lack of DE regarding Azure is to be expected.\n\n&amp;#x200B;\n\nI am getting my first hands on on Azure Serverless connecting to that Synapes sql endpoint. Now I was told I need to generate a token that lasts for 60 min if I want to query the data. \n\nIs it not possible to just link this to the hivestore and have a permanent link to it since I am only using read/list permission and not use that token generation cause its going to effect development, time and have a side-affect by speeding up my hairloss.", "author_fullname": "t2_112dgq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks and Azure Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3og5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665737207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright - Data scientist here so lack of DE regarding Azure is to be expected.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am getting my first hands on on Azure Serverless connecting to that Synapes sql endpoint. Now I was told I need to generate a token that lasts for 60 min if I want to query the data. &lt;/p&gt;\n\n&lt;p&gt;Is it not possible to just link this to the hivestore and have a permanent link to it since I am only using read/list permission and not use that token generation cause its going to effect development, time and have a side-affect by speeding up my hairloss.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3og5g", "is_robot_indexable": true, "report_reasons": null, "author": "psychEcon", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3og5g/databricks_and_azure_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3og5g/databricks_and_azure_synapse/", "subreddit_subscribers": 76426, "created_utc": 1665737207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I'm a Data Engineer from Buenos Aires, Argentina (Latin America) with almost 2 years of experience. Now I'm trying to switch to another company, and I don't know where I can look for new jobs (apart from Linkedin). As I speak English, I'm aiming for remote jobs as a contractor, but most of those positions ask for +3 years exp. Any kind of advice is appreciated :)", "author_fullname": "t2_dhxblshz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where should I look for a job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3wfw1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665760021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m a Data Engineer from Buenos Aires, Argentina (Latin America) with almost 2 years of experience. Now I&amp;#39;m trying to switch to another company, and I don&amp;#39;t know where I can look for new jobs (apart from Linkedin). As I speak English, I&amp;#39;m aiming for remote jobs as a contractor, but most of those positions ask for +3 years exp. Any kind of advice is appreciated :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3wfw1", "is_robot_indexable": true, "report_reasons": null, "author": "Hopeful_Musician_806", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3wfw1/where_should_i_look_for_a_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3wfw1/where_should_i_look_for_a_job/", "subreddit_subscribers": 76426, "created_utc": 1665760021.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was told this would be a Data Analyst role. I mainly chose a DA role since it contributes to DA/DS work experience and its easy to combine with studies (I'm doing an MSc in CS after a CS undergrad). Upon joining I figured out they actually meant \"Data Analyst Engineer\", whatever that even means. Tasks range from optimizing bootstrapping scripts, to modifying and deleting DB columns/merging stuff. Writing logic for processing data in Pyspark, etc.\n\nMy main interest was doing some job that is not heavy on the coding side of things and gain some experience for my eventual transfer to a DS role.\n\nWhat do you guys think of this, any pros in this, aka. is a DE role I described useful for DS role in the future?", "author_fullname": "t2_arhihj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DA Roles with DE Tasks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3sii1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665750578.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665750200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was told this would be a Data Analyst role. I mainly chose a DA role since it contributes to DA/DS work experience and its easy to combine with studies (I&amp;#39;m doing an MSc in CS after a CS undergrad). Upon joining I figured out they actually meant &amp;quot;Data Analyst Engineer&amp;quot;, whatever that even means. Tasks range from optimizing bootstrapping scripts, to modifying and deleting DB columns/merging stuff. Writing logic for processing data in Pyspark, etc.&lt;/p&gt;\n\n&lt;p&gt;My main interest was doing some job that is not heavy on the coding side of things and gain some experience for my eventual transfer to a DS role.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think of this, any pros in this, aka. is a DE role I described useful for DS role in the future?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y3sii1", "is_robot_indexable": true, "report_reasons": null, "author": "JakeSpearOH", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3sii1/da_roles_with_de_tasks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3sii1/da_roles_with_de_tasks/", "subreddit_subscribers": 76426, "created_utc": 1665750200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently working as a junior data engineer for a company. I wont be specific but my job is basically using ssms and visual studio ssis integration to migrate data from customers old system system to our own.  \n\nIts pretty basic but I want to do more. I'm still only learning and getting experience as I've only been here a month but I feel there's more that can be done. \n\nI'd love to use the data to provide more analytics and possible even AI prediction. My biggest interest is creating pipelines within ADF and data warehouses that can be used for those purposes. \n\nMy question is do I stick to what I'm doing and keep learning until I'm more experienced or do I make this suggestion and see if I can work on it (even with my lack of experience). Could I also find a new and better way to migrate the data or find a more efficient way with azure if possible or again, do I wait until I've gotten more experience. \n\nI've been looking at training and learning paths too and there's none out there that really cater towards SQL a lot of certifications and training caters towards azure and python. \n\nAny ideas or suggestions?", "author_fullname": "t2_6o5h731v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "started a new job as a junior data engineer but need help with my career path.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3scoc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665749752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working as a junior data engineer for a company. I wont be specific but my job is basically using ssms and visual studio ssis integration to migrate data from customers old system system to our own.  &lt;/p&gt;\n\n&lt;p&gt;Its pretty basic but I want to do more. I&amp;#39;m still only learning and getting experience as I&amp;#39;ve only been here a month but I feel there&amp;#39;s more that can be done. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to use the data to provide more analytics and possible even AI prediction. My biggest interest is creating pipelines within ADF and data warehouses that can be used for those purposes. &lt;/p&gt;\n\n&lt;p&gt;My question is do I stick to what I&amp;#39;m doing and keep learning until I&amp;#39;m more experienced or do I make this suggestion and see if I can work on it (even with my lack of experience). Could I also find a new and better way to migrate the data or find a more efficient way with azure if possible or again, do I wait until I&amp;#39;ve gotten more experience. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at training and learning paths too and there&amp;#39;s none out there that really cater towards SQL a lot of certifications and training caters towards azure and python. &lt;/p&gt;\n\n&lt;p&gt;Any ideas or suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3scoc", "is_robot_indexable": true, "report_reasons": null, "author": "anonymous6156", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3scoc/started_a_new_job_as_a_junior_data_engineer_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3scoc/started_a_new_job_as_a_junior_data_engineer_but/", "subreddit_subscribers": 76426, "created_utc": 1665749752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The project I am working on has large metadata files in version control. We currently use csv's for these files but this has the following issues:\n\n1. when viewing with a text editor it is hard to see what column a value corresponds to.\n\n2. when viewing a diff in git it is hard to tell exactly what value has changed.[\n\nWe have considered using yaml or json or python dictionaries but this also has disadvantages:\n\n1. These file formats cannot be opened and edited in excel. We have found it useful to edit in excel because you can filter the metadata and more easily edit multiple values.\n\nDoes anyone know a way for metadata files to work somewhat well in both a normal text editor/git and a table/spreadsheet editor?\n\nthanks for any help!", "author_fullname": "t2_bjsqi7xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best file format for large metadata files in version control systems?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3qomb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665744903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The project I am working on has large metadata files in version control. We currently use csv&amp;#39;s for these files but this has the following issues:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;when viewing with a text editor it is hard to see what column a value corresponds to.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;when viewing a diff in git it is hard to tell exactly what value has changed.[&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We have considered using yaml or json or python dictionaries but this also has disadvantages:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;These file formats cannot be opened and edited in excel. We have found it useful to edit in excel because you can filter the metadata and more easily edit multiple values.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Does anyone know a way for metadata files to work somewhat well in both a normal text editor/git and a table/spreadsheet editor?&lt;/p&gt;\n\n&lt;p&gt;thanks for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3qomb", "is_robot_indexable": true, "report_reasons": null, "author": "WonderfulEstimate176", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3qomb/best_file_format_for_large_metadata_files_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3qomb/best_file_format_for_large_metadata_files_in/", "subreddit_subscribers": 76426, "created_utc": 1665744903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let me explain. I'm currently handling data pipelines in python to transform and load facts and dimensions for financial data.\n\nImagine we have code that generates Budget financial data for year 2023. If I follow general git control version principles, next year the code will have changed (new columns, additional features, different parameters, etc.).\n\nA situation that might happen is, in the future one might want to understand how a certain column was calculated in the past, or what logic is behind a certain business rule. You might want to even execute that old code again.\n\nThe complexity increases when there are different scenarios for budget, monthly forecasts and ad-hoc planning requirements. Each case with it's own almost identical but different code.\n\nWhat's the best way to handle this? My first though is to copy paste the code in a folder for future review and execution. Even having a different git repo for each case. But something tells me that's not the best practice.\n\nWhat do you think?", "author_fullname": "t2_a3yboz1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to address having repository snapshots for future reproducability?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3pe5l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665740556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let me explain. I&amp;#39;m currently handling data pipelines in python to transform and load facts and dimensions for financial data.&lt;/p&gt;\n\n&lt;p&gt;Imagine we have code that generates Budget financial data for year 2023. If I follow general git control version principles, next year the code will have changed (new columns, additional features, different parameters, etc.).&lt;/p&gt;\n\n&lt;p&gt;A situation that might happen is, in the future one might want to understand how a certain column was calculated in the past, or what logic is behind a certain business rule. You might want to even execute that old code again.&lt;/p&gt;\n\n&lt;p&gt;The complexity increases when there are different scenarios for budget, monthly forecasts and ad-hoc planning requirements. Each case with it&amp;#39;s own almost identical but different code.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to handle this? My first though is to copy paste the code in a folder for future review and execution. Even having a different git repo for each case. But something tells me that&amp;#39;s not the best practice.&lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "y3pe5l", "is_robot_indexable": true, "report_reasons": null, "author": "Embarrassed_Flight45", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3pe5l/how_to_address_having_repository_snapshots_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3pe5l/how_to_address_having_repository_snapshots_for/", "subreddit_subscribers": 76426, "created_utc": 1665740556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My background is heavily Windows, Azure, Cisco Networks, Terraform, all of these from a Admin/Architecture point of view, current position would be described as a Cloud Architect. I'm looking at moving across into something that is of interest (Data engineering) and would like to explore machine learning at some point. \n\nmy ability to code is beginner, I'm better at interrupting and refactoring a piece of code than writing from scratch, and my DB skills come from a admin view of provision resources, performance and maintenance side. Learning Python more thoroughly at the moment and then will venture in SQL\n\nHow did you find the move?", "author_fullname": "t2_7x058oej", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone moved from a Sys Admin/Architect Role to DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3omyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665737914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My background is heavily Windows, Azure, Cisco Networks, Terraform, all of these from a Admin/Architecture point of view, current position would be described as a Cloud Architect. I&amp;#39;m looking at moving across into something that is of interest (Data engineering) and would like to explore machine learning at some point. &lt;/p&gt;\n\n&lt;p&gt;my ability to code is beginner, I&amp;#39;m better at interrupting and refactoring a piece of code than writing from scratch, and my DB skills come from a admin view of provision resources, performance and maintenance side. Learning Python more thoroughly at the moment and then will venture in SQL&lt;/p&gt;\n\n&lt;p&gt;How did you find the move?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "y3omyj", "is_robot_indexable": true, "report_reasons": null, "author": "Kensarim", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y3omyj/anyone_moved_from_a_sys_adminarchitect_role_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y3omyj/anyone_moved_from_a_sys_adminarchitect_role_to_de/", "subreddit_subscribers": 76426, "created_utc": 1665737914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A big part of me joining the company I\u2019m currently at is the ability to learn from the software engineers and bring the software best practices to data.\n\nWith that in mind, the team is constantly referring to Feature Flags - the ability to have a feature turned on in dev but off in prod. It seems like a great approach, allowing is to embrace CI/CD. But\u2026 how does it work in practice in a data warehouse?\n\nWe use dbt. We can enable or disable a model with a config block. That\u2019s easy. But what about a feature that changes an inner join to a left join? Or that adds a join to a new table in the model and pulls in an additional 3 fields?\n\nI\u2019ve not been able to crack it. Does anyone have an answer?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Flags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y39jvt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665693908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A big part of me joining the company I\u2019m currently at is the ability to learn from the software engineers and bring the software best practices to data.&lt;/p&gt;\n\n&lt;p&gt;With that in mind, the team is constantly referring to Feature Flags - the ability to have a feature turned on in dev but off in prod. It seems like a great approach, allowing is to embrace CI/CD. But\u2026 how does it work in practice in a data warehouse?&lt;/p&gt;\n\n&lt;p&gt;We use dbt. We can enable or disable a model with a config block. That\u2019s easy. But what about a feature that changes an inner join to a left join? Or that adds a join to a new table in the model and pulls in an additional 3 fields?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve not been able to crack it. Does anyone have an answer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "y39jvt", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/y39jvt/feature_flags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/y39jvt/feature_flags/", "subreddit_subscribers": 76426, "created_utc": 1665693908.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}