{"kind": "Listing", "data": {"after": "t3_y4fkca", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I understand USB 3.2 Gen 2 can do \"up to\" 10 GBit/Sec.  \n\nWhat thought of speeds can be expected from pairing this enclosure with M.2 NVMe assuming its connected to the proper USB port.  \n\nAt first hand it seems a bit of an absurdity but assuming a spare  M.2 NVMe does it make sense?\n\nThank you.", "author_fullname": "t2_11dj0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the theoretical read/write speeds of such combo ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_y43pp1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 129, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 129, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/_DpwuCrvwTKyT1KME9zij7lE-wj7brM55plOYsWhCj8.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665777698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand USB 3.2 Gen 2 can do &amp;quot;up to&amp;quot; 10 GBit/Sec.  &lt;/p&gt;\n\n&lt;p&gt;What thought of speeds can be expected from pairing this enclosure with M.2 NVMe assuming its connected to the proper USB port.  &lt;/p&gt;\n\n&lt;p&gt;At first hand it seems a bit of an absurdity but assuming a spare  M.2 NVMe does it make sense?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/hx7quoatutt91.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/hx7quoatutt91.jpg?auto=webp&amp;s=f25808c7830c4f217939792f1c42bb2f788aeef3", "width": 1284, "height": 1621}, "resolutions": [{"url": "https://preview.redd.it/hx7quoatutt91.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1cb1cad5d497b085e53d65e6946635486ef157c2", "width": 108, "height": 136}, {"url": "https://preview.redd.it/hx7quoatutt91.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b6bee85bcaa74a7f55d837132922fe7c6c7ed0d", "width": 216, "height": 272}, {"url": "https://preview.redd.it/hx7quoatutt91.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2427a94668c202013dc172fe244f3935cc91736", "width": 320, "height": 403}, {"url": "https://preview.redd.it/hx7quoatutt91.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=402a7c4856a885d3d3beec2e77ab3501464f71d3", "width": 640, "height": 807}, {"url": "https://preview.redd.it/hx7quoatutt91.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e439ecddb579b54a5591a6a992d629e6b8d216dd", "width": 960, "height": 1211}, {"url": "https://preview.redd.it/hx7quoatutt91.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b6d3d1c19f79e625ea73dcc672c76c501a30b02", "width": 1080, "height": 1363}], "variants": {}, "id": "mVqVlVKYtgGqJOiMyrIzJ3Ezokacr3HZ7vJXc6KuJMw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "130TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y43pp1", "is_robot_indexable": true, "report_reasons": null, "author": "nando1969", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/y43pp1/what_are_the_theoretical_readwrite_speeds_of_such/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/hx7quoatutt91.jpg", "subreddit_subscribers": 647549, "created_utc": 1665777698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like to start keeping an archive of mine and others github accounts with full revision history. Are there any tools that can do this historically? I assume I will have to run my own git or svn server, but I am unsure about how to scrape and then import into my private archive.\n\nEdit: Thanks for the answers", "author_fullname": "t2_tihoe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools for cloning github repos with revision?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y40qsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665776267.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665770333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to start keeping an archive of mine and others github accounts with full revision history. Are there any tools that can do this historically? I assume I will have to run my own git or svn server, but I am unsure about how to scrape and then import into my private archive.&lt;/p&gt;\n\n&lt;p&gt;Edit: Thanks for the answers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y40qsg", "is_robot_indexable": true, "report_reasons": null, "author": "just_for_saving61", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y40qsg/any_tools_for_cloning_github_repos_with_revision/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y40qsg/any_tools_for_cloning_github_repos_with_revision/", "subreddit_subscribers": 647549, "created_utc": 1665770333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Before, I used to be able to right click a stream and copy the source URL, paste it in VLC and stream and record. Seems like the Chaturbate player no longer allows that. Has anyone found a way to hoard their favorite streams from Chaturbate?", "author_fullname": "t2_5i5zgezr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Capturing streams from Chaturbate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4au9r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665796754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before, I used to be able to right click a stream and copy the source URL, paste it in VLC and stream and record. Seems like the Chaturbate player no longer allows that. Has anyone found a way to hoard their favorite streams from Chaturbate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y4au9r", "is_robot_indexable": true, "report_reasons": null, "author": "FantasticContent", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y4au9r/capturing_streams_from_chaturbate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y4au9r/capturing_streams_from_chaturbate/", "subreddit_subscribers": 647549, "created_utc": 1665796754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought a Jonsbo N1 from Newegg and I want to make an 80TB-100TB NAS to edit raw 6K video right out of it on Davinci Resolve. (2GB to 15GB per clip)  \nI've read that 10GBE is a good idea for future proofing, but I'm not sure how this can benefit me right now since all I have at home is a Verizon Wi-Fi 6 router and my PC has a X570i Mobo with (2.5) Intel Gigabit Ethernet &amp; Wi-Fi 6 (802.11ax) (No more expandability on the Mobo, since the only PCIE port has my Graphics Card)  \nI've also read that ECC RAM may be benefitial for this, not really sure how, since I'm ignorant on the subject of NAS &amp; ECC.\n\nI'm planning to buy 5 x 20TB WD Red Pro drives, since sometimes I find them for $350 a piece and they are CMR drives, which are supposed to be faster and more suitable for my purposes.\n\nRequirements:\n\n* Mini-ITX\n* Speed &amp; Total Storage are a priority over parity\n* Fast Read-Write Speeds  \n\n\nI've already read u/poipoipoi post where he ends up buying an EPYC3101D4I-2T since it already has:  \n\n\n1. 2x 10GB Lan Ports\n2. 4 x ECC Compatible DDR4 DIMM slots (rather than 2 as on standard Mini Itx Mobos)\n3. Included EPYC 3101 4 cores, 4 threads processor with Heatsink\n4. 6 SATA Ports in total 4 x Oculink port + 2 x SATAIII ports (instead of 4 like regular Mini Itx Mobos)\n5. u/poipoipoi also added an Ableconn Dual PCIe NVMe M.2 SSD Adapter Card with 2 x 1TB NVMe for write cache + 1 x 512GB NVMe on the Mobo for Read Cache\n\nHere is the original post: [https://www.reddit.com/r/DataHoarder/comments/thwykb/newbie\\_moving\\_off\\_synology\\_and\\_going\\_to\\_build\\_a/](https://www.reddit.com/r/DataHoarder/comments/thwykb/newbie_moving_off_synology_and_going_to_build_a/)  \n\n\nQuestions:  \n\n\n1. Is 64GB of RAM enough for 80TB-100TB?\n2. Is ECC really necessary?\n3. Is WIP exclusively managed by the cache drives if I proper configure TrueNAS/UnRaid?\n4. Is 10GB LAN really necessary for editing 6K raw video files? (2GB to 15GB per clip)\n5. Are CMR drives better for my purpose than SMR drives?\n6. Is this 4 core/threads EPYC 3101 a good option or will a 6c/12t Ryzen achieve more performance?\n7. If I do a setup similar to u/poipoipoi, should I add an additional 2.5\" SSD for a bootable drive?\n\n* Or can a caching/storage drive be used as a bootable drive?\n\nThanks in advance!", "author_fullname": "t2_aye1wimg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newbie building Jonsbo N1 Mini NAS for Raw 6K Video Editing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y40rrh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665770398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a Jonsbo N1 from Newegg and I want to make an 80TB-100TB NAS to edit raw 6K video right out of it on Davinci Resolve. (2GB to 15GB per clip)&lt;br/&gt;\nI&amp;#39;ve read that 10GBE is a good idea for future proofing, but I&amp;#39;m not sure how this can benefit me right now since all I have at home is a Verizon Wi-Fi 6 router and my PC has a X570i Mobo with (2.5) Intel Gigabit Ethernet &amp;amp; Wi-Fi 6 (802.11ax) (No more expandability on the Mobo, since the only PCIE port has my Graphics Card)&lt;br/&gt;\nI&amp;#39;ve also read that ECC RAM may be benefitial for this, not really sure how, since I&amp;#39;m ignorant on the subject of NAS &amp;amp; ECC.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to buy 5 x 20TB WD Red Pro drives, since sometimes I find them for $350 a piece and they are CMR drives, which are supposed to be faster and more suitable for my purposes.&lt;/p&gt;\n\n&lt;p&gt;Requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Mini-ITX&lt;/li&gt;\n&lt;li&gt;Speed &amp;amp; Total Storage are a priority over parity&lt;/li&gt;\n&lt;li&gt;Fast Read-Write Speeds&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve already read &lt;a href=\"/u/poipoipoi\"&gt;u/poipoipoi&lt;/a&gt; post where he ends up buying an EPYC3101D4I-2T since it already has:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2x 10GB Lan Ports&lt;/li&gt;\n&lt;li&gt;4 x ECC Compatible DDR4 DIMM slots (rather than 2 as on standard Mini Itx Mobos)&lt;/li&gt;\n&lt;li&gt;Included EPYC 3101 4 cores, 4 threads processor with Heatsink&lt;/li&gt;\n&lt;li&gt;6 SATA Ports in total 4 x Oculink port + 2 x SATAIII ports (instead of 4 like regular Mini Itx Mobos)&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"/u/poipoipoi\"&gt;u/poipoipoi&lt;/a&gt; also added an Ableconn Dual PCIe NVMe M.2 SSD Adapter Card with 2 x 1TB NVMe for write cache + 1 x 512GB NVMe on the Mobo for Read Cache&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Here is the original post: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/thwykb/newbie_moving_off_synology_and_going_to_build_a/\"&gt;https://www.reddit.com/r/DataHoarder/comments/thwykb/newbie_moving_off_synology_and_going_to_build_a/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Questions:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is 64GB of RAM enough for 80TB-100TB?&lt;/li&gt;\n&lt;li&gt;Is ECC really necessary?&lt;/li&gt;\n&lt;li&gt;Is WIP exclusively managed by the cache drives if I proper configure TrueNAS/UnRaid?&lt;/li&gt;\n&lt;li&gt;Is 10GB LAN really necessary for editing 6K raw video files? (2GB to 15GB per clip)&lt;/li&gt;\n&lt;li&gt;Are CMR drives better for my purpose than SMR drives?&lt;/li&gt;\n&lt;li&gt;Is this 4 core/threads EPYC 3101 a good option or will a 6c/12t Ryzen achieve more performance?&lt;/li&gt;\n&lt;li&gt;If I do a setup similar to &lt;a href=\"/u/poipoipoi\"&gt;u/poipoipoi&lt;/a&gt;, should I add an additional 2.5&amp;quot; SSD for a bootable drive?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Or can a caching/storage drive be used as a bootable drive?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y40rrh", "is_robot_indexable": true, "report_reasons": null, "author": "OrneryReplacement862", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y40rrh/newbie_building_jonsbo_n1_mini_nas_for_raw_6k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y40rrh/newbie_building_jonsbo_n1_mini_nas_for_raw_6k/", "subreddit_subscribers": 647549, "created_utc": 1665770398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nA couple of years ago I used some bash scripting to download about 11,000 archive films off the BFI website in case they were ever put behind a paywall. They're still available to view on BFI but are all now (and newer ones) currently inaccessible to download (brightcove cdn, not figured out how to get at it yet)\n\n&amp;#x200B;\n\nIt's on DHT but I'm the only seeder and slow upload (\\~400KB/s) I just noticed some leechers on my Tixati client a while ago so looks like the meta got picked up.\n\n&amp;#x200B;\n\nCan anyone help seed? You can view/find it here:\n\n&amp;#x200B;\n\n[https://bt4g.org/magnet/57e5b1aee0d46a0c6983ec62f2b6fa7b368d174f](https://bt4g.org/magnet/57e5b1aee0d46a0c6983ec62f2b6fa7b368d174f)\n\n&amp;#x200B;\n\nIt will be a slow burn; I calculated at my upload speed it would take about 2 months to upload the lot.", "author_fullname": "t2_37quudcv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help needed to preserve/seed 11,000 BFI Archive Films (~2.1Tb)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3phti", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665740916.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;A couple of years ago I used some bash scripting to download about 11,000 archive films off the BFI website in case they were ever put behind a paywall. They&amp;#39;re still available to view on BFI but are all now (and newer ones) currently inaccessible to download (brightcove cdn, not figured out how to get at it yet)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s on DHT but I&amp;#39;m the only seeder and slow upload (~400KB/s) I just noticed some leechers on my Tixati client a while ago so looks like the meta got picked up.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can anyone help seed? You can view/find it here:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://bt4g.org/magnet/57e5b1aee0d46a0c6983ec62f2b6fa7b368d174f\"&gt;https://bt4g.org/magnet/57e5b1aee0d46a0c6983ec62f2b6fa7b368d174f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It will be a slow burn; I calculated at my upload speed it would take about 2 months to upload the lot.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3phti", "is_robot_indexable": true, "report_reasons": null, "author": "aPir8", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y3phti/help_needed_to_preserveseed_11000_bfi_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3phti/help_needed_to_preserveseed_11000_bfi_archive/", "subreddit_subscribers": 647549, "created_utc": 1665740916.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello datahoarders!\n\nI've been a lurker here for a while, and I really appreciate the informative community here. I recently purchased a Mac mini M1 with 256GB internal space and 16GB of ram, and I'm looking to use it for school, music production and leisure - it will be my main desktop. Therefore, I am in need of some storage. My question to you all is, how much do I realistically need to invest in to future proof myself? I know this is subjective, so I've conjured up some rough estimates of how much storage I'd need for the different purposes, based on previous experiences and future wants to give you a better understanding, I suppose. Here they are:\n\n&amp;#x200B;\n\nI'm thinking of getting an SSD (Samsung T7) with 2TB:\n\n\\~300GB of Music Project Files on Logic Pro X and Ableton Live\n\n\\~1TB of Audio Plugins / Instrument Libraries\n\nThat's \\~700GB to spare for other activities and such\n\n&amp;#x200B;\n\nI'll also get an HDD for things that don't require a high read/write speed (I think?), my original idea was a 10TB WD Elements External Hard drive:\n\n\\~2TB of TV-Shows\n\n\\~1TB of Movies\n\n\\~500GB of Music rips (from online purchases AND ripping my Vinyl-collection at some point)\n\n\\~300GB of Audiobooks\n\n\\~100GB of Comics\n\n\\~50GB of eBooks\n\nThat's \\~6TB to spare, for now\n\n&amp;#x200B;\n\nIs this a smart way to go about it, or is 6TB wayyyyy too much space to spare? Are there better, or cheaper ways of doing this? I hope these questions are answerable, but I'll happily expand upon anything, if needed.\n\nThanks in advance!", "author_fullname": "t2_76inijfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much storage do I realistically need for music production + leisure (Music, TV, Movies, eBooks etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3ywwd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665765846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello datahoarders!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been a lurker here for a while, and I really appreciate the informative community here. I recently purchased a Mac mini M1 with 256GB internal space and 16GB of ram, and I&amp;#39;m looking to use it for school, music production and leisure - it will be my main desktop. Therefore, I am in need of some storage. My question to you all is, how much do I realistically need to invest in to future proof myself? I know this is subjective, so I&amp;#39;ve conjured up some rough estimates of how much storage I&amp;#39;d need for the different purposes, based on previous experiences and future wants to give you a better understanding, I suppose. Here they are:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of getting an SSD (Samsung T7) with 2TB:&lt;/p&gt;\n\n&lt;p&gt;~300GB of Music Project Files on Logic Pro X and Ableton Live&lt;/p&gt;\n\n&lt;p&gt;~1TB of Audio Plugins / Instrument Libraries&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s ~700GB to spare for other activities and such&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll also get an HDD for things that don&amp;#39;t require a high read/write speed (I think?), my original idea was a 10TB WD Elements External Hard drive:&lt;/p&gt;\n\n&lt;p&gt;~2TB of TV-Shows&lt;/p&gt;\n\n&lt;p&gt;~1TB of Movies&lt;/p&gt;\n\n&lt;p&gt;~500GB of Music rips (from online purchases AND ripping my Vinyl-collection at some point)&lt;/p&gt;\n\n&lt;p&gt;~300GB of Audiobooks&lt;/p&gt;\n\n&lt;p&gt;~100GB of Comics&lt;/p&gt;\n\n&lt;p&gt;~50GB of eBooks&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s ~6TB to spare, for now&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this a smart way to go about it, or is 6TB wayyyyy too much space to spare? Are there better, or cheaper ways of doing this? I hope these questions are answerable, but I&amp;#39;ll happily expand upon anything, if needed.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3ywwd", "is_robot_indexable": true, "report_reasons": null, "author": "Floppi23", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y3ywwd/how_much_storage_do_i_realistically_need_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3ywwd/how_much_storage_do_i_realistically_need_for/", "subreddit_subscribers": 647549, "created_utc": 1665765846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8dw6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "xklb v1.18: subreddit/redditor databases; download from tube and reddit databases; reddit-selftext link extraction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_y432c0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mDQsCVU6p9bYbMkCOZ-eKOx9oldy-4ccA9SF5Pg48Yg.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665776087.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/chapmanjacobd/lb/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bGGX2Uawlc64vMWYFwgisT7WynxSCjNg_7Vh-upnR-4.jpg?auto=webp&amp;s=28f33b18e1e7fa94d8fe139eff68478a2379b4bd", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/bGGX2Uawlc64vMWYFwgisT7WynxSCjNg_7Vh-upnR-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77c77971facf342f57c78cadd62351d86df44d0a", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bGGX2Uawlc64vMWYFwgisT7WynxSCjNg_7Vh-upnR-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=aca45ec743766647104e3a7c5aee8db2f42a78de", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bGGX2Uawlc64vMWYFwgisT7WynxSCjNg_7Vh-upnR-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=864058452ba6a1c25f11f94cdf0e25b0cd54ffd9", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bGGX2Uawlc64vMWYFwgisT7WynxSCjNg_7Vh-upnR-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2c3c494337417f1514a0cdf0934ecd69dc07e5d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bGGX2Uawlc64vMWYFwgisT7WynxSCjNg_7Vh-upnR-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f612dee2ad45b9b8dbd4bdcebb3a8d751184acd", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/bGGX2Uawlc64vMWYFwgisT7WynxSCjNg_7Vh-upnR-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=88eb8f37be9214f96b86a1dd3b633d808d2c5482", "width": 1080, "height": 540}], "variants": {}, "id": "ob8sDkK7HJSxJCaBtV_DV1wQBFtQdUGlDD4w2LFSBhM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "60TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y432c0", "is_robot_indexable": true, "report_reasons": null, "author": "BuonaparteII", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/y432c0/xklb_v118_subredditredditor_databases_download/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/chapmanjacobd/lb/", "subreddit_subscribers": 647549, "created_utc": 1665776087.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Folks,  \n\n\nNeed some help with my product knowledge on overhead scanners. I feel like I've dropped into a bit of an enthusiast black hole which is rare on the internet these days. \n\nKeeping it simple - opinions fine -\n\n\\- why does the Scansnap SV600 still come up in conversations when it's 10 years old? If the product was a success, surely Fujitsu would have released a new version by now? \n\n\\- Is there a current reigning champ for overhead scanners? The SV 600 and CZUR seem to be the main two options, with the rest either a lot less expensive or having some kind of bespoke application that makes the product a little less appealing.\n\nI just need to be able to scan books and ideally product boxes that sometimes have more useful content than the threadbare instructions inside. \n\nThanks to anyone that can help.", "author_fullname": "t2_1jzhoqbr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question re: overhead scanners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3ulvn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665755666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Folks,  &lt;/p&gt;\n\n&lt;p&gt;Need some help with my product knowledge on overhead scanners. I feel like I&amp;#39;ve dropped into a bit of an enthusiast black hole which is rare on the internet these days. &lt;/p&gt;\n\n&lt;p&gt;Keeping it simple - opinions fine -&lt;/p&gt;\n\n&lt;p&gt;- why does the Scansnap SV600 still come up in conversations when it&amp;#39;s 10 years old? If the product was a success, surely Fujitsu would have released a new version by now? &lt;/p&gt;\n\n&lt;p&gt;- Is there a current reigning champ for overhead scanners? The SV 600 and CZUR seem to be the main two options, with the rest either a lot less expensive or having some kind of bespoke application that makes the product a little less appealing.&lt;/p&gt;\n\n&lt;p&gt;I just need to be able to scan books and ideally product boxes that sometimes have more useful content than the threadbare instructions inside. &lt;/p&gt;\n\n&lt;p&gt;Thanks to anyone that can help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3ulvn", "is_robot_indexable": true, "report_reasons": null, "author": "bjd533", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y3ulvn/question_re_overhead_scanners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3ulvn/question_re_overhead_scanners/", "subreddit_subscribers": 647549, "created_utc": 1665755666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Howdy everyone, I was hoping to get some suggestions/help with an upgrade I would like to make to my current setup. \n\nRight now, its a mess, I have a CM stacker case with 5.25\" to 3.5\" adapters taking up the front, I don't even have room for the power button, it is inside the case. I am using cheap pcie sata adapters for the extra ports necessary. I use stable-bits drive pool to manage the storage. I like drivepool, I do not like the trouble it is anytime I need to make a storage change, or find a specific drive... its a nightmare.\n\nso I have a Ryzen 2700x machine I would like to put in its place, but I don't want to rebuild the storage the same way.\n\nsince I have rackspace nearby, I would like to move the 10 or so drives into a hot-swappable rack mount chassis, DAS seems the way to go, so I have been looking at various DAS shelves on ebay. and it fits my budget but I have some questions.\n\nI figure I would be willing to invest around 500 into this, as with most things, if I can save a couple dollars I am ok with it. what would you invest in?\n\nhow much trouble will I have getting my standard ntfs sata drives to show jbod without destroying the data on them on my windows server? \n\nI have been a sysadmin for years, but I am quite unfamiliar with these DAS shelves.", "author_fullname": "t2_6mzp3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with choosing a DAS option.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3wqvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665760755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy everyone, I was hoping to get some suggestions/help with an upgrade I would like to make to my current setup. &lt;/p&gt;\n\n&lt;p&gt;Right now, its a mess, I have a CM stacker case with 5.25&amp;quot; to 3.5&amp;quot; adapters taking up the front, I don&amp;#39;t even have room for the power button, it is inside the case. I am using cheap pcie sata adapters for the extra ports necessary. I use stable-bits drive pool to manage the storage. I like drivepool, I do not like the trouble it is anytime I need to make a storage change, or find a specific drive... its a nightmare.&lt;/p&gt;\n\n&lt;p&gt;so I have a Ryzen 2700x machine I would like to put in its place, but I don&amp;#39;t want to rebuild the storage the same way.&lt;/p&gt;\n\n&lt;p&gt;since I have rackspace nearby, I would like to move the 10 or so drives into a hot-swappable rack mount chassis, DAS seems the way to go, so I have been looking at various DAS shelves on ebay. and it fits my budget but I have some questions.&lt;/p&gt;\n\n&lt;p&gt;I figure I would be willing to invest around 500 into this, as with most things, if I can save a couple dollars I am ok with it. what would you invest in?&lt;/p&gt;\n\n&lt;p&gt;how much trouble will I have getting my standard ntfs sata drives to show jbod without destroying the data on them on my windows server? &lt;/p&gt;\n\n&lt;p&gt;I have been a sysadmin for years, but I am quite unfamiliar with these DAS shelves.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3wqvp", "is_robot_indexable": true, "report_reasons": null, "author": "schmag", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y3wqvp/help_with_choosing_a_das_option/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3wqvp/help_with_choosing_a_das_option/", "subreddit_subscribers": 647549, "created_utc": 1665760755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hey guys, there\u2019s a website with about 300 recipes I\u2019d like to add to my hoard. I\u2019ve just been using the print recipe feature to make pdfs and it works fine, but I was wondering if anyone knows a better way than just doing this about 290 more times? \n\nI\u2019m not interested in archiving the whole website, just the recipes themselves. I\u2019m willing to go through them all individually but I would rather not if there\u2019s an easier way lol. \n\nThanks!", "author_fullname": "t2_1uhlwc4j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading Recipes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4eza9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665809835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey guys, there\u2019s a website with about 300 recipes I\u2019d like to add to my hoard. I\u2019ve just been using the print recipe feature to make pdfs and it works fine, but I was wondering if anyone knows a better way than just doing this about 290 more times? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not interested in archiving the whole website, just the recipes themselves. I\u2019m willing to go through them all individually but I would rather not if there\u2019s an easier way lol. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y4eza9", "is_robot_indexable": true, "report_reasons": null, "author": "UndyingArtist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y4eza9/downloading_recipes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y4eza9/downloading_recipes/", "subreddit_subscribers": 647549, "created_utc": 1665809835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After many months of research, I went with a TB3 4xSATA enclosure and went to work...\n\nInternal chip is reported as:\n\n`ASMedia Technology Inc. ASM1062 Serial ATA Controller (rev 01)`\n\nI installed a pair of old 2tb drives and started badblocks destructive mode to stress the TB3 link and chip. It ran for about 2 days with no errors on the ATA or TB connection.\n\nThen I ran some tests with SMART and hd-idle. SMART pass through worked fine with the exception of an old samsung HD204 2TB disk (known to have some SMART quirks). Continued these tests on the drives I installed later (listed below).\n\nDisks will spin down from hd-idle and hdparm, and they will spin up again when the mounted filesystem is accessed. No filesystem errors.\n\nNext I mounted 2x6TB and 2x5TB drives and began a transfer test. Initiated LUKS encryption then balanced a total of 8TB across all 4 drives with no link resets, full data scrub with no problems. HDD temps stayed around 28C-35C under hours of I/O. Copying data from an individual drive can sustain 200MB/s. I did not saturate the bus with read/s writes to all drives simultaneously. When balancing the data there is a cumulative \\~500MB/s (about 119-130MB/s per drive) for hours.\n\nSet it all up with a mergerfs pool and have been running it as a data server with a few docker containers which periodically write data to the array, and a few containers periodically reading from the array.\n\nSo far I can say everything is working to my liking. Haven't tested hot swap because I don't practice hot swap and I don't know if the device  supports hot swap.\n\nThe most important things are having the drives sleep after inactivity and spin up when accessed, without any ATA, filesystem, or TB link resets/problems.\n\nFor anyone else curious about TB3 external arrays related to data/system stability and behavior, I hope this info helps.\n\nIt's an Akitio TB3, I was pleasantly surprised with the first phone call I made to sales.\n\n[https://www.akitio.com/desktop-storage/akitio-thunder3-quad](https://www.akitio.com/desktop-storage/akitio-thunder3-quad)", "author_fullname": "t2_3z08rn1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My experience with TB3 external enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4aigq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1665795766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After many months of research, I went with a TB3 4xSATA enclosure and went to work...&lt;/p&gt;\n\n&lt;p&gt;Internal chip is reported as:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ASMedia Technology Inc. ASM1062 Serial ATA Controller (rev 01)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I installed a pair of old 2tb drives and started badblocks destructive mode to stress the TB3 link and chip. It ran for about 2 days with no errors on the ATA or TB connection.&lt;/p&gt;\n\n&lt;p&gt;Then I ran some tests with SMART and hd-idle. SMART pass through worked fine with the exception of an old samsung HD204 2TB disk (known to have some SMART quirks). Continued these tests on the drives I installed later (listed below).&lt;/p&gt;\n\n&lt;p&gt;Disks will spin down from hd-idle and hdparm, and they will spin up again when the mounted filesystem is accessed. No filesystem errors.&lt;/p&gt;\n\n&lt;p&gt;Next I mounted 2x6TB and 2x5TB drives and began a transfer test. Initiated LUKS encryption then balanced a total of 8TB across all 4 drives with no link resets, full data scrub with no problems. HDD temps stayed around 28C-35C under hours of I/O. Copying data from an individual drive can sustain 200MB/s. I did not saturate the bus with read/s writes to all drives simultaneously. When balancing the data there is a cumulative ~500MB/s (about 119-130MB/s per drive) for hours.&lt;/p&gt;\n\n&lt;p&gt;Set it all up with a mergerfs pool and have been running it as a data server with a few docker containers which periodically write data to the array, and a few containers periodically reading from the array.&lt;/p&gt;\n\n&lt;p&gt;So far I can say everything is working to my liking. Haven&amp;#39;t tested hot swap because I don&amp;#39;t practice hot swap and I don&amp;#39;t know if the device  supports hot swap.&lt;/p&gt;\n\n&lt;p&gt;The most important things are having the drives sleep after inactivity and spin up when accessed, without any ATA, filesystem, or TB link resets/problems.&lt;/p&gt;\n\n&lt;p&gt;For anyone else curious about TB3 external arrays related to data/system stability and behavior, I hope this info helps.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s an Akitio TB3, I was pleasantly surprised with the first phone call I made to sales.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.akitio.com/desktop-storage/akitio-thunder3-quad\"&gt;https://www.akitio.com/desktop-storage/akitio-thunder3-quad&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/u4veYcBxmWIN_V8phD0kVzJJQkPqzrvWoJjcejBLLwU.jpg?auto=webp&amp;s=b8c0d3cb0be36011ef8dec921343055edc24c57c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/u4veYcBxmWIN_V8phD0kVzJJQkPqzrvWoJjcejBLLwU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f7494472179b825878d4e8cc665a23f4c84309c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/u4veYcBxmWIN_V8phD0kVzJJQkPqzrvWoJjcejBLLwU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1376f527dd1d9bde3e12225d3eda4aa788db395", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/u4veYcBxmWIN_V8phD0kVzJJQkPqzrvWoJjcejBLLwU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0dc787946070a4dabfac40d006b8696c91934b56", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/u4veYcBxmWIN_V8phD0kVzJJQkPqzrvWoJjcejBLLwU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd62da6d5f1c2c77ea9c981d4775cb56083cc0e3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/u4veYcBxmWIN_V8phD0kVzJJQkPqzrvWoJjcejBLLwU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=edea7a2c8fd921f1515a7568e51de713fa6648a4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/u4veYcBxmWIN_V8phD0kVzJJQkPqzrvWoJjcejBLLwU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=295755aed02525eb66e13b3d82632d13f00cf399", "width": 1080, "height": 540}], "variants": {}, "id": "e8ePT1T3p9b5cl5g0yjwYmc-UOVmqjDEAoXAFIo_FlM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y4aigq", "is_robot_indexable": true, "report_reasons": null, "author": "kram3210", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y4aigq/my_experience_with_tb3_external_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y4aigq/my_experience_with_tb3_external_enclosure/", "subreddit_subscribers": 647549, "created_utc": 1665795766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I do regular backups of all of my devices, cards, and external drives, which a lot of the times includes tens of gigabytes of video footage (I do freelance videography work). I know I could most likely just delete all this footage after but as data hoarders understand, we would rather have a thorough archive of what we've shot, even if we'll never use the footage again.\n\nI've recently started using Handbrake to compress some of my archived footage which has been great - I have projects that had over 100GB of video footage in them and compressing the video files has literally halved the amount of storage space needed for those files. The only downside is, the compressed files are brand new files and the old still exist. So the only way I can actually benefit from this system is if I delete the old files, which I'm hesitant to do since those original files have all the \"created at\", \"modified at\", and other data associated with the actual original creation.\n\nWhat I'd love to be able to do is use some kind of tool or process to be able to delete these files but automatically replace them with a little data file that contains all that info. That way if I go looking for the files someday in the future and I navigate to the directory I know they should be in, I can see the files that *were* there plus all the original data from them, but it's just like a .txt file or something like that but which has the same name as the original file (different extension though obviously).\n\nIs anyone aware of something that does this or a workflow I could use to do it manually but in bulk? Any thoughts anyone has would be greatly appreciated! Cheers  \n\n\nEDIT: Forgot to mention, I have both MacOS and Windows machines but usually my backup processes take place on Windows so looking for preferably a tool/process for Windows, but can make Mac work as well.", "author_fullname": "t2_10a2c7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a method of deleting files but keeping some sort of placeholder file indicating that they were there + the metadata associated with them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y44gu2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1665779962.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665779596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I do regular backups of all of my devices, cards, and external drives, which a lot of the times includes tens of gigabytes of video footage (I do freelance videography work). I know I could most likely just delete all this footage after but as data hoarders understand, we would rather have a thorough archive of what we&amp;#39;ve shot, even if we&amp;#39;ll never use the footage again.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently started using Handbrake to compress some of my archived footage which has been great - I have projects that had over 100GB of video footage in them and compressing the video files has literally halved the amount of storage space needed for those files. The only downside is, the compressed files are brand new files and the old still exist. So the only way I can actually benefit from this system is if I delete the old files, which I&amp;#39;m hesitant to do since those original files have all the &amp;quot;created at&amp;quot;, &amp;quot;modified at&amp;quot;, and other data associated with the actual original creation.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;d love to be able to do is use some kind of tool or process to be able to delete these files but automatically replace them with a little data file that contains all that info. That way if I go looking for the files someday in the future and I navigate to the directory I know they should be in, I can see the files that &lt;em&gt;were&lt;/em&gt; there plus all the original data from them, but it&amp;#39;s just like a .txt file or something like that but which has the same name as the original file (different extension though obviously).&lt;/p&gt;\n\n&lt;p&gt;Is anyone aware of something that does this or a workflow I could use to do it manually but in bulk? Any thoughts anyone has would be greatly appreciated! Cheers  &lt;/p&gt;\n\n&lt;p&gt;EDIT: Forgot to mention, I have both MacOS and Windows machines but usually my backup processes take place on Windows so looking for preferably a tool/process for Windows, but can make Mac work as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y44gu2", "is_robot_indexable": true, "report_reasons": null, "author": "DoctorVanNostrandMD", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y44gu2/looking_for_a_method_of_deleting_files_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y44gu2/looking_for_a_method_of_deleting_files_but/", "subreddit_subscribers": 647549, "created_utc": 1665779596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have tried this code snippet but it is not working\n\n`mkdir\u00a0-p\u00a0~/.config/youtube-dl/`  \n`cat\u00a0&gt;\u00a0~/.config/youtube-dl/config\u00a0&lt;&lt;EOF`  \n`-o\u00a0\"[%(upload_date)s][%(id)s]\u00a0%(title)s\u00a0(by\u00a0%(uploader)s).%(ext)s\"`  \n`--external-downloader\u00a0aria2c`  \n`--external-downloader-args\u00a0\"-c\u00a0-j\u00a03\u00a0-x\u00a03\u00a0-s\u00a03\u00a0-k\u00a01M\"`  \n`EOF`", "author_fullname": "t2_87dvtck6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to configure youtube-dl and aria2 together to fast download?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3o4qq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665736030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried this code snippet but it is not working&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;mkdir\u00a0-p\u00a0~/.config/youtube-dl/&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;cat\u00a0&amp;gt;\u00a0~/.config/youtube-dl/config\u00a0&amp;lt;&amp;lt;EOF&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;-o\u00a0&amp;quot;[%(upload_date)s][%(id)s]\u00a0%(title)s\u00a0(by\u00a0%(uploader)s).%(ext)s&amp;quot;&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;--external-downloader\u00a0aria2c&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;--external-downloader-args\u00a0&amp;quot;-c\u00a0-j\u00a03\u00a0-x\u00a03\u00a0-s\u00a03\u00a0-k\u00a01M&amp;quot;&lt;/code&gt;&lt;br/&gt;\n&lt;code&gt;EOF&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "10TB To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3o4qq", "is_robot_indexable": true, "report_reasons": null, "author": "Firm-Bunch-5049", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/y3o4qq/how_to_configure_youtubedl_and_aria2_together_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3o4qq/how_to_configure_youtubedl_and_aria2_together_to/", "subreddit_subscribers": 647549, "created_utc": 1665736030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For those who don't know : [https://www.hetzner.com/storage/storage-box](https://www.hetzner.com/storage/storage-box)  \n\n\n  \nI searched on Google for a software that could help me do that (something similar to dropbox/backblaze backup), but didn't find anything.  \n\n\nThanks in advance!", "author_fullname": "t2_j2jycayf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to find a way to automatically backup a certain folder/drive to a Hetzner Storage box.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3tql8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665753455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who don&amp;#39;t know : &lt;a href=\"https://www.hetzner.com/storage/storage-box\"&gt;https://www.hetzner.com/storage/storage-box&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I searched on Google for a software that could help me do that (something similar to dropbox/backblaze backup), but didn&amp;#39;t find anything.  &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3tql8", "is_robot_indexable": true, "report_reasons": null, "author": "ketaminejunkie1337", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y3tql8/looking_to_find_a_way_to_automatically_backup_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3tql8/looking_to_find_a_way_to_automatically_backup_a/", "subreddit_subscribers": 647549, "created_utc": 1665753455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, i am planning to buy ultrium 1840 tape drive. \n\nI would like to connect it with my motherboard  GA-EX58-DS4 via sata, but the drive has  Ultra320 LVD SCSI interface. \n\nWhat kind of adapter do i need?\n\nSorry if this is the wrong sub.", "author_fullname": "t2_6banu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HP Ultrium 1840 tpo 4", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3p9bg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665740099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i am planning to buy ultrium 1840 tape drive. &lt;/p&gt;\n\n&lt;p&gt;I would like to connect it with my motherboard  GA-EX58-DS4 via sata, but the drive has  Ultra320 LVD SCSI interface. &lt;/p&gt;\n\n&lt;p&gt;What kind of adapter do i need?&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is the wrong sub.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3p9bg", "is_robot_indexable": true, "report_reasons": null, "author": "ljutabrlja", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y3p9bg/hp_ultrium_1840_tpo_4/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3p9bg/hp_ultrium_1840_tpo_4/", "subreddit_subscribers": 647549, "created_utc": 1665740099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Cheapest I've seen them lately are about 70-75 dollars a TB.\n\nWhen you take into consideration, their read/write speeds cap at roughly 530GB/sec  and the prices and performance of M.2 NVMe or the cheap TB Price of HDD ($13-15/TB) they have to be of the worse values out there for storage.\n\nIs it because they can fit laptops due to their 2.5\" form factor?\n\nUnless of course, I'm missing something obvious, would not be the first time.\n\nPlease share your input, thank you.", "author_fullname": "t2_11dj0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts about the SAMSUNG 870 QVO SATA III 2.5\" SSD 8TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y4g45a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665813762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cheapest I&amp;#39;ve seen them lately are about 70-75 dollars a TB.&lt;/p&gt;\n\n&lt;p&gt;When you take into consideration, their read/write speeds cap at roughly 530GB/sec  and the prices and performance of M.2 NVMe or the cheap TB Price of HDD ($13-15/TB) they have to be of the worse values out there for storage.&lt;/p&gt;\n\n&lt;p&gt;Is it because they can fit laptops due to their 2.5&amp;quot; form factor?&lt;/p&gt;\n\n&lt;p&gt;Unless of course, I&amp;#39;m missing something obvious, would not be the first time.&lt;/p&gt;\n\n&lt;p&gt;Please share your input, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "130TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "y4g45a", "is_robot_indexable": true, "report_reasons": null, "author": "nando1969", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/y4g45a/thoughts_about_the_samsung_870_qvo_sata_iii_25/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y4g45a/thoughts_about_the_samsung_870_qvo_sata_iii_25/", "subreddit_subscribers": 647549, "created_utc": 1665813762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been googling for a day, but unable to find what should be an easy answer. This is a super noob question I'm sure, but I'm at a loss. I have quite a large amount of data that I need to run a script on, and want the hard drives where I'm storing and saving the data to be mirrored to prevent issues. The amount of data that is actually being manipulated is more then I'm used to, and big enough that I am actually using a surprising amount of storage (satellite images, with smallish pixels, over many years, many times a year, for a large area). \n\nUpgrading my workstation to work with and edit some large datasets. I have 4 internal hard drives: two identical 1TB SSDs a separate 2TB drive for backup, and a Boot drive is at 500GB. Cloud-based backup as well, but that is not important. \n\nI have been trying to get the two identical 1TB SSDs to be mirrors of each other, (Ideally maintaining the files in Disk 3, but I have room to move them off if I need to reformat) but the option is not even showing up in Windows. (see photo). I'm running Windows 10 home. My other windows machine does have the option. I'm at a loss, and tutorial videos are not helpful at this point. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/mz95r24gfwt91.png?width=767&amp;format=png&amp;auto=webp&amp;s=17345abafe6ada11e45c41932fb356edb88d8956", "author_fullname": "t2_5vkgrd9n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mirror drive not showing up as an option", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "media_metadata": {"mz95r24gfwt91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/mz95r24gfwt91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13f42ed4613c5b11f9cf5ff60020a8a13aaa1f03"}, {"y": 84, "x": 216, "u": "https://preview.redd.it/mz95r24gfwt91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=02286cb6365c464b160387f81254c2039238ab89"}, {"y": 124, "x": 320, "u": "https://preview.redd.it/mz95r24gfwt91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=05ca5fe533a777c0858b34921e4412231549ec15"}, {"y": 249, "x": 640, "u": "https://preview.redd.it/mz95r24gfwt91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=12be49c49ef4647241bbeb6054699c6b34c75e06"}], "s": {"y": 299, "x": 767, "u": "https://preview.redd.it/mz95r24gfwt91.png?width=767&amp;format=png&amp;auto=webp&amp;s=17345abafe6ada11e45c41932fb356edb88d8956"}, "id": "mz95r24gfwt91"}}, "name": "t3_y4etjq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WIkCKSDz7xRNUW4tR3VmlbCgCbnwOAjwXNWPPt6U6zU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665809275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been googling for a day, but unable to find what should be an easy answer. This is a super noob question I&amp;#39;m sure, but I&amp;#39;m at a loss. I have quite a large amount of data that I need to run a script on, and want the hard drives where I&amp;#39;m storing and saving the data to be mirrored to prevent issues. The amount of data that is actually being manipulated is more then I&amp;#39;m used to, and big enough that I am actually using a surprising amount of storage (satellite images, with smallish pixels, over many years, many times a year, for a large area). &lt;/p&gt;\n\n&lt;p&gt;Upgrading my workstation to work with and edit some large datasets. I have 4 internal hard drives: two identical 1TB SSDs a separate 2TB drive for backup, and a Boot drive is at 500GB. Cloud-based backup as well, but that is not important. &lt;/p&gt;\n\n&lt;p&gt;I have been trying to get the two identical 1TB SSDs to be mirrors of each other, (Ideally maintaining the files in Disk 3, but I have room to move them off if I need to reformat) but the option is not even showing up in Windows. (see photo). I&amp;#39;m running Windows 10 home. My other windows machine does have the option. I&amp;#39;m at a loss, and tutorial videos are not helpful at this point. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mz95r24gfwt91.png?width=767&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17345abafe6ada11e45c41932fb356edb88d8956\"&gt;https://preview.redd.it/mz95r24gfwt91.png?width=767&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17345abafe6ada11e45c41932fb356edb88d8956&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y4etjq", "is_robot_indexable": true, "report_reasons": null, "author": "Geog_Master", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y4etjq/mirror_drive_not_showing_up_as_an_option/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y4etjq/mirror_drive_not_showing_up_as_an_option/", "subreddit_subscribers": 647549, "created_utc": 1665809275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I apologize if this isn't really a DataHoarder question, but I wasn't sure where else to ask.   \n\nThe drives I mentioned are also a different part number (PN).    \n  \nAfter speaking with a Seagate rep, he mentioned there shouldn't really be a big difference. But I have screenshots and test results showing that the temp of the new drives (with a different part number) are +10\u00b0F and causing the drives around them to go up by around +8\u00b0F.    \nThe PN's in question are 2M7101-500 and 1ZF11G-500.    \nThis is the second RMA for these drives, so I don't know what to do at this point.    \n\nAm I just overreacting?\n    \n    \nedit: there are 10 drives total in my NAS and I am having this issue with 3 of them.", "author_fullname": "t2_wfilq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I overreacting? RMA'd some Seagate HDD's and the temp on the replacement drives are +10-13\u00b0F", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y4aeox", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665795462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize if this isn&amp;#39;t really a DataHoarder question, but I wasn&amp;#39;t sure where else to ask.   &lt;/p&gt;\n\n&lt;p&gt;The drives I mentioned are also a different part number (PN).    &lt;/p&gt;\n\n&lt;p&gt;After speaking with a Seagate rep, he mentioned there shouldn&amp;#39;t really be a big difference. But I have screenshots and test results showing that the temp of the new drives (with a different part number) are +10\u00b0F and causing the drives around them to go up by around +8\u00b0F.&lt;br/&gt;\nThe PN&amp;#39;s in question are 2M7101-500 and 1ZF11G-500.&lt;br/&gt;\nThis is the second RMA for these drives, so I don&amp;#39;t know what to do at this point.    &lt;/p&gt;\n\n&lt;p&gt;Am I just overreacting?&lt;/p&gt;\n\n&lt;p&gt;edit: there are 10 drives total in my NAS and I am having this issue with 3 of them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y4aeox", "is_robot_indexable": true, "report_reasons": null, "author": "ComcastIsWatchingMe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y4aeox/am_i_overreacting_rmad_some_seagate_hdds_and_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y4aeox/am_i_overreacting_rmad_some_seagate_hdds_and_the/", "subreddit_subscribers": 647549, "created_utc": 1665795462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to come up with something similar to those expensive DAS units that I can configure myself.\n\nI found a unit that even used BTRFS, but I would assume their \"gui\" wouldn't include the same level of features as accessing the drives themselves.\n\nSo, I wonder what options are available to build an external DAS unit that works over usb 3.2 and or thunderbolt 3/4?\n\n&amp;#x200B;\n\nAlternatively, an external drive enclosure that supports raid 1+0?\n\nAlso, would you recommend if going the enclosure route that I have an extra external drive to keep important files safe?\n\n&amp;#x200B;\n\nWhat are your experiences with roadwarrior style for managing storage?  \nI'll have a large lithium battery to run this, plugged into the 12v on a vehicle. I don't need much storage on the go, but I'd like to be able to keep VM backups and storage for video/photos local.\n\nI'm also likely to have the option for remote VMs, but this is a road I've yet to cross.", "author_fullname": "t2_gru9m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roadwarrior configuration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y47mjv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665787559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to come up with something similar to those expensive DAS units that I can configure myself.&lt;/p&gt;\n\n&lt;p&gt;I found a unit that even used BTRFS, but I would assume their &amp;quot;gui&amp;quot; wouldn&amp;#39;t include the same level of features as accessing the drives themselves.&lt;/p&gt;\n\n&lt;p&gt;So, I wonder what options are available to build an external DAS unit that works over usb 3.2 and or thunderbolt 3/4?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Alternatively, an external drive enclosure that supports raid 1+0?&lt;/p&gt;\n\n&lt;p&gt;Also, would you recommend if going the enclosure route that I have an extra external drive to keep important files safe?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What are your experiences with roadwarrior style for managing storage?&lt;br/&gt;\nI&amp;#39;ll have a large lithium battery to run this, plugged into the 12v on a vehicle. I don&amp;#39;t need much storage on the go, but I&amp;#39;d like to be able to keep VM backups and storage for video/photos local.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also likely to have the option for remote VMs, but this is a road I&amp;#39;ve yet to cross.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y47mjv", "is_robot_indexable": true, "report_reasons": null, "author": "uberbewb", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y47mjv/roadwarrior_configuration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y47mjv/roadwarrior_configuration/", "subreddit_subscribers": 647549, "created_utc": 1665787559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, \nI'm in the process of digitizing a large storage of paper documents. I have access to good office printers, so scanning most of the standard documents was a breeze. But I'm having issues with larger format documents. Especially paper rolls (so about 3-4m x 80cm wide), but also larger DIN A0 documents (e.g. Blueprints, plans, family trees, posters, etc.). \nI checked local services, but they can only handle DIN formats, but not the rolls. \nDo you guys have any ideas on how I could digitize them in somewhat good quality? I was thinking of taking lots of photos and stitching them with photomerge in Photoshop, but that failed quite badly. \nAre there maybe some scanner apps specialized for this, or are there any other techniques than I'm missing? \nThanks for any help in advance!", "author_fullname": "t2_uxafe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digitizing large format paper documents for storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y46ool", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665785158.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, \nI&amp;#39;m in the process of digitizing a large storage of paper documents. I have access to good office printers, so scanning most of the standard documents was a breeze. But I&amp;#39;m having issues with larger format documents. Especially paper rolls (so about 3-4m x 80cm wide), but also larger DIN A0 documents (e.g. Blueprints, plans, family trees, posters, etc.). \nI checked local services, but they can only handle DIN formats, but not the rolls. \nDo you guys have any ideas on how I could digitize them in somewhat good quality? I was thinking of taking lots of photos and stitching them with photomerge in Photoshop, but that failed quite badly. \nAre there maybe some scanner apps specialized for this, or are there any other techniques than I&amp;#39;m missing? \nThanks for any help in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y46ool", "is_robot_indexable": true, "report_reasons": null, "author": "corny96", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y46ool/digitizing_large_format_paper_documents_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y46ool/digitizing_large_format_paper_documents_for/", "subreddit_subscribers": 647549, "created_utc": 1665785158.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First off if this is the wrong place or wrong question i am sorry ill look elsewhere.\n\nI have 2x3tb 2x8tb of growing data. All drives are external and really spread out , not sure the total actual data vs copies of data (future project) im looking to build a second PC for shucking drives in the future and build up a better setup. My question is what route should i go? Should i purchase 2 10+tb drives and put them into the new system and copy all my data over or just keep adding and growing my current setup. The 2 n 8 drives will go into a 10bay pc and be replaced once bay is full or drives die off. I've not played or tried any raid setup. Ive just been manually cloning the drives once a year.", "author_fullname": "t2_3suaq7sd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "building a NAS for my growing collection.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y44xge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665780763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First off if this is the wrong place or wrong question i am sorry ill look elsewhere.&lt;/p&gt;\n\n&lt;p&gt;I have 2x3tb 2x8tb of growing data. All drives are external and really spread out , not sure the total actual data vs copies of data (future project) im looking to build a second PC for shucking drives in the future and build up a better setup. My question is what route should i go? Should i purchase 2 10+tb drives and put them into the new system and copy all my data over or just keep adding and growing my current setup. The 2 n 8 drives will go into a 10bay pc and be replaced once bay is full or drives die off. I&amp;#39;ve not played or tried any raid setup. Ive just been manually cloning the drives once a year.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y44xge", "is_robot_indexable": true, "report_reasons": null, "author": "JBizz86", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y44xge/building_a_nas_for_my_growing_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y44xge/building_a_nas_for_my_growing_collection/", "subreddit_subscribers": 647549, "created_utc": 1665780763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As the title indicates, I'm looking for S3 compatible storage provider that I can use with rclone on Linux to dump (long term backup) files. These will be primarily Borg backup files from my NAS storage. Any suggestions for a reasonably priced provider for home use segment?", "author_fullname": "t2_s7edcv9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for S3 compatible cold storage provider with ultra low price tag?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y44rep", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665780337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title indicates, I&amp;#39;m looking for S3 compatible storage provider that I can use with rclone on Linux to dump (long term backup) files. These will be primarily Borg backup files from my NAS storage. Any suggestions for a reasonably priced provider for home use segment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y44rep", "is_robot_indexable": true, "report_reasons": null, "author": "useless-oracle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y44rep/recommendations_for_s3_compatible_cold_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y44rep/recommendations_for_s3_compatible_cold_storage/", "subreddit_subscribers": 647549, "created_utc": 1665780337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest way to archive an entire website subdirectory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y43zou", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_shla9q08", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "WaybackMachine", "selftext": "Before anyone tells me to Google it, I did and it was unsurprisingly futile \ud83d\ude05. The algorithm isn't exactly designed to provide helpful results anymore, so naturally I've come to Reddit for answers.\n\nEarlier this week, I was laid off from a nascent digital media publication after finding out our site was being shuttered. We were to our parent company what The Wirecutter became to the New York Times a few years ago: a subdirectory hosted on a more prominent domain offering purchasing advice based on product testing insights.\n\nAs everyone on my team is about to lose all record of the hard work we put in over the last six months, I'm desperate to find a straightforward method of archiving every page in the subdirectory without manually pasting every URL into Wayback Machine. I'm skeptical the company is going to provide a backup given how long it's taking to reach a decision on whether or not they even want to. It's seemingly in limbo, and I'm not sure how much time we have before the site goes dark.\n\n[HTTrack](https://www.httrack.com/) seems to be a popular option, but I don't have a Windows or Linux machine, as I exclusively work on a Mac, and it doesn't appear to be compatible with MacOS. I also found [WaybackArchiver](https://github.com/buren/wayback_archiver) on GitHub, but I'm out of my depth just trying to follow the installation instructions. I like to think i'm tech literate, but I only have a vague understanding of sitemaps, let alone how to use a crawler or what a Gemfile even is. \n\nPlease, if anyone has a solution to automate a full subdirectory archival a layman could quickly pick up and execute, I'll owe you a beer or a blunt \u2014 whatever floats your boat.", "author_fullname": "t2_shla9q08", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest way to archive an entire website subdirectory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/WaybackMachine", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y43z5s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1665778353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WaybackMachine", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Before anyone tells me to Google it, I did and it was unsurprisingly futile \ud83d\ude05. The algorithm isn&amp;#39;t exactly designed to provide helpful results anymore, so naturally I&amp;#39;ve come to Reddit for answers.&lt;/p&gt;\n\n&lt;p&gt;Earlier this week, I was laid off from a nascent digital media publication after finding out our site was being shuttered. We were to our parent company what The Wirecutter became to the New York Times a few years ago: a subdirectory hosted on a more prominent domain offering purchasing advice based on product testing insights.&lt;/p&gt;\n\n&lt;p&gt;As everyone on my team is about to lose all record of the hard work we put in over the last six months, I&amp;#39;m desperate to find a straightforward method of archiving every page in the subdirectory without manually pasting every URL into Wayback Machine. I&amp;#39;m skeptical the company is going to provide a backup given how long it&amp;#39;s taking to reach a decision on whether or not they even want to. It&amp;#39;s seemingly in limbo, and I&amp;#39;m not sure how much time we have before the site goes dark.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.httrack.com/\"&gt;HTTrack&lt;/a&gt; seems to be a popular option, but I don&amp;#39;t have a Windows or Linux machine, as I exclusively work on a Mac, and it doesn&amp;#39;t appear to be compatible with MacOS. I also found &lt;a href=\"https://github.com/buren/wayback_archiver\"&gt;WaybackArchiver&lt;/a&gt; on GitHub, but I&amp;#39;m out of my depth just trying to follow the installation instructions. I like to think i&amp;#39;m tech literate, but I only have a vague understanding of sitemaps, let alone how to use a crawler or what a Gemfile even is. &lt;/p&gt;\n\n&lt;p&gt;Please, if anyone has a solution to automate a full subdirectory archival a layman could quickly pick up and execute, I&amp;#39;ll owe you a beer or a blunt \u2014 whatever floats your boat.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tgdafAt2hOfyEFP2YVpJAL51ZycsTiEt8VxN2HoM1eg.jpg?auto=webp&amp;s=d7f0508bee931f5ec01c81898799c76e53a49c3a", "width": 300, "height": 233}, "resolutions": [{"url": "https://external-preview.redd.it/tgdafAt2hOfyEFP2YVpJAL51ZycsTiEt8VxN2HoM1eg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ddefedb0f954f724be9d99fc4940672fa80d82c", "width": 108, "height": 83}, {"url": "https://external-preview.redd.it/tgdafAt2hOfyEFP2YVpJAL51ZycsTiEt8VxN2HoM1eg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=881ad6280f8960bc672a6e1d3566590968c5eca8", "width": 216, "height": 167}], "variants": {}, "id": "IqlRJ_uGr1gJ9Z81mzoF33bSshgJZY-BlRfO9FbM4NA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qptp", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y43z5s", "is_robot_indexable": true, "report_reasons": null, "author": "FrackingEnthusiast", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/WaybackMachine/comments/y43z5s/easiest_way_to_archive_an_entire_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/WaybackMachine/comments/y43z5s/easiest_way_to_archive_an_entire_website/", "subreddit_subscribers": 1463, "created_utc": 1665778353.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1665778393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WaybackMachine", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/WaybackMachine/comments/y43z5s/easiest_way_to_archive_an_entire_website/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tgdafAt2hOfyEFP2YVpJAL51ZycsTiEt8VxN2HoM1eg.jpg?auto=webp&amp;s=d7f0508bee931f5ec01c81898799c76e53a49c3a", "width": 300, "height": 233}, "resolutions": [{"url": "https://external-preview.redd.it/tgdafAt2hOfyEFP2YVpJAL51ZycsTiEt8VxN2HoM1eg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ddefedb0f954f724be9d99fc4940672fa80d82c", "width": 108, "height": 83}, {"url": "https://external-preview.redd.it/tgdafAt2hOfyEFP2YVpJAL51ZycsTiEt8VxN2HoM1eg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=881ad6280f8960bc672a6e1d3566590968c5eca8", "width": 216, "height": 167}], "variants": {}, "id": "IqlRJ_uGr1gJ9Z81mzoF33bSshgJZY-BlRfO9FbM4NA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y43zou", "is_robot_indexable": true, "report_reasons": null, "author": "FrackingEnthusiast", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_y43z5s", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y43zou/easiest_way_to_archive_an_entire_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/WaybackMachine/comments/y43z5s/easiest_way_to_archive_an_entire_website/", "subreddit_subscribers": 647549, "created_utc": 1665778393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nI wanted to upload my files to a cloud that I know my files there will be secured and not be worried of my account being hacked.\n\nWould really appreciate your suggestions.", "author_fullname": "t2_eaodwhbx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The best cloud storage in terms of security?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y3v21f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1665756732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I wanted to upload my files to a cloud that I know my files there will be secured and not be worried of my account being hacked.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate your suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y3v21f", "is_robot_indexable": true, "report_reasons": null, "author": "Yanfei_is_me", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y3v21f/the_best_cloud_storage_in_terms_of_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/y3v21f/the_best_cloud_storage_in_terms_of_security/", "subreddit_subscribers": 647549, "created_utc": 1665756732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_v2a9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Publish Or Perish: Data Storage And Civilization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_y4fkca", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/SEOnwRUFlAgpTlKFgnwlf_6DJKtT56WGjjV4BoN1k8U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1665811815.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hackaday.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hackaday.com/2022/10/13/publish-or-perish-data-storage-and-civilization/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QEfK8FDE2X79ayo7_XsFLAqvsJXPxDFme5k1rR9s2vo.jpg?auto=webp&amp;s=a5cf9782ac4da3cfc3090422ec564d09b56aa36c", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/QEfK8FDE2X79ayo7_XsFLAqvsJXPxDFme5k1rR9s2vo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=60e9a86cfdeddd9f3072a6a6f223577f5a70c9ac", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/QEfK8FDE2X79ayo7_XsFLAqvsJXPxDFme5k1rR9s2vo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25475cb6e83a5519e6e350b77a2852874f54c283", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/QEfK8FDE2X79ayo7_XsFLAqvsJXPxDFme5k1rR9s2vo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ed72f89a8985b8bfaa280da9d2562d3c0fc64e2", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/QEfK8FDE2X79ayo7_XsFLAqvsJXPxDFme5k1rR9s2vo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=37de3bc84c9668ed36ebc7a020171a6ec3d30906", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/QEfK8FDE2X79ayo7_XsFLAqvsJXPxDFme5k1rR9s2vo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0db5c9aebc820576ffd7cdfb8975cc9f5704da5", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/QEfK8FDE2X79ayo7_XsFLAqvsJXPxDFme5k1rR9s2vo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=63b9c4d617b4b6daf9f669c40cdef5e92fce8a78", "width": 1080, "height": 720}], "variants": {}, "id": "_tPTj5dA7ezvomdc8VxxDD8G73pOaBgpTlD3uP9t6aY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "y4fkca", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy-Red-Fox", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/y4fkca/publish_or_perish_data_storage_and_civilization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hackaday.com/2022/10/13/publish-or-perish-data-storage-and-civilization/", "subreddit_subscribers": 647549, "created_utc": 1665811815.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}