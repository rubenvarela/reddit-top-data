{"kind": "Listing", "data": {"after": "t3_y82jtt", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Do you link to your Kaggle? Or perhaps your Github, which contains the underlying .ipynb files? I want to make sure I\u2019m communicating my work in a way that aligns with how other data science practitioners do it. \n\nThanks for your input!", "author_fullname": "t2_ceqh7wvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you present your portfolio on LinkedIn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y87smb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 105, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Job Search", "can_mod_post": false, "score": 105, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666198209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you link to your Kaggle? Or perhaps your Github, which contains the underlying .ipynb files? I want to make sure I\u2019m communicating my work in a way that aligns with how other data science practitioners do it. &lt;/p&gt;\n\n&lt;p&gt;Thanks for your input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "71803d7a-469d-11e9-890b-0e5d959976c8", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#edeff1", "id": "y87smb", "is_robot_indexable": true, "report_reasons": null, "author": "boston_acc", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y87smb/how_do_you_present_your_portfolio_on_linkedin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y87smb/how_do_you_present_your_portfolio_on_linkedin/", "subreddit_subscribers": 814512, "created_utc": 1666198209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Business Objective - The company started a LTIP (long term incentive program) to compensate the employees as a replacement of stocks and finance team wants to track the program and **figure out if the plan is effective in keeping the talent at the company.** How do I figure out if this program **caused** the turnover rate to drop?\n\nThis program is very new and the data we have is the performance of each individual. For example, the units assigned to each employee but have never looked/tracked at how they are been distributed as per employee.\n\nHow units are assigned? On the day of January 1st of each year, each employee gets certain LTIP unit (1 unit = 1USD). But the number of units each employee gets is dependent on the manager and number of team members under manager.\n\nAll the units are vested after 3rd year. Price of unit is dependent on how the company grows.\n\nHow the amount is calculated?Let\u2019s say I receive a 1000 units on Jan 1st, 2019 and the company grew 150% across 3 years. On Jan 1st 2022, the amount will be 1500$ compared to 1000$ in 2019.", "author_fullname": "t2_rochx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do analysis on this data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8lop0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666232671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Business Objective - The company started a LTIP (long term incentive program) to compensate the employees as a replacement of stocks and finance team wants to track the program and &lt;strong&gt;figure out if the plan is effective in keeping the talent at the company.&lt;/strong&gt; How do I figure out if this program &lt;strong&gt;caused&lt;/strong&gt; the turnover rate to drop?&lt;/p&gt;\n\n&lt;p&gt;This program is very new and the data we have is the performance of each individual. For example, the units assigned to each employee but have never looked/tracked at how they are been distributed as per employee.&lt;/p&gt;\n\n&lt;p&gt;How units are assigned? On the day of January 1st of each year, each employee gets certain LTIP unit (1 unit = 1USD). But the number of units each employee gets is dependent on the manager and number of team members under manager.&lt;/p&gt;\n\n&lt;p&gt;All the units are vested after 3rd year. Price of unit is dependent on how the company grows.&lt;/p&gt;\n\n&lt;p&gt;How the amount is calculated?Let\u2019s say I receive a 1000 units on Jan 1st, 2019 and the company grew 150% across 3 years. On Jan 1st 2022, the amount will be 1500$ compared to 1000$ in 2019.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8lop0", "is_robot_indexable": true, "report_reasons": null, "author": "yashk1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8lop0/how_to_do_analysis_on_this_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8lop0/how_to_do_analysis_on_this_data/", "subreddit_subscribers": 814512, "created_utc": 1666232671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, so I have a dataset with 8000 rows and 200+ features, mostly numerical. By looking at summary statistics, I see that a good chunk of these features (100ish) have zeroes for 75%+ of their entries. Seems to me like all these zeroes won't contain any useful info for an eventual model, and it would simplify my process a lot to reduce the dimensionality. Would it be wise to simply remove features like this and continue on my way or would I be missing out on lots of possible info? Maybe take all of these sparse columns and use PCA to hopefully reduce them to one or two principal components?", "author_fullname": "t2_d86jaoff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Filtering out features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8fnsj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666216639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, so I have a dataset with 8000 rows and 200+ features, mostly numerical. By looking at summary statistics, I see that a good chunk of these features (100ish) have zeroes for 75%+ of their entries. Seems to me like all these zeroes won&amp;#39;t contain any useful info for an eventual model, and it would simplify my process a lot to reduce the dimensionality. Would it be wise to simply remove features like this and continue on my way or would I be missing out on lots of possible info? Maybe take all of these sparse columns and use PCA to hopefully reduce them to one or two principal components?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8fnsj", "is_robot_indexable": true, "report_reasons": null, "author": "Objective-Simple-836", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8fnsj/filtering_out_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8fnsj/filtering_out_features/", "subreddit_subscribers": 814512, "created_utc": 1666216639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Dear fellow smart data expert \n\nI\u2019m working on this Ubereats project to measure/determine restaurant\u2019s success through these variables \nI\u2019ve these variables (restaurant ratings 1-6) ,(restaurant ratings count 1-7k) , (food hygiene ratings 1-5) (daily deals %) (spend $ )  (delivery fees $)\n\nWhat confuse me most is some restaurants have high ratings with low ratings counts \n\nCan\u2019t think further from this \n\nHelp \n\nThanks", "author_fullname": "t2_6ku703xx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I determine if a restaurant is doing well through these variables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y87aea", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666197013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear fellow smart data expert &lt;/p&gt;\n\n&lt;p&gt;I\u2019m working on this Ubereats project to measure/determine restaurant\u2019s success through these variables \nI\u2019ve these variables (restaurant ratings 1-6) ,(restaurant ratings count 1-7k) , (food hygiene ratings 1-5) (daily deals %) (spend $ )  (delivery fees $)&lt;/p&gt;\n\n&lt;p&gt;What confuse me most is some restaurants have high ratings with low ratings counts &lt;/p&gt;\n\n&lt;p&gt;Can\u2019t think further from this &lt;/p&gt;\n\n&lt;p&gt;Help &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y87aea", "is_robot_indexable": true, "report_reasons": null, "author": "Street-Target9245", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y87aea/how_can_i_determine_if_a_restaurant_is_doing_well/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y87aea/how_can_i_determine_if_a_restaurant_is_doing_well/", "subreddit_subscribers": 814512, "created_utc": 1666197013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Data collected from the census shows advancement in the student-teacher ratio when compared for 2012 and 2021. Such improvements in the education sector are crucial indicators of long-term economic growth through human capital investment.   \n\n\nhttps://preview.redd.it/1cuuhtvb7yu91.png?width=848&amp;format=png&amp;auto=webp&amp;s=9bca24461ab09895fd92fff3272bdd67041a1e72\n\nhttps://preview.redd.it/puelhe1a7yu91.png?width=866&amp;format=png&amp;auto=webp&amp;s=39a5a3bc783f90c0aabfbf5c72045456e6f5f52f", "author_fullname": "t2_q7jl09p1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Student-teacher ratio 2012 vs 2021", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 138, "top_awarded_type": null, "hide_score": true, "media_metadata": {"1cuuhtvb7yu91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/1cuuhtvb7yu91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9504e0a1846416060ae3965b1b931aaf57854e87"}, {"y": 213, "x": 216, "u": "https://preview.redd.it/1cuuhtvb7yu91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1410a71ae5d6d6233d2e6d4ccd77db23d0cee7a"}, {"y": 315, "x": 320, "u": "https://preview.redd.it/1cuuhtvb7yu91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ace667aa26daefdb0c39ea538b1cdd281aa65f09"}, {"y": 631, "x": 640, "u": "https://preview.redd.it/1cuuhtvb7yu91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=084d7a2f687b0f04eb6c7862b5939d5d44bb69fd"}], "s": {"y": 837, "x": 848, "u": "https://preview.redd.it/1cuuhtvb7yu91.png?width=848&amp;format=png&amp;auto=webp&amp;s=9bca24461ab09895fd92fff3272bdd67041a1e72"}, "id": "1cuuhtvb7yu91"}, "puelhe1a7yu91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 102, "x": 108, "u": "https://preview.redd.it/puelhe1a7yu91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b2428bfca5a59ab08648020c1584e8e577d2971"}, {"y": 205, "x": 216, "u": "https://preview.redd.it/puelhe1a7yu91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e2502e6a59a6f3e46f70d744a595b28e3825cba"}, {"y": 304, "x": 320, "u": "https://preview.redd.it/puelhe1a7yu91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dcb4650a22aec8580c9adbb87e022620a9350dde"}, {"y": 609, "x": 640, "u": "https://preview.redd.it/puelhe1a7yu91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ebab4e751e74df771869c94ce25e48659c9940d2"}], "s": {"y": 825, "x": 866, "u": "https://preview.redd.it/puelhe1a7yu91.png?width=866&amp;format=png&amp;auto=webp&amp;s=39a5a3bc783f90c0aabfbf5c72045456e6f5f52f"}, "id": "puelhe1a7yu91"}}, "name": "t3_y8vzbw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gutnjI3C7cT3L_yZk9dDohzzFKGa8QFVcwaF6_aH0_s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666266233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data collected from the census shows advancement in the student-teacher ratio when compared for 2012 and 2021. Such improvements in the education sector are crucial indicators of long-term economic growth through human capital investment.   &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1cuuhtvb7yu91.png?width=848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9bca24461ab09895fd92fff3272bdd67041a1e72\"&gt;https://preview.redd.it/1cuuhtvb7yu91.png?width=848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9bca24461ab09895fd92fff3272bdd67041a1e72&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/puelhe1a7yu91.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39a5a3bc783f90c0aabfbf5c72045456e6f5f52f\"&gt;https://preview.redd.it/puelhe1a7yu91.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39a5a3bc783f90c0aabfbf5c72045456e6f5f52f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8vzbw", "is_robot_indexable": true, "report_reasons": null, "author": "geoiq_io", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8vzbw/studentteacher_ratio_2012_vs_2021/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8vzbw/studentteacher_ratio_2012_vs_2021/", "subreddit_subscribers": 814512, "created_utc": 1666266233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically the title\n\nI am working on an image dataset with images from 17 classes... And I find myself doing mostly guess work of how many dense and convoluted layers should be there and results are not very intuitive. \n\nLike one of the answer that I read was keep increasing the dense layers till accuracy stops increasing but instead of increasing, my accuracy decreases with more perceptrons i add in a layer. It also decreases if I increase the number of hidden layers.\n\nFor less hidden layers, my model learns with abysmal accuracy I.e. starting from 0.09 (not kidding, also super embarrassing) it reaches around 0.55 at around 5th or 6th epoch and from then it increases at a pace of 0.02-0.03 increase per epoch (which is bogus as the accuracy on my validation data actually decreases after this point)\n\nAs for the data, it is highly uneven. With some classes having around 400-600 images while others have around 20000-30000. For training data, I am using 1000 images per each class (directly 1000 for classes that have them and augmenting through randomflips for those classes that have less). \n\nAny insights in designing would be much appreciated. \n\nP.S. I did the rescaling and normalization of images. Also the images are of different types of blood cells.", "author_fullname": "t2_jo4irqsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys determine convolution layers and dense layers, their shape that is, for a CNN", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8p2nd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666242572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically the title&lt;/p&gt;\n\n&lt;p&gt;I am working on an image dataset with images from 17 classes... And I find myself doing mostly guess work of how many dense and convoluted layers should be there and results are not very intuitive. &lt;/p&gt;\n\n&lt;p&gt;Like one of the answer that I read was keep increasing the dense layers till accuracy stops increasing but instead of increasing, my accuracy decreases with more perceptrons i add in a layer. It also decreases if I increase the number of hidden layers.&lt;/p&gt;\n\n&lt;p&gt;For less hidden layers, my model learns with abysmal accuracy I.e. starting from 0.09 (not kidding, also super embarrassing) it reaches around 0.55 at around 5th or 6th epoch and from then it increases at a pace of 0.02-0.03 increase per epoch (which is bogus as the accuracy on my validation data actually decreases after this point)&lt;/p&gt;\n\n&lt;p&gt;As for the data, it is highly uneven. With some classes having around 400-600 images while others have around 20000-30000. For training data, I am using 1000 images per each class (directly 1000 for classes that have them and augmenting through randomflips for those classes that have less). &lt;/p&gt;\n\n&lt;p&gt;Any insights in designing would be much appreciated. &lt;/p&gt;\n\n&lt;p&gt;P.S. I did the rescaling and normalization of images. Also the images are of different types of blood cells.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8p2nd", "is_robot_indexable": true, "report_reasons": null, "author": "thanderrine", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8p2nd/how_do_you_guys_determine_convolution_layers_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8p2nd/how_do_you_guys_determine_convolution_layers_and/", "subreddit_subscribers": 814512, "created_utc": 1666242572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to set up some Python scripts to run automatically on a recurring basis, dump to .csv, upload to a Snowflake database. Pretty simple. In my professional life I\u2019m familiar with Alteryx but it\u2019s way too expensive for me to buy a personal license lol. What lower cost alternatives are out there? I\u2019ve been looking at stuff like Cascade, Stitch, and Tableau Prep, but I\u2019m feeling a little lost so hoped to just get some recommendations from any folks with experience here\u2026 thank you in advance for any insights!", "author_fullname": "t2_535x4dqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software recommendations to set up automated Python jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8lnjn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666232578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to set up some Python scripts to run automatically on a recurring basis, dump to .csv, upload to a Snowflake database. Pretty simple. In my professional life I\u2019m familiar with Alteryx but it\u2019s way too expensive for me to buy a personal license lol. What lower cost alternatives are out there? I\u2019ve been looking at stuff like Cascade, Stitch, and Tableau Prep, but I\u2019m feeling a little lost so hoped to just get some recommendations from any folks with experience here\u2026 thank you in advance for any insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8lnjn", "is_robot_indexable": true, "report_reasons": null, "author": "vizualbasic", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8lnjn/software_recommendations_to_set_up_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8lnjn/software_recommendations_to_set_up_automated/", "subreddit_subscribers": 814512, "created_utc": 1666232578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's a concept in graph theory, in particular when talking about tournaments, known as a median order, which is essentially an ordering of the vertices that maximizes the amount of edges pointing in the increasing direction with respect to the ordering.\n\nThis can be thought of multiplying the adjacency matrix by a permutation matrix on the right and on the left, such that the sum of the upper triangular part of the matrix is maximal.\n\nI've coded it as an integer program [github link here](https://github.com/alonso-cancino/median_orders), but the problem is really slow. I've been thinking if maybe it could be done with some sort of reinforcement learning procedure, where you'd input the adjacency matrix of the graph and it orders the rows/columns in such a way that maximices the upper triangular sum.\n\nIs this a viable strategy or is this one of those places where ML does not apply? Is there any good reference for trying to solve integer optimization problems with ML?", "author_fullname": "t2_8j3x0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this problem \"solvable\" with some ML technique?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8ctq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666209996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a concept in graph theory, in particular when talking about tournaments, known as a median order, which is essentially an ordering of the vertices that maximizes the amount of edges pointing in the increasing direction with respect to the ordering.&lt;/p&gt;\n\n&lt;p&gt;This can be thought of multiplying the adjacency matrix by a permutation matrix on the right and on the left, such that the sum of the upper triangular part of the matrix is maximal.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve coded it as an integer program &lt;a href=\"https://github.com/alonso-cancino/median_orders\"&gt;github link here&lt;/a&gt;, but the problem is really slow. I&amp;#39;ve been thinking if maybe it could be done with some sort of reinforcement learning procedure, where you&amp;#39;d input the adjacency matrix of the graph and it orders the rows/columns in such a way that maximices the upper triangular sum.&lt;/p&gt;\n\n&lt;p&gt;Is this a viable strategy or is this one of those places where ML does not apply? Is there any good reference for trying to solve integer optimization problems with ML?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?auto=webp&amp;s=a76366a3402c347763a45516a3e596c74737b1ed", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3916f66b61f9600c4d90738234d1e0a3e4e4d437", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c72f2e956ed5ea858865141458240b1f18146cb8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8dbc48c0818783af329f8dde028dad84f5cb9b11", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0f809d905f8a09e0c37d4b606670734e7cbfb60", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e5361c376e5dcca8eac85bcf0b729a47aaa4aa3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=27e5e2356ab0271b5a7d504c914070696b6e8a7b", "width": 1080, "height": 540}], "variants": {}, "id": "Wq9ag5G5LtLj5Qnn-YA2InyN3z_A52IZbamkEA_wXec"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8ctq4", "is_robot_indexable": true, "report_reasons": null, "author": "Alozzk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8ctq4/is_this_problem_solvable_with_some_ml_technique/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8ctq4/is_this_problem_solvable_with_some_ml_technique/", "subreddit_subscribers": 814512, "created_utc": 1666209996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm new to data analysis and excel. I have a question to see how you all might address a problem I'm having. \n\nA group of 20 students take an exam. Some pass and some fail. Students who fail, retake the test a few days later. This continues until all students have passed the exam. \n\nI have data on the student name, pass/fail, date of exam, and exam attempt #. \n\nWhat's the best way to represent this data to show student performance over time? Ideally there would be some indication of exam attempt # on the graph/chart as well. \n\nThanks in advance for help and advice!", "author_fullname": "t2_40s55kmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to represent this data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y89tsj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666203006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to data analysis and excel. I have a question to see how you all might address a problem I&amp;#39;m having. &lt;/p&gt;\n\n&lt;p&gt;A group of 20 students take an exam. Some pass and some fail. Students who fail, retake the test a few days later. This continues until all students have passed the exam. &lt;/p&gt;\n\n&lt;p&gt;I have data on the student name, pass/fail, date of exam, and exam attempt #. &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to represent this data to show student performance over time? Ideally there would be some indication of exam attempt # on the graph/chart as well. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for help and advice!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y89tsj", "is_robot_indexable": true, "report_reasons": null, "author": "guppyguyco", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y89tsj/whats_the_best_way_to_represent_this_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y89tsj/whats_the_best_way_to_represent_this_data/", "subreddit_subscribers": 814512, "created_utc": 1666203006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all! \n\nI've been pretty excited reading about all the advancements in LLMs over the years but never took the time to dip my toes in. I'm also a PC gamer and waited until the gpu market dipped a bit to finally buy a RTX3080 and figured might as well get into some hobbyist ML/deep learning experimentation with a new GPU with so many CUDA cores. \n\nI'm aware its still nothing in comparison to an A100 or the like and that I might still be fairly limited working with any LLM, but wondering if anyone has any experience, tips or guidance on what's possible without a multi-gpu at-home setup, like what's the biggest model one could play with on a single 3080, what to expect in terms of process time per token, the biggest feasible model one could or should use to fine tune on a closed domain dataset, etc. \n\nIf I'm being completely unrealistic in even attempting to fine-tune an LLM locally, please also feel free to clear me of any dilusions. \n\nRegardless, it's still so fascinating to see all the developments in the space happening so quickly. This is a super exciting time we're living in, and hopefully these kinds of technologies will be more accessible to hobbyists as they're developed to require less compute and consume less energy.\n\nThanks in advance for any guidance!", "author_fullname": "t2_5z0my9eb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At-home LLM experimentation questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y87bub", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666197106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been pretty excited reading about all the advancements in LLMs over the years but never took the time to dip my toes in. I&amp;#39;m also a PC gamer and waited until the gpu market dipped a bit to finally buy a RTX3080 and figured might as well get into some hobbyist ML/deep learning experimentation with a new GPU with so many CUDA cores. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware its still nothing in comparison to an A100 or the like and that I might still be fairly limited working with any LLM, but wondering if anyone has any experience, tips or guidance on what&amp;#39;s possible without a multi-gpu at-home setup, like what&amp;#39;s the biggest model one could play with on a single 3080, what to expect in terms of process time per token, the biggest feasible model one could or should use to fine tune on a closed domain dataset, etc. &lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m being completely unrealistic in even attempting to fine-tune an LLM locally, please also feel free to clear me of any dilusions. &lt;/p&gt;\n\n&lt;p&gt;Regardless, it&amp;#39;s still so fascinating to see all the developments in the space happening so quickly. This is a super exciting time we&amp;#39;re living in, and hopefully these kinds of technologies will be more accessible to hobbyists as they&amp;#39;re developed to require less compute and consume less energy.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any guidance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y87bub", "is_robot_indexable": true, "report_reasons": null, "author": "smarthaiti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y87bub/athome_llm_experimentation_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y87bub/athome_llm_experimentation_questions/", "subreddit_subscribers": 814512, "created_utc": 1666197106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I don't get enough \u201cbad\u201d samples, because it\u2019s almost impossible and challenging for our customers to artificially capture defects (bad samples) in production. Without bad samples, we cannot create or optimize algorithms. \n\n&amp;#x200B;\n\nAny suggestions?", "author_fullname": "t2_t9mt0bic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need more bad samples in order to create and optimize algorithms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8bzin", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666208013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t get enough \u201cbad\u201d samples, because it\u2019s almost impossible and challenging for our customers to artificially capture defects (bad samples) in production. Without bad samples, we cannot create or optimize algorithms. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8bzin", "is_robot_indexable": true, "report_reasons": null, "author": "alovna88", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8bzin/i_need_more_bad_samples_in_order_to_create_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8bzin/i_need_more_bad_samples_in_order_to_create_and/", "subreddit_subscribers": 814512, "created_utc": 1666208013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Pretty unsophisticated data person here. I work in housing/urban planning so mostly work with census/county assessor parcel data to examine neighborhood housing market trends/indicators. Starting to feel like there might be a better tool for doing more sophisticated analysis than excel. Maybe SPSS? Thoughts?", "author_fullname": "t2_r38s8kc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Next step up from excel for data analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8azf0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666205709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty unsophisticated data person here. I work in housing/urban planning so mostly work with census/county assessor parcel data to examine neighborhood housing market trends/indicators. Starting to feel like there might be a better tool for doing more sophisticated analysis than excel. Maybe SPSS? Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8azf0", "is_robot_indexable": true, "report_reasons": null, "author": "theEarnestUrbanist", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8azf0/next_step_up_from_excel_for_data_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8azf0/next_step_up_from_excel_for_data_analysis/", "subreddit_subscribers": 814512, "created_utc": 1666205709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Im using the `refresh_token` &gt; `fetch new access_token` &gt; `make request` flow to fetch json data from Xero. \n\nBut for some reason providing the same tokens and credentials to tap-xero's config.json returns `tap_xero.client.XeroNotFoundError: HTTP-error-code: 404, Error: The resource you have specified cannot be found.`\n\nThe command args are: `tap-xero -c config.json -d &gt; cat.json`\n\nSinger's documentation is really lacking for their dedicated taps.", "author_fullname": "t2_n732f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Singer's tap-xero to fetch Xero data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y89cok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666201883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im using the &lt;code&gt;refresh_token&lt;/code&gt; &amp;gt; &lt;code&gt;fetch new access_token&lt;/code&gt; &amp;gt; &lt;code&gt;make request&lt;/code&gt; flow to fetch json data from Xero. &lt;/p&gt;\n\n&lt;p&gt;But for some reason providing the same tokens and credentials to tap-xero&amp;#39;s config.json returns &lt;code&gt;tap_xero.client.XeroNotFoundError: HTTP-error-code: 404, Error: The resource you have specified cannot be found.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;The command args are: &lt;code&gt;tap-xero -c config.json -d &amp;gt; cat.json&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Singer&amp;#39;s documentation is really lacking for their dedicated taps.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y89cok", "is_robot_indexable": true, "report_reasons": null, "author": "buckypimpin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y89cok/has_anyone_used_singers_tapxero_to_fetch_xero_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y89cok/has_anyone_used_singers_tapxero_to_fetch_xero_data/", "subreddit_subscribers": 814512, "created_utc": 1666201883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, I have been trying to scrap the school schedule (for obvious reasons, of course). And so far, I saw coursicle claims that they get the data from \"Smith's public listing course\". I've tried to find it but no use, then I found a reddit post from the cofounder of coursicle himself saying that he ping the school gateway ([https://www.reddit.com/r/gatech/comments/r0datq/where\\_does\\_coursicle\\_get\\_its\\_class\\_seats\\_data\\_from/](https://www.reddit.com/r/gatech/comments/r0datq/where_does_coursicle_get_its_class_seats_data_from/)). I'm not sure whether I understand his idea right, but I can't find any way to access my school schedule without actually have to login in, which is a big deal when employ bot to server. So do you have any suggestion, or have any source that has college's courses available publicly, please let me know. Thank you\n\nhttps://preview.redd.it/m23sd3e6ssu91.png?width=907&amp;format=png&amp;auto=webp&amp;s=288959bf76d09287f908b4b019fa0890756c9c51", "author_fullname": "t2_eo1lh8sj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Public College schedule from Coursicle", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 16, "top_awarded_type": null, "hide_score": false, "media_metadata": {"m23sd3e6ssu91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 12, "x": 108, "u": "https://preview.redd.it/m23sd3e6ssu91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f035c5a7fe36151c0497efd5b4c7266ba7bca5b"}, {"y": 25, "x": 216, "u": "https://preview.redd.it/m23sd3e6ssu91.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f49e3225bd5cb82be7a0e1662a497a2cf33116ad"}, {"y": 38, "x": 320, "u": "https://preview.redd.it/m23sd3e6ssu91.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=503a99350e3bfa9efeebe99f181cd505f09d7f4a"}, {"y": 76, "x": 640, "u": "https://preview.redd.it/m23sd3e6ssu91.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=db7ff2816c4a6a1220dfbba36873020eb55aa599"}], "s": {"y": 109, "x": 907, "u": "https://preview.redd.it/m23sd3e6ssu91.png?width=907&amp;format=png&amp;auto=webp&amp;s=288959bf76d09287f908b4b019fa0890756c9c51"}, "id": "m23sd3e6ssu91"}}, "name": "t3_y88uyt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-A-f9IFIg2xrA82oOkM5O_ESc1W53HXKOTUR4W59Z2A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666200713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been trying to scrap the school schedule (for obvious reasons, of course). And so far, I saw coursicle claims that they get the data from &amp;quot;Smith&amp;#39;s public listing course&amp;quot;. I&amp;#39;ve tried to find it but no use, then I found a reddit post from the cofounder of coursicle himself saying that he ping the school gateway (&lt;a href=\"https://www.reddit.com/r/gatech/comments/r0datq/where_does_coursicle_get_its_class_seats_data_from/\"&gt;https://www.reddit.com/r/gatech/comments/r0datq/where_does_coursicle_get_its_class_seats_data_from/&lt;/a&gt;). I&amp;#39;m not sure whether I understand his idea right, but I can&amp;#39;t find any way to access my school schedule without actually have to login in, which is a big deal when employ bot to server. So do you have any suggestion, or have any source that has college&amp;#39;s courses available publicly, please let me know. Thank you&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/m23sd3e6ssu91.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=288959bf76d09287f908b4b019fa0890756c9c51\"&gt;https://preview.redd.it/m23sd3e6ssu91.png?width=907&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=288959bf76d09287f908b4b019fa0890756c9c51&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y88uyt", "is_robot_indexable": true, "report_reasons": null, "author": "PiccoloStreet3002", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y88uyt/public_college_schedule_from_coursicle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y88uyt/public_college_schedule_from_coursicle/", "subreddit_subscribers": 814512, "created_utc": 1666200713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q] What are good Deep Learning packages that maximize the M1 chip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8tgte", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666258094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8tgte", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8tgte/q_what_are_good_deep_learning_packages_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8tgte/q_what_are_good_deep_learning_packages_that/", "subreddit_subscribers": 814512, "created_utc": 1666258094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nif got a data science problem and i am not quite sure how to solve it. I have a dataset about bus arrival times with the Target variable (isDelayed True/False). The dataset has a lot of features that could potentially predict a delay. Such as Weatherdata, Bustype, Weekday or Buffertimes and so on. The assumption is that often not through one variable a delay can be predicted but through complex interactions of multiple variables. To avoid delays in the future, it is critical to not only predict delays but to detect the different cases where delays might occur. So, it is critical to understand the difference of the variable combinations that lead to delay und such that don't. Also it is assumed that there can be multiple different scenarios that can lead to a delay. And so different classes within the delays should be distinguished. Do you have any suggestions how to solve this problem?\n\nMy Proposel to solve such a Problem goes as follows:\n\n1. Use a feature selection method to derive important attributes from the dataset\n\n2. Use a simple (or white box) supervised prediction method to find rules about when a bus delays and when not\n\n3. Use a clustering algorithm to explain different scenarios of delays\n\nMy Question are:\n\n\\- Is this a valid methodologie for such a problem?\n\n\\- Is the second step a valid approach to find generalizable rules about the distinction between the two classes with high dimensional data, that sufficiently pictures interactions of variables and are explainable?\n\n\\- What could be a good algorithm for step two?\n\n\\- Is there a better way to solve this problem?\n\nI hope I provided all Information needed. Thank you very much in advance.\n\nKind regards", "author_fullname": "t2_jear2r3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can bus delays be explained with data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8s5er", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666253214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;if got a data science problem and i am not quite sure how to solve it. I have a dataset about bus arrival times with the Target variable (isDelayed True/False). The dataset has a lot of features that could potentially predict a delay. Such as Weatherdata, Bustype, Weekday or Buffertimes and so on. The assumption is that often not through one variable a delay can be predicted but through complex interactions of multiple variables. To avoid delays in the future, it is critical to not only predict delays but to detect the different cases where delays might occur. So, it is critical to understand the difference of the variable combinations that lead to delay und such that don&amp;#39;t. Also it is assumed that there can be multiple different scenarios that can lead to a delay. And so different classes within the delays should be distinguished. Do you have any suggestions how to solve this problem?&lt;/p&gt;\n\n&lt;p&gt;My Proposel to solve such a Problem goes as follows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Use a feature selection method to derive important attributes from the dataset&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use a simple (or white box) supervised prediction method to find rules about when a bus delays and when not&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use a clustering algorithm to explain different scenarios of delays&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My Question are:&lt;/p&gt;\n\n&lt;p&gt;- Is this a valid methodologie for such a problem?&lt;/p&gt;\n\n&lt;p&gt;- Is the second step a valid approach to find generalizable rules about the distinction between the two classes with high dimensional data, that sufficiently pictures interactions of variables and are explainable?&lt;/p&gt;\n\n&lt;p&gt;- What could be a good algorithm for step two?&lt;/p&gt;\n\n&lt;p&gt;- Is there a better way to solve this problem?&lt;/p&gt;\n\n&lt;p&gt;I hope I provided all Information needed. Thank you very much in advance.&lt;/p&gt;\n\n&lt;p&gt;Kind regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8s5er", "is_robot_indexable": true, "report_reasons": null, "author": "Mammoth_Advice1571", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8s5er/how_can_bus_delays_be_explained_with_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8s5er/how_can_bus_delays_be_explained_with_data_science/", "subreddit_subscribers": 814512, "created_utc": 1666253214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know there are some functions which will convert pandas dataframe to HTML table output. After converting to an HTML table, how can we style the table?", "author_fullname": "t2_9r5qrevk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DATAFRAME TO HTML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8m1z4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666233702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there are some functions which will convert pandas dataframe to HTML table output. After converting to an HTML table, how can we style the table?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8m1z4", "is_robot_indexable": true, "report_reasons": null, "author": "Fabro_vaz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8m1z4/dataframe_to_html/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8m1z4/dataframe_to_html/", "subreddit_subscribers": 814512, "created_utc": 1666233702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Did you work remotely or do you know any data scientist that worked remotely before the pandemic? If so, how common was it?", "author_fullname": "t2_k82la2mu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Were remote jobs common in data science/data analytics before the pandemic?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8kxrh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666230596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did you work remotely or do you know any data scientist that worked remotely before the pandemic? If so, how common was it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8kxrh", "is_robot_indexable": true, "report_reasons": null, "author": "benedick2", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8kxrh/were_remote_jobs_common_in_data_sciencedata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8kxrh/were_remote_jobs_common_in_data_sciencedata/", "subreddit_subscribers": 814512, "created_utc": 1666230596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a free account and I've made some visualizations to share with my linkedin network. I plan to save it on github. What is the best way to share it ?\n\ni can't embed it, it won't allow me on my free account.", "author_fullname": "t2_13t60b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a best way to share powerBI reports to Github ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8ksmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666230196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a free account and I&amp;#39;ve made some visualizations to share with my linkedin network. I plan to save it on github. What is the best way to share it ?&lt;/p&gt;\n\n&lt;p&gt;i can&amp;#39;t embed it, it won&amp;#39;t allow me on my free account.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8ksmj", "is_robot_indexable": true, "report_reasons": null, "author": "drugsarebadmky", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8ksmj/what_is_a_best_way_to_share_powerbi_reports_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8ksmj/what_is_a_best_way_to_share_powerbi_reports_to/", "subreddit_subscribers": 814512, "created_utc": 1666230196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working on a project where we have users that perform a series of actions in a specific order, every day. I also have the delay between each action\n\nIf I know that at least a know of subset of users do not perform a fraudulent action. What is a possible way to know which users are fraudulent or at least start an investigation.\n\nI have many ideas but I don't know where to start and not sure my ideas make sense. Here they are:\n\n\\- create a markov chain using the historical data and tag users that do a series of action with low probability\n\n\\- create the markov chain and cluster it in one way or another (dont know how yet), then tag users that do not belong to a certain cluster\n\n\\- create a markov chain and use a graph neural network to do some sort of classification (dont know how I will proceed without labels)\n\nCan you please guide me, provide ressources or any kind of feedback or concept that can help me move forward. Thanks", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Graph classification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8kggy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666229306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a project where we have users that perform a series of actions in a specific order, every day. I also have the delay between each action&lt;/p&gt;\n\n&lt;p&gt;If I know that at least a know of subset of users do not perform a fraudulent action. What is a possible way to know which users are fraudulent or at least start an investigation.&lt;/p&gt;\n\n&lt;p&gt;I have many ideas but I don&amp;#39;t know where to start and not sure my ideas make sense. Here they are:&lt;/p&gt;\n\n&lt;p&gt;- create a markov chain using the historical data and tag users that do a series of action with low probability&lt;/p&gt;\n\n&lt;p&gt;- create the markov chain and cluster it in one way or another (dont know how yet), then tag users that do not belong to a certain cluster&lt;/p&gt;\n\n&lt;p&gt;- create a markov chain and use a graph neural network to do some sort of classification (dont know how I will proceed without labels)&lt;/p&gt;\n\n&lt;p&gt;Can you please guide me, provide ressources or any kind of feedback or concept that can help me move forward. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8kggy", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8kggy/graph_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8kggy/graph_classification/", "subreddit_subscribers": 814512, "created_utc": 1666229306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "*(I apologize in advance if this is the wrong forum for my question. I'm not entirely sure where to direct it.)*\n\n**Background:**  \nI'm a business professor in Boston, and I expect to be awarded tenure this spring. I also expect my teaching and service demands to reduce quite a bit in the next year. I would like to use this sudden flexibility to begin developing my technical and data skills, which are severely lacking. I am fine with a long-term pursuit (2+ years). I teach undergraduate analytics, but in a very applied and simple way (e.g., Google Analytics, SPSS, linear regression, binary logistic regression, cluster analysis, etc.).\n\n**What I'm looking for:**  \nMy priorities in developing a side hustle are as such (in this order):\n\n1. Gain career flexibility. My PhD currently limits me to academia.\n2. Gain some opportunities for freelance consulting.\n3. Improve my capacity to build interesting datasets that could help my research and my students.\n4. Develop a more visible skillset that I can publicly showcase (this is understandably vague). Long-term ambitions are to build more of an online presence oriented around an in-demand skillset.\n\n**Ideas:**  \nSome areas that I am brainstorming:\n\n1. Machine Learning (Not sure what this industry looks like, so may be a naiive idea)\n2. Data engineering (could help me build unique datasets, but would not have access to servers)\n3. Web development\n4. Data visualization\n5. Data science (I understand that some of the topics above fall under this category)\n\nAll of these topics have equal intuitive appeal to me. My problem is that I do not fully understand the dynamics of these industries, as I've been tucked away in my academic bubble plugging away at my personal research projects. I am looking to break out of this bubble a bit, while also holding onto the security it provides (not looking to leave academia anytime soon).\n\nWould love any insights you may have to offer!", "author_fullname": "t2_1ybsrlm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me pick a side hustle", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y85i96", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666192813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;(I apologize in advance if this is the wrong forum for my question. I&amp;#39;m not entirely sure where to direct it.)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt;&lt;br/&gt;\nI&amp;#39;m a business professor in Boston, and I expect to be awarded tenure this spring. I also expect my teaching and service demands to reduce quite a bit in the next year. I would like to use this sudden flexibility to begin developing my technical and data skills, which are severely lacking. I am fine with a long-term pursuit (2+ years). I teach undergraduate analytics, but in a very applied and simple way (e.g., Google Analytics, SPSS, linear regression, binary logistic regression, cluster analysis, etc.).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;m looking for:&lt;/strong&gt;&lt;br/&gt;\nMy priorities in developing a side hustle are as such (in this order):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Gain career flexibility. My PhD currently limits me to academia.&lt;/li&gt;\n&lt;li&gt;Gain some opportunities for freelance consulting.&lt;/li&gt;\n&lt;li&gt;Improve my capacity to build interesting datasets that could help my research and my students.&lt;/li&gt;\n&lt;li&gt;Develop a more visible skillset that I can publicly showcase (this is understandably vague). Long-term ambitions are to build more of an online presence oriented around an in-demand skillset.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Ideas:&lt;/strong&gt;&lt;br/&gt;\nSome areas that I am brainstorming:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Machine Learning (Not sure what this industry looks like, so may be a naiive idea)&lt;/li&gt;\n&lt;li&gt;Data engineering (could help me build unique datasets, but would not have access to servers)&lt;/li&gt;\n&lt;li&gt;Web development&lt;/li&gt;\n&lt;li&gt;Data visualization&lt;/li&gt;\n&lt;li&gt;Data science (I understand that some of the topics above fall under this category)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;All of these topics have equal intuitive appeal to me. My problem is that I do not fully understand the dynamics of these industries, as I&amp;#39;ve been tucked away in my academic bubble plugging away at my personal research projects. I am looking to break out of this bubble a bit, while also holding onto the security it provides (not looking to leave academia anytime soon).&lt;/p&gt;\n\n&lt;p&gt;Would love any insights you may have to offer!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y85i96", "is_robot_indexable": true, "report_reasons": null, "author": "iscurred", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y85i96/help_me_pick_a_side_hustle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y85i96/help_me_pick_a_side_hustle/", "subreddit_subscribers": 814512, "created_utc": 1666192813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have some mixed data that I thought I'd first transform into embeddings and then cluster the embedding. However, since I'm clustering embeddings does that mean that I'm going to lose all interpretability of the clusters? I know there's some loss of interpreability when using embeddings but I'm not clear on what the degree of that loss is. Anyone care to clarify?", "author_fullname": "t2_5ox025vb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Clustering embeddings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y82y0u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666186559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some mixed data that I thought I&amp;#39;d first transform into embeddings and then cluster the embedding. However, since I&amp;#39;m clustering embeddings does that mean that I&amp;#39;m going to lose all interpretability of the clusters? I know there&amp;#39;s some loss of interpreability when using embeddings but I&amp;#39;m not clear on what the degree of that loss is. Anyone care to clarify?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y82y0u", "is_robot_indexable": true, "report_reasons": null, "author": "SomaDomaBoma", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y82y0u/clustering_embeddings/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y82y0u/clustering_embeddings/", "subreddit_subscribers": 814512, "created_utc": 1666186559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to create signature recognition, they have two input sources, one is the signature in plain paper the other is the signature from ID Card. \n\nCurrently my preprocessing for plain paper is converting to gray, using HSV threshold to remove, remove noise by blurring, cropping with boundingRect\n\nThis has the benefits that all input will have the same format. But Is it too much, or maybe can cause it to unable to generalize?", "author_fullname": "t2_ktg7qk2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much preprocessing is too much (Image Preprocessing)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8kwyb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666230530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to create signature recognition, they have two input sources, one is the signature in plain paper the other is the signature from ID Card. &lt;/p&gt;\n\n&lt;p&gt;Currently my preprocessing for plain paper is converting to gray, using HSV threshold to remove, remove noise by blurring, cropping with boundingRect&lt;/p&gt;\n\n&lt;p&gt;This has the benefits that all input will have the same format. But Is it too much, or maybe can cause it to unable to generalize?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8kwyb", "is_robot_indexable": true, "report_reasons": null, "author": "chervilious", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8kwyb/how_much_preprocessing_is_too_much_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8kwyb/how_much_preprocessing_is_too_much_image/", "subreddit_subscribers": 814512, "created_utc": 1666230530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone! I'm starting to delve into this field and want to know what are the biggest challenges of this topic.", "author_fullname": "t2_lp9fwhgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the biggest challenges of natural language processing now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8cl4u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666209448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I&amp;#39;m starting to delve into this field and want to know what are the biggest challenges of this topic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8cl4u", "is_robot_indexable": true, "report_reasons": null, "author": "comrade_pustota", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8cl4u/what_are_the_biggest_challenges_of_natural/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8cl4u/what_are_the_biggest_challenges_of_natural/", "subreddit_subscribers": 814512, "created_utc": 1666209448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently working to address customer churn among deposit accounts of a retail bank. In this case, the churn is not explicitly defined since a customer- either an individual, or a corporate entity, both using the account primarily to fulfil business related transactions- is free to deposit or withdraw any amount at any point in time.\n\nI am inclined towards approaching this as an anomaly detection problem rather a classification one: i.e., identify customers as likely to churn by detecting anomalous patterns in their behaviour such as frequent, increasing withdrawals and/or declining credits?\n\nI would like to hear your opinion on this. Also, if you could cite any approach/existing solution for a similar problem. I am currently reading up on the following that considers hierarchical Temporal Memory Approach to address churn.\n\nhttps://core.ac.uk/download/pdf/225895269.pdf", "author_fullname": "t2_1mp5bur7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing churn when it cannot be defined by an event", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y82jtt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666185558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working to address customer churn among deposit accounts of a retail bank. In this case, the churn is not explicitly defined since a customer- either an individual, or a corporate entity, both using the account primarily to fulfil business related transactions- is free to deposit or withdraw any amount at any point in time.&lt;/p&gt;\n\n&lt;p&gt;I am inclined towards approaching this as an anomaly detection problem rather a classification one: i.e., identify customers as likely to churn by detecting anomalous patterns in their behaviour such as frequent, increasing withdrawals and/or declining credits?&lt;/p&gt;\n\n&lt;p&gt;I would like to hear your opinion on this. Also, if you could cite any approach/existing solution for a similar problem. I am currently reading up on the following that considers hierarchical Temporal Memory Approach to address churn.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://core.ac.uk/download/pdf/225895269.pdf\"&gt;https://core.ac.uk/download/pdf/225895269.pdf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y82jtt", "is_robot_indexable": true, "report_reasons": null, "author": "sn71", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y82jtt/analyzing_churn_when_it_cannot_be_defined_by_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y82jtt/analyzing_churn_when_it_cannot_be_defined_by_an/", "subreddit_subscribers": 814512, "created_utc": 1666185558.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}