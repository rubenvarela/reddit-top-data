{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to set up some Python scripts to run automatically on a recurring basis, dump to .csv, upload to a Snowflake database. Pretty simple. In my professional life I\u2019m familiar with Alteryx but it\u2019s way too expensive for me to buy a personal license lol. What lower cost alternatives are out there? I\u2019ve been looking at stuff like Cascade, Stitch, and Tableau Prep, but I\u2019m feeling a little lost so hoped to just get some recommendations from any folks with experience here\u2026 thank you in advance for any insights!", "author_fullname": "t2_535x4dqy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software recommendations to set up automated Python jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8lnjn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666232578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to set up some Python scripts to run automatically on a recurring basis, dump to .csv, upload to a Snowflake database. Pretty simple. In my professional life I\u2019m familiar with Alteryx but it\u2019s way too expensive for me to buy a personal license lol. What lower cost alternatives are out there? I\u2019ve been looking at stuff like Cascade, Stitch, and Tableau Prep, but I\u2019m feeling a little lost so hoped to just get some recommendations from any folks with experience here\u2026 thank you in advance for any insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8lnjn", "is_robot_indexable": true, "report_reasons": null, "author": "vizualbasic", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8lnjn/software_recommendations_to_set_up_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8lnjn/software_recommendations_to_set_up_automated/", "subreddit_subscribers": 814563, "created_utc": 1666232578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Business Objective - The company started a LTIP (long term incentive program) to compensate the employees as a replacement of stocks and finance team wants to track the program and **figure out if the plan is effective in keeping the talent at the company.** How do I figure out if this program **caused** the turnover rate to drop?\n\nThis program is very new and the data we have is the performance of each individual. For example, the units assigned to each employee but have never looked/tracked at how they are been distributed as per employee.\n\nHow units are assigned? On the day of January 1st of each year, each employee gets certain LTIP unit (1 unit = 1USD). But the number of units each employee gets is dependent on the manager and number of team members under manager.\n\nAll the units are vested after 3rd year. Price of unit is dependent on how the company grows.\n\nHow the amount is calculated?Let\u2019s say I receive a 1000 units on Jan 1st, 2019 and the company grew 150% across 3 years. On Jan 1st 2022, the amount will be 1500$ compared to 1000$ in 2019.", "author_fullname": "t2_rochx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to do analysis on this data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8lop0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666232671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Business Objective - The company started a LTIP (long term incentive program) to compensate the employees as a replacement of stocks and finance team wants to track the program and &lt;strong&gt;figure out if the plan is effective in keeping the talent at the company.&lt;/strong&gt; How do I figure out if this program &lt;strong&gt;caused&lt;/strong&gt; the turnover rate to drop?&lt;/p&gt;\n\n&lt;p&gt;This program is very new and the data we have is the performance of each individual. For example, the units assigned to each employee but have never looked/tracked at how they are been distributed as per employee.&lt;/p&gt;\n\n&lt;p&gt;How units are assigned? On the day of January 1st of each year, each employee gets certain LTIP unit (1 unit = 1USD). But the number of units each employee gets is dependent on the manager and number of team members under manager.&lt;/p&gt;\n\n&lt;p&gt;All the units are vested after 3rd year. Price of unit is dependent on how the company grows.&lt;/p&gt;\n\n&lt;p&gt;How the amount is calculated?Let\u2019s say I receive a 1000 units on Jan 1st, 2019 and the company grew 150% across 3 years. On Jan 1st 2022, the amount will be 1500$ compared to 1000$ in 2019.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8lop0", "is_robot_indexable": true, "report_reasons": null, "author": "yashk1", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8lop0/how_to_do_analysis_on_this_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8lop0/how_to_do_analysis_on_this_data/", "subreddit_subscribers": 814563, "created_utc": 1666232671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nif got a data science problem and i am not quite sure how to solve it. I have a dataset about bus arrival times with the Target variable (isDelayed True/False). The dataset has a lot of features that could potentially predict a delay. Such as Weatherdata, Bustype, Weekday or Buffertimes and so on. The assumption is that often not through one variable a delay can be predicted but through complex interactions of multiple variables. To avoid delays in the future, it is critical to not only predict delays but to detect the different cases where delays might occur. So, it is critical to understand the difference of the variable combinations that lead to delay und such that don't. Also it is assumed that there can be multiple different scenarios that can lead to a delay. And so different classes within the delays should be distinguished. Do you have any suggestions how to solve this problem?\n\nMy Proposel to solve such a Problem goes as follows:\n\n1. Use a feature selection method to derive important attributes from the dataset\n\n2. Use a simple (or white box) supervised prediction method to find rules about when a bus delays and when not\n\n3. Use a clustering algorithm to explain different scenarios of delays\n\nMy Question are:\n\n\\- Is this a valid methodologie for such a problem?\n\n\\- Is the second step a valid approach to find generalizable rules about the distinction between the two classes with high dimensional data, that sufficiently pictures interactions of variables and are explainable?\n\n\\- What could be a good algorithm for step two?\n\n\\- Is there a better way to solve this problem?\n\nI hope I provided all Information needed. Thank you very much in advance.\n\nKind regards", "author_fullname": "t2_jear2r3i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can bus delays be explained with data science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8s5er", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666253214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;if got a data science problem and i am not quite sure how to solve it. I have a dataset about bus arrival times with the Target variable (isDelayed True/False). The dataset has a lot of features that could potentially predict a delay. Such as Weatherdata, Bustype, Weekday or Buffertimes and so on. The assumption is that often not through one variable a delay can be predicted but through complex interactions of multiple variables. To avoid delays in the future, it is critical to not only predict delays but to detect the different cases where delays might occur. So, it is critical to understand the difference of the variable combinations that lead to delay und such that don&amp;#39;t. Also it is assumed that there can be multiple different scenarios that can lead to a delay. And so different classes within the delays should be distinguished. Do you have any suggestions how to solve this problem?&lt;/p&gt;\n\n&lt;p&gt;My Proposel to solve such a Problem goes as follows:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Use a feature selection method to derive important attributes from the dataset&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use a simple (or white box) supervised prediction method to find rules about when a bus delays and when not&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Use a clustering algorithm to explain different scenarios of delays&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My Question are:&lt;/p&gt;\n\n&lt;p&gt;- Is this a valid methodologie for such a problem?&lt;/p&gt;\n\n&lt;p&gt;- Is the second step a valid approach to find generalizable rules about the distinction between the two classes with high dimensional data, that sufficiently pictures interactions of variables and are explainable?&lt;/p&gt;\n\n&lt;p&gt;- What could be a good algorithm for step two?&lt;/p&gt;\n\n&lt;p&gt;- Is there a better way to solve this problem?&lt;/p&gt;\n\n&lt;p&gt;I hope I provided all Information needed. Thank you very much in advance.&lt;/p&gt;\n\n&lt;p&gt;Kind regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8s5er", "is_robot_indexable": true, "report_reasons": null, "author": "Mammoth_Advice1571", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8s5er/how_can_bus_delays_be_explained_with_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8s5er/how_can_bus_delays_be_explained_with_data_science/", "subreddit_subscribers": 814563, "created_utc": 1666253214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm wondering a certain thing specifically. Suppose you have values of 10, 20, 24, 30, 30. Would the tree evaluate splits at 15, 22, 27, *and* 30 (because the mean of 2 30s is 30), or would 30 be ignored because it's a repeated value?", "author_fullname": "t2_y9a4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do decision trees choose split points for continuous predictor variables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y920w9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666281473.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering a certain thing specifically. Suppose you have values of 10, 20, 24, 30, 30. Would the tree evaluate splits at 15, 22, 27, &lt;em&gt;and&lt;/em&gt; 30 (because the mean of 2 30s is 30), or would 30 be ignored because it&amp;#39;s a repeated value?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y920w9", "is_robot_indexable": true, "report_reasons": null, "author": "WartimeHotTot", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y920w9/how_do_decision_trees_choose_split_points_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y920w9/how_do_decision_trees_choose_split_points_for/", "subreddit_subscribers": 814563, "created_utc": 1666281473.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, so I have a dataset with 8000 rows and 200+ features, mostly numerical. By looking at summary statistics, I see that a good chunk of these features (100ish) have zeroes for 75%+ of their entries. Seems to me like all these zeroes won't contain any useful info for an eventual model, and it would simplify my process a lot to reduce the dimensionality. Would it be wise to simply remove features like this and continue on my way or would I be missing out on lots of possible info? Maybe take all of these sparse columns and use PCA to hopefully reduce them to one or two principal components?", "author_fullname": "t2_d86jaoff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Filtering out features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8fnsj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666216639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, so I have a dataset with 8000 rows and 200+ features, mostly numerical. By looking at summary statistics, I see that a good chunk of these features (100ish) have zeroes for 75%+ of their entries. Seems to me like all these zeroes won&amp;#39;t contain any useful info for an eventual model, and it would simplify my process a lot to reduce the dimensionality. Would it be wise to simply remove features like this and continue on my way or would I be missing out on lots of possible info? Maybe take all of these sparse columns and use PCA to hopefully reduce them to one or two principal components?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8fnsj", "is_robot_indexable": true, "report_reasons": null, "author": "Objective-Simple-836", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8fnsj/filtering_out_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8fnsj/filtering_out_features/", "subreddit_subscribers": 814563, "created_utc": 1666216639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a project at work I'm working on and it's a recommender system for e-commerce plaform, but I am having difficulty because what I learned in school about recommenders was very high level. I've been a DS for a year now and know very little about them. I still sometimes confuse the scenarios for what collaborative filtering and content-based recommenders due to my lack of exposure. You'd think DS programs would put more emphasis on a field like this but many don't. It feels like there is a lot of secrecy with recommender systems despite it actually being a huge topic in the field and something that most seasoned data scientists will have to encounter at some point. When I speak to other data scientists, they aren't as quick to explain concepts for recommenders like they do others since it's so nuanced and specific to a situation.\n\nDoes anyone have guidelines for recommender systems scenarios, and requirements to implement them? My boss at work wants results, an mvp at the very minimum and asap, but all I can come up with are implementation diagrams for CF scenarios. So I need to go from zero to hero in like a few weeks and I'm willing to put in the work. Still wrapping my head around content-based, session-based, and some of the deep learning architectures. I'm going to have to implement graph-based architecture too at some point.\n\nAlso, I'm not the most technical ds yet. When I read some academic ds papers, I don't always get them right away and sometimes, have to read them multiple times to get a clear picture of what's being talked about.", "author_fullname": "t2_8ovhcsu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are quality recommender system materials so hard to come by? Or am I wrong?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y90dom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666277995.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666277632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a project at work I&amp;#39;m working on and it&amp;#39;s a recommender system for e-commerce plaform, but I am having difficulty because what I learned in school about recommenders was very high level. I&amp;#39;ve been a DS for a year now and know very little about them. I still sometimes confuse the scenarios for what collaborative filtering and content-based recommenders due to my lack of exposure. You&amp;#39;d think DS programs would put more emphasis on a field like this but many don&amp;#39;t. It feels like there is a lot of secrecy with recommender systems despite it actually being a huge topic in the field and something that most seasoned data scientists will have to encounter at some point. When I speak to other data scientists, they aren&amp;#39;t as quick to explain concepts for recommenders like they do others since it&amp;#39;s so nuanced and specific to a situation.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have guidelines for recommender systems scenarios, and requirements to implement them? My boss at work wants results, an mvp at the very minimum and asap, but all I can come up with are implementation diagrams for CF scenarios. So I need to go from zero to hero in like a few weeks and I&amp;#39;m willing to put in the work. Still wrapping my head around content-based, session-based, and some of the deep learning architectures. I&amp;#39;m going to have to implement graph-based architecture too at some point.&lt;/p&gt;\n\n&lt;p&gt;Also, I&amp;#39;m not the most technical ds yet. When I read some academic ds papers, I don&amp;#39;t always get them right away and sometimes, have to read them multiple times to get a clear picture of what&amp;#39;s being talked about.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y90dom", "is_robot_indexable": true, "report_reasons": null, "author": "FellowZellow", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y90dom/why_are_quality_recommender_system_materials_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y90dom/why_are_quality_recommender_system_materials_so/", "subreddit_subscribers": 814563, "created_utc": 1666277632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically the title\n\nI am working on an image dataset with images from 17 classes... And I find myself doing mostly guess work of how many dense and convoluted layers should be there and results are not very intuitive. \n\nLike one of the answer that I read was keep increasing the dense layers till accuracy stops increasing but instead of increasing, my accuracy decreases with more perceptrons i add in a layer. It also decreases if I increase the number of hidden layers.\n\nFor less hidden layers, my model learns with abysmal accuracy I.e. starting from 0.09 (not kidding, also super embarrassing) it reaches around 0.55 at around 5th or 6th epoch and from then it increases at a pace of 0.02-0.03 increase per epoch (which is bogus as the accuracy on my validation data actually decreases after this point)\n\nAs for the data, it is highly uneven. With some classes having around 400-600 images while others have around 20000-30000. For training data, I am using 1000 images per each class (directly 1000 for classes that have them and augmenting through randomflips for those classes that have less). \n\nAny insights in designing would be much appreciated. \n\nP.S. I did the rescaling and normalization of images. Also the images are of different types of blood cells.", "author_fullname": "t2_jo4irqsu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys determine convolution layers and dense layers, their shape that is, for a CNN", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8p2nd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666242572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically the title&lt;/p&gt;\n\n&lt;p&gt;I am working on an image dataset with images from 17 classes... And I find myself doing mostly guess work of how many dense and convoluted layers should be there and results are not very intuitive. &lt;/p&gt;\n\n&lt;p&gt;Like one of the answer that I read was keep increasing the dense layers till accuracy stops increasing but instead of increasing, my accuracy decreases with more perceptrons i add in a layer. It also decreases if I increase the number of hidden layers.&lt;/p&gt;\n\n&lt;p&gt;For less hidden layers, my model learns with abysmal accuracy I.e. starting from 0.09 (not kidding, also super embarrassing) it reaches around 0.55 at around 5th or 6th epoch and from then it increases at a pace of 0.02-0.03 increase per epoch (which is bogus as the accuracy on my validation data actually decreases after this point)&lt;/p&gt;\n\n&lt;p&gt;As for the data, it is highly uneven. With some classes having around 400-600 images while others have around 20000-30000. For training data, I am using 1000 images per each class (directly 1000 for classes that have them and augmenting through randomflips for those classes that have less). &lt;/p&gt;\n\n&lt;p&gt;Any insights in designing would be much appreciated. &lt;/p&gt;\n\n&lt;p&gt;P.S. I did the rescaling and normalization of images. Also the images are of different types of blood cells.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8p2nd", "is_robot_indexable": true, "report_reasons": null, "author": "thanderrine", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8p2nd/how_do_you_guys_determine_convolution_layers_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8p2nd/how_do_you_guys_determine_convolution_layers_and/", "subreddit_subscribers": 814563, "created_utc": 1666242572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Did you work remotely or do you know any data scientist that worked remotely before the pandemic? If so, how common was it?", "author_fullname": "t2_k82la2mu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Were remote jobs common in data science/data analytics before the pandemic?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8kxrh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666230596.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did you work remotely or do you know any data scientist that worked remotely before the pandemic? If so, how common was it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8kxrh", "is_robot_indexable": true, "report_reasons": null, "author": "benedick2", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8kxrh/were_remote_jobs_common_in_data_sciencedata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8kxrh/were_remote_jobs_common_in_data_sciencedata/", "subreddit_subscribers": 814563, "created_utc": 1666230596.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's a concept in graph theory, in particular when talking about tournaments, known as a median order, which is essentially an ordering of the vertices that maximizes the amount of edges pointing in the increasing direction with respect to the ordering.\n\nThis can be thought of multiplying the adjacency matrix by a permutation matrix on the right and on the left, such that the sum of the upper triangular part of the matrix is maximal.\n\nI've coded it as an integer program [github link here](https://github.com/alonso-cancino/median_orders), but the problem is really slow. I've been thinking if maybe it could be done with some sort of reinforcement learning procedure, where you'd input the adjacency matrix of the graph and it orders the rows/columns in such a way that maximices the upper triangular sum.\n\nIs this a viable strategy or is this one of those places where ML does not apply? Is there any good reference for trying to solve integer optimization problems with ML?", "author_fullname": "t2_8j3x0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this problem \"solvable\" with some ML technique?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8ctq4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1666209996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a concept in graph theory, in particular when talking about tournaments, known as a median order, which is essentially an ordering of the vertices that maximizes the amount of edges pointing in the increasing direction with respect to the ordering.&lt;/p&gt;\n\n&lt;p&gt;This can be thought of multiplying the adjacency matrix by a permutation matrix on the right and on the left, such that the sum of the upper triangular part of the matrix is maximal.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve coded it as an integer program &lt;a href=\"https://github.com/alonso-cancino/median_orders\"&gt;github link here&lt;/a&gt;, but the problem is really slow. I&amp;#39;ve been thinking if maybe it could be done with some sort of reinforcement learning procedure, where you&amp;#39;d input the adjacency matrix of the graph and it orders the rows/columns in such a way that maximices the upper triangular sum.&lt;/p&gt;\n\n&lt;p&gt;Is this a viable strategy or is this one of those places where ML does not apply? Is there any good reference for trying to solve integer optimization problems with ML?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?auto=webp&amp;s=a76366a3402c347763a45516a3e596c74737b1ed", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3916f66b61f9600c4d90738234d1e0a3e4e4d437", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c72f2e956ed5ea858865141458240b1f18146cb8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8dbc48c0818783af329f8dde028dad84f5cb9b11", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0f809d905f8a09e0c37d4b606670734e7cbfb60", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e5361c376e5dcca8eac85bcf0b729a47aaa4aa3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Ah6w4nBNbPGP36O_njNAtcMATQmVY7Ul5UnwtVOKTm0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=27e5e2356ab0271b5a7d504c914070696b6e8a7b", "width": 1080, "height": 540}], "variants": {}, "id": "Wq9ag5G5LtLj5Qnn-YA2InyN3z_A52IZbamkEA_wXec"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8ctq4", "is_robot_indexable": true, "report_reasons": null, "author": "Alozzk", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8ctq4/is_this_problem_solvable_with_some_ml_technique/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8ctq4/is_this_problem_solvable_with_some_ml_technique/", "subreddit_subscribers": 814563, "created_utc": 1666209996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is anyone here working in the field of materials? I would love to share experiences,  learn what machine learning algorithm worked well or not, discuss advances, etc.\nHave you found good online resources, training and courses, books, maybe a forum or reddit community?", "author_fullname": "t2_19qf9hu9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[D] Materials Informatics Good Resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_y957n6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666288827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone here working in the field of materials? I would love to share experiences,  learn what machine learning algorithm worked well or not, discuss advances, etc.\nHave you found good online resources, training and courses, books, maybe a forum or reddit community?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y957n6", "is_robot_indexable": true, "report_reasons": null, "author": "DreamyPen", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y957n6/d_materials_informatics_good_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y957n6/d_materials_informatics_good_resources/", "subreddit_subscribers": 814563, "created_utc": 1666288827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I don't get enough \u201cbad\u201d samples, because it\u2019s almost impossible and challenging for our customers to artificially capture defects (bad samples) in production. Without bad samples, we cannot create or optimize algorithms. \n\n&amp;#x200B;\n\nAny suggestions?", "author_fullname": "t2_t9mt0bic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need more bad samples in order to create and optimize algorithms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8bzin", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666208013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t get enough \u201cbad\u201d samples, because it\u2019s almost impossible and challenging for our customers to artificially capture defects (bad samples) in production. Without bad samples, we cannot create or optimize algorithms. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8bzin", "is_robot_indexable": true, "report_reasons": null, "author": "alovna88", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8bzin/i_need_more_bad_samples_in_order_to_create_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8bzin/i_need_more_bad_samples_in_order_to_create_and/", "subreddit_subscribers": 814563, "created_utc": 1666208013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys,\n\nRecently graduated and accepted an offer for a grad DS role, and have some time to fill whilst I wait for my start date. Looking to use this time to read about data engineering and cloud tech, so does anyone have any book recommendations, or any other sources, to get me started in those areas (preferably from a DS perspective)?\n\nCheers!", "author_fullname": "t2_2y06cmcz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y91nn9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666280609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;Recently graduated and accepted an offer for a grad DS role, and have some time to fill whilst I wait for my start date. Looking to use this time to read about data engineering and cloud tech, so does anyone have any book recommendations, or any other sources, to get me started in those areas (preferably from a DS perspective)?&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y91nn9", "is_robot_indexable": true, "report_reasons": null, "author": "kikiiiiiiiiii", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y91nn9/book_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y91nn9/book_recommendations/", "subreddit_subscribers": 814563, "created_utc": 1666280609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_amfdjuba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Q] What are good Deep Learning packages that maximize the M1 chip?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8tgte", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666258094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8tgte", "is_robot_indexable": true, "report_reasons": null, "author": "limedove", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8tgte/q_what_are_good_deep_learning_packages_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8tgte/q_what_are_good_deep_learning_packages_that/", "subreddit_subscribers": 814563, "created_utc": 1666258094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a free account and I've made some visualizations to share with my linkedin network. I plan to save it on github. What is the best way to share it ?\n\ni can't embed it, it won't allow me on my free account.", "author_fullname": "t2_13t60b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a best way to share powerBI reports to Github ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8ksmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666230196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a free account and I&amp;#39;ve made some visualizations to share with my linkedin network. I plan to save it on github. What is the best way to share it ?&lt;/p&gt;\n\n&lt;p&gt;i can&amp;#39;t embed it, it won&amp;#39;t allow me on my free account.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8ksmj", "is_robot_indexable": true, "report_reasons": null, "author": "drugsarebadmky", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8ksmj/what_is_a_best_way_to_share_powerbi_reports_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8ksmj/what_is_a_best_way_to_share_powerbi_reports_to/", "subreddit_subscribers": 814563, "created_utc": 1666230196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working on a project where we have users that perform a series of actions in a specific order, every day. I also have the delay between each action\n\nIf I know that at least a know of subset of users do not perform a fraudulent action. What is a possible way to know which users are fraudulent or at least start an investigation.\n\nI have many ideas but I don't know where to start and not sure my ideas make sense. Here they are:\n\n\\- create a markov chain using the historical data and tag users that do a series of action with low probability\n\n\\- create the markov chain and cluster it in one way or another (dont know how yet), then tag users that do not belong to a certain cluster\n\n\\- create a markov chain and use a graph neural network to do some sort of classification (dont know how I will proceed without labels)\n\nCan you please guide me, provide ressources or any kind of feedback or concept that can help me move forward. Thanks", "author_fullname": "t2_4fa0ibvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Graph classification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8kggy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666229306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a project where we have users that perform a series of actions in a specific order, every day. I also have the delay between each action&lt;/p&gt;\n\n&lt;p&gt;If I know that at least a know of subset of users do not perform a fraudulent action. What is a possible way to know which users are fraudulent or at least start an investigation.&lt;/p&gt;\n\n&lt;p&gt;I have many ideas but I don&amp;#39;t know where to start and not sure my ideas make sense. Here they are:&lt;/p&gt;\n\n&lt;p&gt;- create a markov chain using the historical data and tag users that do a series of action with low probability&lt;/p&gt;\n\n&lt;p&gt;- create the markov chain and cluster it in one way or another (dont know how yet), then tag users that do not belong to a certain cluster&lt;/p&gt;\n\n&lt;p&gt;- create a markov chain and use a graph neural network to do some sort of classification (dont know how I will proceed without labels)&lt;/p&gt;\n\n&lt;p&gt;Can you please guide me, provide ressources or any kind of feedback or concept that can help me move forward. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8kggy", "is_robot_indexable": true, "report_reasons": null, "author": "dimem16", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8kggy/graph_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8kggy/graph_classification/", "subreddit_subscribers": 814563, "created_utc": 1666229306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to create signature recognition, they have two input sources, one is the signature in plain paper the other is the signature from ID Card. \n\nCurrently my preprocessing for plain paper is converting to gray, using HSV threshold to remove, remove noise by blurring, cropping with boundingRect\n\nThis has the benefits that all input will have the same format. But Is it too much, or maybe can cause it to unable to generalize?", "author_fullname": "t2_ktg7qk2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much preprocessing is too much (Image Preprocessing)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8kwyb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666230530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to create signature recognition, they have two input sources, one is the signature in plain paper the other is the signature from ID Card. &lt;/p&gt;\n\n&lt;p&gt;Currently my preprocessing for plain paper is converting to gray, using HSV threshold to remove, remove noise by blurring, cropping with boundingRect&lt;/p&gt;\n\n&lt;p&gt;This has the benefits that all input will have the same format. But Is it too much, or maybe can cause it to unable to generalize?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8kwyb", "is_robot_indexable": true, "report_reasons": null, "author": "chervilious", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8kwyb/how_much_preprocessing_is_too_much_image/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8kwyb/how_much_preprocessing_is_too_much_image/", "subreddit_subscribers": 814563, "created_utc": 1666230530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone! I'm starting to delve into this field and want to know what are the biggest challenges of this topic.", "author_fullname": "t2_lp9fwhgv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the biggest challenges of natural language processing now?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8cl4u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666209448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I&amp;#39;m starting to delve into this field and want to know what are the biggest challenges of this topic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8cl4u", "is_robot_indexable": true, "report_reasons": null, "author": "comrade_pustota", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8cl4u/what_are_the_biggest_challenges_of_natural/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8cl4u/what_are_the_biggest_challenges_of_natural/", "subreddit_subscribers": 814563, "created_utc": 1666209448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello!\n\nPlease refer to my username... Could someone please advise me on the best way forward.\nIn case it is important, the project outlined is not for work, it is for my own education.\n\n**Problem**:\nI want to take a set of structured data including prompts (company name, topic, and a few keywords) and generate an introduction paragraph for the company.  \n\nI am interested in trying some of the open source large language models for this but I am unsure what is the most appropriate model or methods for this.  \n  \n**Question 1a)**: What is the most appropriate open source model (and sub-variant) for content generation as outlined above? I have seen BLOOM, GPT-NEO, and OPT but unsure which is best for my needs.  \n\n**Question 1b)**: Given the answer to 1a) above, what is the most appropriate way of obtaining decent result to my task? Training a smaller parameter version of one of the above on some training data? Using few shot learning?  \n  \n**Question 1c)**: For few shot learning, is there any advantages or disadvantages to providing many prompt examples? Most of the time I have seen people use 1 or 2 prompts, but what if I gave 20? Would it improve the results, or just slow the whole thing down?\n\nI'm also interested as a first step to download one of the smaller models to help me to understand how to use these models better. In the future perhaps using an online API like Huggingface will be better, but for now I will try locally.  \n  \n**Question 3**: Can anyone give me an idea of how much RAM some of the smaller LLM require? 8GB? 16GB? 1000GB?  \nAre they able to run in a reasonable time on just a CPU? Just a single GPU?  \n  \nIs there anything else I am missing that you think I should consider?  \n  \nThank you for reading :--)", "author_fullname": "t2_rueknvvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about utilizing large language models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8p6u8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666242946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;Please refer to my username... Could someone please advise me on the best way forward.\nIn case it is important, the project outlined is not for work, it is for my own education.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;:\nI want to take a set of structured data including prompts (company name, topic, and a few keywords) and generate an introduction paragraph for the company.  &lt;/p&gt;\n\n&lt;p&gt;I am interested in trying some of the open source large language models for this but I am unsure what is the most appropriate model or methods for this.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 1a)&lt;/strong&gt;: What is the most appropriate open source model (and sub-variant) for content generation as outlined above? I have seen BLOOM, GPT-NEO, and OPT but unsure which is best for my needs.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 1b)&lt;/strong&gt;: Given the answer to 1a) above, what is the most appropriate way of obtaining decent result to my task? Training a smaller parameter version of one of the above on some training data? Using few shot learning?  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 1c)&lt;/strong&gt;: For few shot learning, is there any advantages or disadvantages to providing many prompt examples? Most of the time I have seen people use 1 or 2 prompts, but what if I gave 20? Would it improve the results, or just slow the whole thing down?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also interested as a first step to download one of the smaller models to help me to understand how to use these models better. In the future perhaps using an online API like Huggingface will be better, but for now I will try locally.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question 3&lt;/strong&gt;: Can anyone give me an idea of how much RAM some of the smaller LLM require? 8GB? 16GB? 1000GB?&lt;br/&gt;\nAre they able to run in a reasonable time on just a CPU? Just a single GPU?  &lt;/p&gt;\n\n&lt;p&gt;Is there anything else I am missing that you think I should consider?  &lt;/p&gt;\n\n&lt;p&gt;Thank you for reading :--)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8p6u8", "is_robot_indexable": true, "report_reasons": null, "author": "some_stupidquestions", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8p6u8/questions_about_utilizing_large_language_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8p6u8/questions_about_utilizing_large_language_models/", "subreddit_subscribers": 814563, "created_utc": 1666242946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\n[Creating GeoDataFrame from DataFrame with coordinates or wkt](https://preview.redd.it/lhwzwtfktwu91.png?width=200&amp;format=png&amp;auto=webp&amp;s=e3bcd0d61a3030e455114c02115f36218c332b8e)\n\n[Creating GeoDataFrame from DataFrame with coordinates or wkt](https://spatial-dev.guru/2022/09/26/creating-geodataframe-from-dataframe-with-coordinates-or-wkt/)", "author_fullname": "t2_avt84u4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating GeoDataFrame from DataFrame with coordinates or wkt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "media_metadata": {"lhwzwtfktwu91": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 43, "x": 108, "u": "https://preview.redd.it/lhwzwtfktwu91.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b25215a958186413f6f99291014df0f5271650b"}], "s": {"y": 81, "x": 200, "u": "https://preview.redd.it/lhwzwtfktwu91.png?width=200&amp;format=png&amp;auto=webp&amp;s=e3bcd0d61a3030e455114c02115f36218c332b8e"}, "id": "lhwzwtfktwu91"}}, "name": "t3_y8r4rq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gAt3s7VchJHP0fmmNZuk-av1Hek8adxGOemZcAgOw8A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666249461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lhwzwtfktwu91.png?width=200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3bcd0d61a3030e455114c02115f36218c332b8e\"&gt;Creating GeoDataFrame from DataFrame with coordinates or wkt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spatial-dev.guru/2022/09/26/creating-geodataframe-from-dataframe-with-coordinates-or-wkt/\"&gt;Creating GeoDataFrame from DataFrame with coordinates or wkt&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8r4rq", "is_robot_indexable": true, "report_reasons": null, "author": "iamgeoknight", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8r4rq/creating_geodataframe_from_dataframe_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8r4rq/creating_geodataframe_from_dataframe_with/", "subreddit_subscribers": 814563, "created_utc": 1666249461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Imagine to build ML models, if we can build a ML model just selecting dataset, target variables and hyper parameters, with out writing any python code.\n\nIs ther any such open source platforms available?", "author_fullname": "t2_lrllrd1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is there any no code/low code platform for ML", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y917a8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666279548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imagine to build ML models, if we can build a ML model just selecting dataset, target variables and hyper parameters, with out writing any python code.&lt;/p&gt;\n\n&lt;p&gt;Is ther any such open source platforms available?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y917a8", "is_robot_indexable": true, "report_reasons": null, "author": "vishal-vora", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y917a8/is_there_any_no_codelow_code_platform_for_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y917a8/is_there_any_no_codelow_code_platform_for_ml/", "subreddit_subscribers": 814563, "created_utc": 1666279548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Please understand that I am not the employer and I will indeed benefit from referring you if you land the job. This is a no bull-shit, true position available\n\nKey item here is that you will lead all data science projects and will define the data future of the company.\n\nMust haves:\n- Data Visualization\n- Python\n- SQL\n- PowerBI\n- Amazon Redshift\n- Mathematica\n\n\nNice to haves:\n- Experience with marketing campaigns\n- Performance Analysis\n- Java\n\n\n\nIt is assumed that you:\n\n- Have a solid understanding of statistics and time-series analysis: Bayesian inference, ARIMA, MCMC, etc.\n\n- Are fluent in programming with Python and using common machine learning libraries (e.g. sklearn, keras/tensorflow, statsmodels, PyMC3 etc.)\n\n- Have first-hand experience in deploying, maintaining, and updating models in production, as well as tracking experiments\n\n- Have first-hand experience in serving ML models via APIs (e.g. Flask, FastAPI).l\n\n- Experienced working end-to-end (from ideation to deployment) to deliver production-ready data products\n\n- Are a mentor for your team members and help them to expand and develop their technical skills\n\n- You have excellent communication skills and proficiency in English. German skills are a big plus", "author_fullname": "t2_57ndwk3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a Data Scientist to lead all data operations in the Marketing engine of a company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y91w52", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.06, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1666282473.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666281161.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please understand that I am not the employer and I will indeed benefit from referring you if you land the job. This is a no bull-shit, true position available&lt;/p&gt;\n\n&lt;p&gt;Key item here is that you will lead all data science projects and will define the data future of the company.&lt;/p&gt;\n\n&lt;p&gt;Must haves:\n- Data Visualization\n- Python\n- SQL\n- PowerBI\n- Amazon Redshift\n- Mathematica&lt;/p&gt;\n\n&lt;p&gt;Nice to haves:\n- Experience with marketing campaigns\n- Performance Analysis\n- Java&lt;/p&gt;\n\n&lt;p&gt;It is assumed that you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Have a solid understanding of statistics and time-series analysis: Bayesian inference, ARIMA, MCMC, etc.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are fluent in programming with Python and using common machine learning libraries (e.g. sklearn, keras/tensorflow, statsmodels, PyMC3 etc.)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have first-hand experience in deploying, maintaining, and updating models in production, as well as tracking experiments&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Have first-hand experience in serving ML models via APIs (e.g. Flask, FastAPI).l&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Experienced working end-to-end (from ideation to deployment) to deliver production-ready data products&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are a mentor for your team members and help them to expand and develop their technical skills&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;You have excellent communication skills and proficiency in English. German skills are a big plus&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y91w52", "is_robot_indexable": true, "report_reasons": null, "author": "MhdAgami", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y91w52/looking_for_a_data_scientist_to_lead_all_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y91w52/looking_for_a_data_scientist_to_lead_all_data/", "subreddit_subscribers": 814563, "created_utc": 1666281161.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was wondering if anyone can provide me with a roadmap or courses I can take that will give me a good chance of landing a data science job. \n\nI am almost done with my first Machine Learning specialization on Coursera. I found that I already knew a lot of the statistical concepts due to my career in actuarial science which is heavy on stats. However, I did not feel like their was enough practice with python as most of the code was given to you in the assignments. So I wouldn\u2019t feel confident saying I can implement a neural network from start to finish for example. Is that expected?\n\nAny tips are appreciated!", "author_fullname": "t2_1z6uqvx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career switch into data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_y8m3m1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.07, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1666233838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if anyone can provide me with a roadmap or courses I can take that will give me a good chance of landing a data science job. &lt;/p&gt;\n\n&lt;p&gt;I am almost done with my first Machine Learning specialization on Coursera. I found that I already knew a lot of the statistical concepts due to my career in actuarial science which is heavy on stats. However, I did not feel like their was enough practice with python as most of the code was given to you in the assignments. So I wouldn\u2019t feel confident saying I can implement a neural network from start to finish for example. Is that expected?&lt;/p&gt;\n\n&lt;p&gt;Any tips are appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "y8m3m1", "is_robot_indexable": true, "report_reasons": null, "author": "examfml", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/y8m3m1/career_switch_into_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/y8m3m1/career_switch_into_data_science/", "subreddit_subscribers": 814563, "created_utc": 1666233838.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}